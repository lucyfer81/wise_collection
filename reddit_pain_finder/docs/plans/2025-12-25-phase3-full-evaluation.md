# Phase 3: å…¨é¢è¿è¡Œã€è¯„ä¼°ä¸ä¼˜åŒ–

> **For Claude:** REQUIRED SUB-SKILL: Use superpowers:executing-plans to implement this plan task-by-task.

**ç›®æ ‡:** åœ¨å…¨é‡æ•°æ®ä¸Šéƒ¨ç½²æ–°æµç¨‹ï¼Œè¯„ä¼°æœ€ç»ˆ"å•†ä¸šæœºä¼šæŠ¥å‘Š"çš„ä»·å€¼ï¼Œå¹¶é‡åŒ–æ–°æµç¨‹å¸¦æ¥çš„æˆæœ¬å˜åŒ–

**æ¶æ„:** åˆ›å»ºæ€§èƒ½ç›‘æ§è„šæœ¬ â†’ è¿è¡Œå…¨é‡æ•°æ®å¤„ç† â†’ ç”Ÿæˆå•†ä¸šæœºä¼šæŠ¥å‘Š â†’ åˆ†ææˆæœ¬ä¸æ€§èƒ½ â†’ è¾“å‡ºè¯„ä¼°æŠ¥å‘Š

**Tech Stack:** Python 3.10+, SQLite, LLM API (SiliconFlow), Markdown reports

---

## ä»»åŠ¡æ¦‚è¿°

Phase 3åŒ…å«ä»¥ä¸‹ä¸»è¦ä»»åŠ¡ï¼š

1. **æ€§èƒ½ç›‘æ§è„šæœ¬** - è·Ÿè¸ªLLMè°ƒç”¨å’Œæˆæœ¬
2. **å…¨é‡æ•°æ®å¤„ç†** - è¿è¡Œå®Œæ•´æµæ°´çº¿
3. **å•†ä¸šä»·å€¼è¯„ä¼°** - åˆ†æç”Ÿæˆçš„æœºä¼šæŠ¥å‘Š
4. **æˆæœ¬æ€§èƒ½æŠ¥å‘Š** - ç”ŸæˆROIåˆ†æ

---

## Task 1: åˆ›å»ºæ€§èƒ½ç›‘æ§è£…é¥°å™¨

**Files:**
- Create: `utils/performance_monitor.py`

**Step 1: åˆ›å»ºæ€§èƒ½ç›‘æ§åŸºç¡€ç±»**

```python
"""
Performance monitoring utility for Phase 3
Tracks LLM calls, token usage, and execution time
"""
import time
import json
from datetime import datetime
from typing import Dict, Any, Optional, Callable
from functools import wraps
import logging

logger = logging.getLogger(__name__)

class PerformanceMonitor:
    """æ€§èƒ½ç›‘æ§å™¨"""

    def __init__(self):
        self.metrics = {
            "start_time": None,
            "end_time": None,
            "stages": {},
            "llm_calls": {
                "total_calls": 0,
                "total_tokens": 0,
                "prompt_tokens": 0,
                "completion_tokens": 0,
                "total_cost": 0.0,
                "calls_by_stage": {}
            }
        }

    def start_stage(self, stage_name: str):
        """å¼€å§‹ä¸€ä¸ªé˜¶æ®µ"""
        if stage_name not in self.metrics["stages"]:
            self.metrics["stages"][stage_name] = {
                "start_time": datetime.now().isoformat(),
                "end_time": None,
                "duration_seconds": 0,
                "items_processed": 0,
                "llm_calls": 0,
                "tokens_used": 0
            }
        else:
            # å¦‚æœå·²å­˜åœ¨ï¼Œæ›´æ–°å¼€å§‹æ—¶é—´
            self.metrics["stages"][stage_name]["start_time"] = datetime.now().isoformat()

    def end_stage(self, stage_name: str, items_processed: int = 0):
        """ç»“æŸä¸€ä¸ªé˜¶æ®µ"""
        if stage_name in self.metrics["stages"]:
            self.metrics["stages"][stage_name]["end_time"] = datetime.now().isoformat()

            # è®¡ç®—æŒç»­æ—¶é—´
            start = datetime.fromisoformat(self.metrics["stages"][stage_name]["start_time"])
            end = datetime.fromisoformat(self.metrics["stages"][stage_name]["end_time"])
            self.metrics["stages"][stage_name]["duration_seconds"] = (end - start).total_seconds()
            self.metrics["stages"][stage_name]["items_processed"] = items_processed

    def record_llm_call(self, stage_name: str, usage: Dict[str, Any]):
        """è®°å½•LLMè°ƒç”¨"""
        self.metrics["llm_calls"]["total_calls"] += 1

        prompt_tokens = usage.get("prompt_tokens", 0)
        completion_tokens = usage.get("completion_tokens", 0)
        total_tokens = usage.get("total_tokens", 0)

        self.metrics["llm_calls"]["prompt_tokens"] += prompt_tokens
        self.metrics["llm_calls"]["completion_tokens"] += completion_tokens
        self.metrics["llm_calls"]["total_tokens"] += total_tokens

        # æ›´æ–°é˜¶æ®µç»Ÿè®¡
        if stage_name not in self.metrics["llm_calls"]["calls_by_stage"]:
            self.metrics["llm_calls"]["calls_by_stage"][stage_name] = {
                "calls": 0,
                "tokens": 0
            }

        self.metrics["llm_calls"]["calls_by_stage"][stage_name]["calls"] += 1
        self.metrics["llm_calls"]["calls_by_stage"][stage_name]["tokens"] += total_tokens

        if stage_name in self.metrics["stages"]:
            self.metrics["stages"][stage_name]["llm_calls"] += 1
            self.metrics["stages"][stage_name]["tokens_used"] += total_tokens

    def calculate_cost(self, prompt_price_per_1k: float = 0.001,
                      completion_price_per_1k: float = 0.002):
        """è®¡ç®—æˆæœ¬ï¼ˆæ ¹æ®å®é™…å®šä»·è°ƒæ•´ï¼‰"""
        prompt_cost = (self.metrics["llm_calls"]["prompt_tokens"] / 1000) * prompt_price_per_1k
        completion_cost = (self.metrics["llm_calls"]["completion_tokens"] / 1000) * completion_price_per_1k
        self.metrics["llm_calls"]["total_cost"] = prompt_cost + completion_cost

        return self.metrics["llm_calls"]["total_cost"]

    def get_summary(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡æ‘˜è¦"""
        # è®¡ç®—æ€»æ—¶é—´
        if self.metrics["stages"]:
            total_duration = sum(
                stage["duration_seconds"]
                for stage in self.metrics["stages"].values()
            )
        else:
            total_duration = 0

        return {
            "total_duration_seconds": total_duration,
            "total_duration_minutes": round(total_duration / 60, 2),
            "total_llm_calls": self.metrics["llm_calls"]["total_calls"],
            "total_tokens": self.metrics["llm_calls"]["total_tokens"],
            "estimated_cost_usd": self.calculate_cost(),
            "stages_summary": {
                name: {
                    "duration_seconds": stage["duration_seconds"],
                    "items_processed": stage["items_processed"],
                    "llm_calls": stage["llm_calls"],
                    "tokens_used": stage["tokens_used"]
                }
                for name, stage in self.metrics["stages"].items()
            }
        }

    def save_metrics(self, filepath: str):
        """ä¿å­˜æŒ‡æ ‡åˆ°æ–‡ä»¶"""
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(self.metrics, f, indent=2, default=str)

    @classmethod
    def load_metrics(cls, filepath: str) -> 'PerformanceMonitor':
        """ä»æ–‡ä»¶åŠ è½½æŒ‡æ ‡"""
        monitor = cls()
        with open(filepath, 'r', encoding='utf-8') as f:
            monitor.metrics = json.load(f)
        return monitor


# å…¨å±€ç›‘æ§å™¨å®ä¾‹
performance_monitor = PerformanceMonitor()
```

**Step 2: æµ‹è¯•æ€§èƒ½ç›‘æ§ç±»**

Run: `python3 -c "from utils.performance_monitor import PerformanceMonitor; pm = PerformanceMonitor(); pm.start_stage('test'); pm.end_stage('test', 10); print(pm.get_summary())"`

Expected: `{'total_duration_seconds': ..., 'stages_summary': {'test': {...}}}`

**Step 3: æäº¤**

```bash
git add utils/performance_monitor.py
git commit -m "feat: add performance monitor for Phase 3"
```

---

## Task 2: é›†æˆæ€§èƒ½ç›‘æ§åˆ°LLMå®¢æˆ·ç«¯

**Files:**
- Modify: `utils/llm_client.py:91-178`

**Step 1: ä¿®æ”¹chat_completionæ–¹æ³•ä»¥é›†æˆç›‘æ§**

åœ¨ `utils/llm_client.py` é¡¶éƒ¨æ·»åŠ å¯¼å…¥ï¼š
```python
from utils.performance_monitor import performance_monitor
```

ä¿®æ”¹ `chat_completion` æ–¹æ³•ï¼Œåœ¨è®°å½•è¯·æ±‚æ—¶é—´åæ·»åŠ ï¼š
```python
# åœ¨ç¬¬160è¡Œé™„è¿‘ï¼Œresponseè·å–ä¹‹å
performance_monitor.record_llm_call(
    stage_name=model_type,
    usage=result["usage"]
)
```

**Step 2: æµ‹è¯•ç›‘æ§é›†æˆ**

Run: `python3 -c "from utils.llm_client import llm_client; from utils.performance_monitor import performance_monitor; llm_client.validate_pain_signal('test'); print(performance_monitor.get_summary())"`

Expected: æ˜¾ç¤ºåŒ…å«LLMè°ƒç”¨ç»Ÿè®¡çš„æ‘˜è¦

**Step 3: æäº¤**

```bash
git add utils/llm_client.py
git commit -m "feat: integrate performance monitor into LLM client"
```

---

## Task 3: åˆ›å»ºå…¨é‡æµæ°´çº¿è¿è¡Œè„šæœ¬

**Files:**
- Create: `scripts/run_phase3_full_pipeline.py`

**Step 1: åˆ›å»ºè„šæœ¬ä¸»æ¡†æ¶**

```python
#!/usr/bin/env python3
"""
Phase 3: Full Pipeline Execution with Performance Monitoring
è¿è¡Œå®Œæ•´æµæ°´çº¿å¹¶æ”¶é›†æ€§èƒ½æ•°æ®
"""
import sys
import os
import argparse
import json
import logging
from datetime import datetime
from pathlib import Path

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from pipeline.extract_pain import PainPointExtractor
from pipeline.embed import PainEventEmbedder
from pipeline.cluster import PainEventClusterer
from pipeline.score_viability import ViabilityScorer
from pipeline.map_opportunity import OpportunityMapper
from utils.performance_monitor import performance_monitor
from utils.db import db

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def run_phase3_pipeline(limit_posts: int = 100, save_metrics: bool = True):
    """è¿è¡ŒPhase 3å®Œæ•´æµæ°´çº¿"""

    logger.info("=" * 60)
    logger.info("PHASE 3: Full Pipeline Execution")
    logger.info(f"Limit: {limit_posts} posts")
    logger.info("=" * 60)

    results = {}

    # Stage 1: Extract Pain Points
    logger.info("\n[Stage 1/5] Extracting pain points...")
    performance_monitor.start_stage("extract")

    try:
        extractor = PainPointExtractor()
        extract_result = extractor.process_unextracted_posts(limit=limit_posts)
        results["extract"] = extract_result

        performance_monitor.end_stage("extract", extract_result.get("processed", 0))
        logger.info(f"âœ“ Extracted {extract_result.get('pain_events_saved', 0)} pain events")
    except Exception as e:
        logger.error(f"âœ— Extraction failed: {e}")
        performance_monitor.end_stage("extract", 0)
        results["extract"] = {"error": str(e)}

    # Stage 2: Create Embeddings
    logger.info("\n[Stage 2/5] Creating embeddings...")
    performance_monitor.start_stage("embed")

    try:
        embedder = PainEventEmbedder()
        embed_result = embedder.process_missing_embeddings(limit=limit_posts * 2)
        results["embed"] = embed_result

        performance_monitor.end_stage("embed", embed_result.get("embeddings_created", 0))
        logger.info(f"âœ“ Created {embed_result.get('embeddings_created', 0)} embeddings")
    except Exception as e:
        logger.error(f"âœ— Embedding failed: {e}")
        performance_monitor.end_stage("embed", 0)
        results["embed"] = {"error": str(e)}

    # Stage 3: Cluster Pain Events
    logger.info("\n[Stage 3/5] Clustering pain events...")
    performance_monitor.start_stage("cluster")

    try:
        clusterer = PainEventClusterer()
        cluster_result = clusterer.cluster_pain_events(limit=limit_posts * 2)
        results["cluster"] = cluster_result

        performance_monitor.end_stage("cluster", cluster_result.get("clusters_created", 0))
        logger.info(f"âœ“ Created {cluster_result.get('clusters_created', 0)} clusters")
    except Exception as e:
        logger.error(f"âœ— Clustering failed: {e}")
        performance_monitor.end_stage("cluster", 0)
        results["cluster"] = {"error": str(e)}

    # Stage 4: Map Opportunities
    logger.info("\n[Stage 4/5] Mapping opportunities...")
    performance_monitor.start_stage("map_opportunities")

    try:
        mapper = OpportunityMapper()
        map_result = mapper.map_opportunities_for_clusters(limit=50)
        results["map_opportunities"] = map_result

        performance_monitor.end_stage("map_opportunities", map_result.get("opportunities_created", 0))
        logger.info(f"âœ“ Mapped {map_result.get('opportunities_created', 0)} opportunities")
    except Exception as e:
        logger.error(f"âœ— Opportunity mapping failed: {e}")
        performance_monitor.end_stage("map_opportunities", 0)
        results["map_opportunities"] = {"error": str(e)}

    # Stage 5: Score Viability
    logger.info("\n[Stage 5/5] Scoring viability...")
    performance_monitor.start_stage("score")

    try:
        scorer = ViabilityScorer()
        score_result = scorer.score_opportunities(limit=100)
        results["score"] = score_result

        performance_monitor.end_stage("score", score_result.get("opportunities_scored", 0))
        logger.info(f"âœ“ Scored {score_result.get('opportunities_scored', 0)} opportunities")
    except Exception as e:
        logger.error(f"âœ— Viability scoring failed: {e}")
        performance_monitor.end_stage("score", 0)
        results["score"] = {"error": str(e)}

    # Generate Summary
    logger.info("\n" + "=" * 60)
    logger.info("PIPELINE COMPLETE - GENERATING SUMMARY")
    logger.info("=" * 60)

    summary = performance_monitor.get_summary()

    logger.info(f"\nğŸ“Š Performance Summary:")
    logger.info(f"   â€¢ Total Duration: {summary['total_duration_minutes']} minutes")
    logger.info(f"   â€¢ LLM Calls: {summary['total_llm_calls']}")
    logger.info(f"   â€¢ Total Tokens: {summary['total_tokens']:,}")
    logger.info(f"   â€¢ Est. Cost: ${summary['estimated_cost_usd']:.4f} USD")

    logger.info(f"\nğŸ“ˆ Stage Details:")
    for stage_name, stage_stats in summary['stages_summary'].items():
        logger.info(f"   â€¢ {stage_name}:")
        logger.info(f"     - Duration: {stage_stats['duration_seconds']:.1f}s")
        logger.info(f"     - Items: {stage_stats['items_processed']}")
        logger.info(f"     - Tokens: {stage_stats['tokens_used']:,}")

    # Save metrics
    if save_metrics:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        metrics_file = f"docs/reports/phase3_metrics_{timestamp}.json"
        os.makedirs(os.path.dirname(metrics_file), exist_ok=True)
        performance_monitor.save_metrics(metrics_file)
        logger.info(f"\nğŸ’¾ Metrics saved to: {metrics_file}")

    return results, summary


def main():
    parser = argparse.ArgumentParser(description="Phase 3 Full Pipeline Execution")
    parser.add_argument("--limit-posts", type=int, default=100,
                       help="Number of posts to process (default: 100)")
    parser.add_argument("--no-save", action="store_true",
                       help="Don't save metrics to file")

    args = parser.parse_args()

    try:
        results, summary = run_phase3_pipeline(
            limit_posts=args.limit_posts,
            save_metrics=not args.no_save
        )

        # Save results
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        results_file = f"docs/reports/phase3_results_{timestamp}.json"
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump({"results": results, "summary": summary}, f, indent=2)

        logger.info(f"\nâœ… All results saved to: {results_file}")

    except Exception as e:
        logger.error(f"Pipeline execution failed: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
```

**Step 2: èµ‹äºˆæ‰§è¡Œæƒé™å¹¶æµ‹è¯•**

Run: `chmod +x scripts/run_phase3_full_pipeline.py`

Run (dry run with small limit): `python3 scripts/run_phase3_full_pipeline.py --limit-posts 5`

Expected: Pipeline runs through all stages and generates metrics file

**Step 3: æäº¤**

```bash
git add scripts/run_phase3_full_pipeline.py
git commit -m "feat: add Phase 3 full pipeline script with monitoring"
```

---

## Task 4: åˆ›å»ºå•†ä¸šæœºä¼šè¯„ä¼°è„šæœ¬

**Files:**
- Create: `scripts/evaluate_opportunity_reports.py`

**Step 1: åˆ›å»ºè¯„ä¼°è„šæœ¬**

```python
#!/usr/bin/env python3
"""
Phase 3: Evaluate Opportunity Reports
è¯„ä¼°ç”Ÿæˆçš„å•†ä¸šæœºä¼šæŠ¥å‘Šè´¨é‡
"""
import sys
import os
import argparse
import json
import re
from pathlib import Path
from datetime import datetime

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from pain_point_analyzer import PainPointAnalyzer


class OpportunityReportEvaluator:
    """å•†ä¸šæœºä¼šæŠ¥å‘Šè¯„ä¼°å™¨"""

    def __init__(self):
        self.evaluation_metrics = {
            "total_reports": 0,
            "reports_with_comment_evidence": 0,
            "avg_problem_length": 0,
            "reports_with_mvp_suggestions": 0,
            "reports_with_target_users": 0,
            "reports_with_risk_analysis": 0,
            "top_opportunities": []
        }

    def analyze_report(self, report_path: str) -> dict:
        """åˆ†æå•ä¸ªæŠ¥å‘Šæ–‡ä»¶"""
        with open(report_path, 'r', encoding='utf-8') as f:
            content = f.read()

        metrics = {
            "file_path": report_path,
            "has_comment_evidence": False,
            "problem_descriptions": [],
            "has_mvp_suggestion": False,
            "has_target_users": False,
            "has_risk_analysis": False,
            "opportunity_count": 0
        }

        # æ£€æŸ¥æ˜¯å¦å¼•ç”¨è¯„è®ºä½œä¸ºè¯æ®
        comment_patterns = [
            r'è¯„è®º|comment',
            r'evidence.*comment',
            r'æ¥æº.*è¯„è®º|source.*comment'
        ]
        for pattern in comment_patterns:
            if re.search(pattern, content, re.IGNORECASE):
                metrics["has_comment_evidence"] = True
                break

        # æå–é—®é¢˜æè¿°ï¼ˆåœ¨å…¸å‹ç—›ç‚¹äº‹ä»¶éƒ¨åˆ†ï¼‰
        problem_section = re.search(
            r'### å…¸å‹ç—›ç‚¹äº‹ä»¶.*?(?=###|---|$)',
            content,
            re.DOTALL
        )
        if problem_section:
            problems = re.findall(r'\*\*é—®é¢˜\*\*:\s*(.+?)(?=\n|$)', problem_section.group(0))
            metrics["problem_descriptions"] = [p.strip() for p in problems]

        # æ£€æŸ¥MVPå»ºè®®
        mvp_patterns = [
            r'MVP|mvp',
            r'æœ€å°å¯è¡Œäº§å“',
            r'åŠŸèƒ½å»ºè®®|feature.*suggest'
        ]
        for pattern in mvp_patterns:
            if re.search(pattern, content, re.IGNORECASE):
                metrics["has_mvp_suggestion"] = True
                break

        # æ£€æŸ¥ç›®æ ‡ç”¨æˆ·
        user_patterns = [
            r'ç›®æ ‡ç”¨æˆ·|target.*user',
            r'ç”¨æˆ·ç¾¤ä½“|user.*group',
            r'é€‚ç”¨.*äºº|for.*who'
        ]
        for pattern in user_patterns:
            if re.search(pattern, content, re.IGNORECASE):
                metrics["has_target_users"] = True
                break

        # æ£€æŸ¥é£é™©åˆ†æ
        risk_patterns = [
            r'é£é™©|risk',
            r'æŒ‘æˆ˜|challenge',
            r'éšœç¢|barrier'
        ]
        for pattern in risk_patterns:
            if re.search(pattern, content, re.IGNORECASE):
                metrics["has_risk_analysis"] = True
                break

        # è®¡ç®—æœºä¼šæ•°é‡
        opportunity_matches = re.findall(r'\*\*([^*]+)\*\*\s*\(è¯„åˆ†:', content)
        metrics["opportunity_count"] = len(opportunity_matches)

        return metrics

    def evaluate_directory(self, reports_dir: str) -> dict:
        """è¯„ä¼°ç›®å½•ä¸­çš„æ‰€æœ‰æŠ¥å‘Š"""
        reports_path = Path(reports_dir)
        if not reports_path.exists():
            print(f"âŒ Reports directory not found: {reports_dir}")
            return {}

        markdown_files = list(reports_path.glob("*.md"))
        # æ’é™¤READMEæ–‡ä»¶
        markdown_files = [f for f in markdown_files if f.name.lower() != 'readme.md']

        print(f"ğŸ“Š Found {len(markdown_files)} reports to evaluate")

        all_metrics = []

        for report_file in markdown_files:
            print(f"  â€¢ Analyzing: {report_file.name}")
            metrics = self.analyze_report(str(report_file))
            all_metrics.append(metrics)

        # æ±‡æ€»ç»Ÿè®¡
        if all_metrics:
            self.evaluation_metrics["total_reports"] = len(all_metrics)
            self.evaluation_metrics["reports_with_comment_evidence"] = sum(
                1 for m in all_metrics if m["has_comment_evidence"]
            )

            # å¹³å‡é—®é¢˜æè¿°é•¿åº¦
            all_problems = []
            for m in all_metrics:
                all_problems.extend(m["problem_descriptions"])

            if all_problems:
                avg_length = sum(len(p) for p in all_problems) / len(all_problems)
                self.evaluation_metrics["avg_problem_length"] = round(avg_length, 1)

            self.evaluation_metrics["reports_with_mvp_suggestions"] = sum(
                1 for m in all_metrics if m["has_mvp_suggestion"]
            )
            self.evaluation_metrics["reports_with_target_users"] = sum(
                1 for m in all_metrics if m["has_target_users"]
            )
            self.evaluation_metrics["reports_with_risk_analysis"] = sum(
                1 for m in all_metrics if m["has_risk_analysis"]
            )

            # Top 3 opportunities (by opportunity count)
            all_metrics.sort(key=lambda x: x["opportunity_count"], reverse=True)
            self.evaluation_metrics["top_opportunities"] = [
                {
                    "file": m["file_path"],
                    "opportunity_count": m["opportunity_count"]
                }
                for m in all_metrics[:3]
            ]

        return self.evaluation_metrics

    def generate_evaluation_report(self) -> str:
        """ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š"""
        metrics = self.evaluation_metrics

        if metrics["total_reports"] == 0:
            return "# No reports to evaluate\n"

        report = f"""# Phase 3: å•†ä¸šæœºä¼šæŠ¥å‘Šè´¨é‡è¯„ä¼°

**è¯„ä¼°æ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
**è¯„ä¼°æŠ¥å‘Šæ•°é‡**: {metrics['total_reports']}

---

## ğŸ“Š æ€»ä½“è¯„åˆ†

| æŒ‡æ ‡ | æ•°å€¼ | å æ¯” |
|------|------|------|
| **æ€»æŠ¥å‘Šæ•°** | {metrics['total_reports']} | 100% |
| **åŒ…å«è¯„è®ºè¯æ®** | {metrics['reports_with_comment_evidence']} | {metrics['reports_with_comment_evidence']/max(metrics['total_reports'],1)*100:.1f}% |
| **åŒ…å«MVPå»ºè®®** | {metrics['reports_with_mvp_suggestions']} | {metrics['reports_with_mvp_suggestions']/max(metrics['total_reports'],1)*100:.1f}% |
| **æ˜ç¡®ç›®æ ‡ç”¨æˆ·** | {metrics['reports_with_target_users']} | {metrics['reports_with_target_users']/max(metrics['total_reports'],1)*100:.1f}% |
| **åŒ…å«é£é™©åˆ†æ** | {metrics['reports_with_risk_analysis']} | {metrics['reports_with_risk_analysis']/max(metrics['total_reports'],1)*100:.1f}% |
| **å¹³å‡é—®é¢˜æè¿°é•¿åº¦** | {metrics['avg_problem_length']} å­—ç¬¦ | - |

---

## ğŸ¯ å…³é”®å‘ç°

### 1. å¸‚åœºè¯æ®è´¨é‡
{'âœ… ä¼˜ç§€' if metrics['reports_with_comment_evidence']/metrics['total_reports'] > 0.7 else 'âš ï¸ éœ€æ”¹è¿›'} \
- {metrics['reports_with_comment_evidence']}/{metrics['total_reports']} ä»½æŠ¥å‘Šå¼•ç”¨äº†è¯„è®ºä½œä¸ºå¸‚åœºè¯æ®
- è¯„è®ºè¯æ®å¢å¼ºäº†æŠ¥å‘Šçš„è¯´æœåŠ›

### 2. å¯æ“ä½œæ€§è¯„ä¼°
{'âœ… ä¼˜ç§€' if metrics['reports_with_mvp_suggestions']/metrics['total_reports'] > 0.7 else 'âš ï¸ éœ€æ”¹è¿›'} \
- {metrics['reports_with_mvp_suggestions']}/{metrics['total_reports']} ä»½æŠ¥å‘ŠåŒ…å«MVPåŠŸèƒ½å»ºè®®
- {metrics['reports_with_target_users']}/{metrics['total_reports']} ä»½æŠ¥å‘Šæ˜ç¡®äº†ç›®æ ‡ç”¨æˆ·ç¾¤ä½“

### 3. é—®é¢˜æè¿°è´¨é‡
å¹³å‡é—®é¢˜æè¿°é•¿åº¦: **{metrics['avg_problem_length']}** å­—ç¬¦
{'âœ… ä¼˜ç§€ (å…·ä½“)' if metrics['avg_problem_length'] > 50 else 'âš ï¸ éœ€æ”¹è¿› (è¿‡äºç®€ç•¥)'}

### 4. é£é™©æ„è¯†
{'âœ… ä¼˜ç§€' if metrics['reports_with_risk_analysis']/metrics['total_reports'] > 0.5 else 'âš ï¸ éœ€æ”¹è¿›'} \
- {metrics['reports_with_risk_analysis']}/{metrics['total_reports']} ä»½æŠ¥å‘ŠåŒ…å«é£é™©åˆ†æ

---

## ğŸ† Top 3 æœºä¼šæŠ¥å‘Š

"""

        for i, opp in enumerate(metrics["top_opportunities"], 1):
            report_name = Path(opp["file"]).stem
            report_name = report_name.replace("_opportunity_analysis", "").replace("_", " ").title()
            report += f"{i}. **{report_name}**\n"
            report += f"   - æœºä¼šæ•°é‡: {opp['opportunity_count']}\n"
            report += f"   - æ–‡ä»¶: `{opp['file']}`\n\n"

        report += """---

## ğŸ“ å»ºè®®

### ç«‹å³å¯è¡ŒåŠ¨é¡¹
1. **ä¼˜å…ˆçº§æ’åº**: æ ¹æ®æœºä¼šè¯„åˆ†å’Œå¸‚åœºè§„æ¨¡é€‰æ‹©Top 3æœºä¼š
2. **ç”¨æˆ·éªŒè¯**: é’ˆå¯¹Top 3æœºä¼šè¿›è¡Œç”¨æˆ·è®¿è°ˆ
3. **MVPè§„åˆ’**: ä¸ºæœ€é«˜ä»·å€¼æœºä¼šåˆ¶å®š3ä¸ªæœˆMVPå¼€å‘è®¡åˆ’

### è´¨é‡æ”¹è¿›å»ºè®®
"""

        if metrics['reports_with_comment_evidence'] / metrics['total_reports'] < 0.7:
            report += "- æå‡è¯„è®ºè¯æ®å¼•ç”¨ç‡ï¼ˆå½“å‰<70%ï¼‰\n"
        if metrics['avg_problem_length'] < 50:
            report += "- å¢å¼ºé—®é¢˜æè¿°çš„å…·ä½“æ€§ï¼ˆå½“å‰<50å­—ç¬¦ï¼‰\n"
        if metrics['reports_with_mvp_suggestions'] / metrics['total_reports'] < 0.7:
            report += "- è¡¥å……æ›´å¤šMVPåŠŸèƒ½å»ºè®®\n"

        report += "\n---\n\n*æœ¬æŠ¥å‘Šç”± Phase 3 è¯„ä¼°è„šæœ¬è‡ªåŠ¨ç”Ÿæˆ*\n"

        return report


def main():
    parser = argparse.ArgumentParser(description="Evaluate Phase 3 opportunity reports")
    parser.add_argument("--reports-dir", type=str, default="pain_analysis_reports",
                       help="Path to reports directory (default: pain_analysis_reports)")
    parser.add_argument("--output", type=str, default=None,
                       help="Output report path (default: docs/reports/phase3_evaluation_YYYYMMDD.md)")

    args = parser.parse_args()

    print("ğŸ” Starting opportunity report evaluation...")
    print(f"   Reports directory: {args.reports_dir}")

    # Run evaluation
    evaluator = OpportunityReportEvaluator()
    metrics = evaluator.evaluate_directory(args.reports_dir)

    if not metrics:
        print("âŒ No reports found or evaluation failed")
        sys.exit(1)

    # Generate report
    report_content = evaluator.generate_evaluation_report()

    # Save report
    if args.output is None:
        timestamp = datetime.now().strftime("%Y%m%d")
        args.output = f"docs/reports/phase3_evaluation_{timestamp}.md"

    os.makedirs(os.path.dirname(args.output), exist_ok=True)
    with open(args.output, 'w', encoding='utf-8') as f:
        f.write(report_content)

    print(f"\nâœ… Evaluation complete!")
    print(f"   â€¢ Reports evaluated: {metrics['total_reports']}")
    print(f"   â€¢ Evaluation report: {args.output}")


if __name__ == "__main__":
    main()
```

**Step 2: èµ‹äºˆæ‰§è¡Œæƒé™**

Run: `chmod +x scripts/evaluate_opportunity_reports.py`

**Step 3: æµ‹è¯•è„šæœ¬ï¼ˆéœ€è¦å…ˆç”Ÿæˆä¸€äº›æŠ¥å‘Šï¼‰**

Run: `python3 pain_point_analyzer.py --limit 5 --dry-run` ï¼ˆæ£€æŸ¥æ˜¯å¦æœ‰æ•°æ®ï¼‰

**Step 4: æäº¤**

```bash
git add scripts/evaluate_opportunity_reports.py
git commit -m "feat: add opportunity report evaluation script"
```

---

## Task 5: åˆ›å»ºæˆæœ¬æ€§èƒ½åˆ†ææŠ¥å‘Šç”Ÿæˆå™¨

**Files:**
- Create: `scripts/generate_cost_performance_report.py`

**Step 1: åˆ›å»ºæŠ¥å‘Šç”Ÿæˆå™¨**

```python
#!/usr/bin/env python3
"""
Phase 3: Generate Cost & Performance Analysis Report
ç”Ÿæˆæˆæœ¬ä¸æ€§èƒ½åˆ†ææŠ¥å‘Š
"""
import sys
import os
import json
import argparse
from pathlib import Path
from datetime import datetime

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from utils.performance_monitor import PerformanceMonitor


def generate_cost_performance_report(metrics_file: str, output_file: str = None) -> str:
    """ç”Ÿæˆæˆæœ¬ä¸æ€§èƒ½åˆ†ææŠ¥å‘Š"""

    # Load metrics
    monitor = PerformanceMonitor.load_metrics(metrics_file)
    summary = monitor.get_summary()

    # Generate report
    report = f"""# Phase 3: æˆæœ¬ä¸æ€§èƒ½åˆ†ææŠ¥å‘Š

**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
**æ•°æ®æ¥æº**: {metrics_file}

---

## ğŸ“Š æ‰§è¡Œæ‘˜è¦

æœ¬æ¬¡Phase 3è¿è¡Œå¤„ç†äº†å®Œæ•´çš„æ•°æ®æµæ°´çº¿ï¼ŒåŒ…æ‹¬ç—›ç‚¹æŠ½å–ã€å‘é‡åŒ–ã€èšç±»ã€æœºä¼šæ˜ å°„å’Œå¯è¡Œæ€§è¯„åˆ†ã€‚

### å…³é”®æŒ‡æ ‡

| æŒ‡æ ‡ | æ•°å€¼ |
|------|------|
| **æ€»è¿è¡Œæ—¶é—´** | {summary['total_duration_minutes']} åˆ†é’Ÿ |
| **LLM APIè°ƒç”¨æ¬¡æ•°** | {summary['total_llm_calls']:,} |
| **Tokenæ¶ˆè€—æ€»é‡** | {summary['total_tokens']:,} |
| **é¢„ä¼°æˆæœ¬** | ${summary['estimated_cost_usd']:.4f} USD |

---

## ğŸ” é˜¶æ®µè¯¦ç»†åˆ†æ

"""

    # Stage breakdown
    for stage_name, stage_stats in summary['stages_summary'].items():
        stage_name_cn = {
            "extract": "ç—›ç‚¹æŠ½å–",
            "embed": "å‘é‡åŒ–",
            "cluster": "èšç±»",
            "map_opportunities": "æœºä¼šæ˜ å°„",
            "score": "å¯è¡Œæ€§è¯„åˆ†"
        }.get(stage_name, stage_name)

        report += f"""### {stage_name_cn} ({stage_name})

| æŒ‡æ ‡ | æ•°å€¼ |
|------|------|
| **è¿è¡Œæ—¶é•¿** | {stage_stats['duration_seconds']:.1f} ç§’ |
| **å¤„ç†é¡¹ç›®æ•°** | {stage_stats['items_processed']} |
| **LLMè°ƒç”¨æ¬¡æ•°** | {stage_stats['llm_calls']} |
| **Tokenæ¶ˆè€—** | {stage_stats['tokens_used']:,} |
| **æ¯é¡¹ç›®å¹³å‡Token** | {stage_stats['tokens_used'] / max(stage_stats['items_processed'], 1):.0f} |
| **æ¯é¡¹ç›®å¹³å‡æ—¶é—´** | {stage_stats['duration_seconds'] / max(stage_stats['items_processed'], 1):.2f} ç§’ |

"""

    # Cost breakdown
    report += """---

## ğŸ’° æˆæœ¬åˆ†æ

### Tokenæ¶ˆè€—åˆ†å¸ƒï¼ˆæŒ‰é˜¶æ®µï¼‰

| é˜¶æ®µ | Tokenæ¶ˆè€— | å æ¯” |
|------|----------|------|
"""

    total_tokens = summary['total_tokens']
    for stage_name, stage_stats in summary['stages_summary'].items():
        percentage = (stage_stats['tokens_used'] / total_tokens * 100) if total_tokens > 0 else 0
        stage_name_cn = {
            "extract": "ç—›ç‚¹æŠ½å–",
            "embed": "å‘é‡åŒ–",
            "cluster": "èšç±»",
            "map_opportunities": "æœºä¼šæ˜ å°„",
            "score": "å¯è¡Œæ€§è¯„åˆ†"
        }.get(stage_name, stage_name)
        report += f"| {stage_name_cn} | {stage_stats['tokens_used']:,} | {percentage:.1f}% |\n"

    report += f"| **æ€»è®¡** | **{total_tokens:,}** | **100%** |\n"

    # Cost estimation
    report += f"""

### æˆæœ¬é¢„ä¼°ï¼ˆåŸºäºSiliconFlowå®šä»·ï¼‰

| é¡¹ç›® | æ•°å€¼ |
|------|------|
| **æ€»æˆæœ¬** | ${summary['estimated_cost_usd']:.4f} USD |
| **æ¯100ä¸ªå¸–å­æˆæœ¬** | ${(summary['estimated_cost_usd'] / max(summary['stages_summary'].get('extract', {}).get('items_processed', 1), 1)) * 100:.4f} USD |
| **æ¯ä¸ªæœºä¼šæˆæœ¬** | ${(summary['estimated_cost_usd'] / max(summary['stages_summary'].get('map_opportunities', {}).get('items_processed', 1), 1)):.4f} USD |

---

## ğŸ“ˆ æ€§èƒ½æŒ‡æ ‡

### å¤„ç†æ•ˆç‡

| é˜¶æ®µ | ååé‡ |
|------|--------|
"""

    for stage_name, stage_stats in summary['stages_summary'].items():
        if stage_stats['items_processed'] > 0 and stage_stats['duration_seconds'] > 0:
            throughput = stage_stats['items_processed'] / (stage_stats['duration_seconds'] / 60)
            stage_name_cn = {
                "extract": "ç—›ç‚¹æŠ½å–",
                "embed": "å‘é‡åŒ–",
                "cluster": "èšç±»",
                "map_opportunities": "æœºä¼šæ˜ å°„",
                "score": "å¯è¡Œæ€§è¯„åˆ†"
            }.get(stage_name, stage_name)
            report += f"| {stage_name_cn} | {throughput:.1f} é¡¹ç›®/åˆ†é’Ÿ |\n"

    report += """

---

## ğŸ”„ Phase 1 vs Phase 2 vs Phase 3 å¯¹æ¯”

### è´¨é‡æ”¹è¿›ROIåˆ†æ

æ ¹æ®Phase 2è´¨é‡åˆ†æç»“æœï¼š

| æŒ‡æ ‡ | Phase 1 (æ— è¯„è®º) | Phase 2 (å«è¯„è®º) | æ”¹è¿›å¹…åº¦ |
|------|-----------------|-----------------|----------|
| **æ¯å¸–å­ç—›ç‚¹äº‹ä»¶** | 0.2 | 1.8 | **+900%** |
| **é—®é¢˜æè¿°é•¿åº¦** | 8.6 å­—ç¬¦ | 67.4 å­—ç¬¦ | **+684%** |
| **æå–ç½®ä¿¡åº¦** | 0.087 | 0.514 | **+491%** |
| **Tokenæ¶ˆè€—** | ~600 | ~1,500 | +150% |
| **æˆæœ¬å¢åŠ ** | åŸºå‡† | ~2.5x | - |

### ROIè®¡ç®—

å‡è®¾Phase 1å¤„ç†100ä¸ªå¸–å­çš„æˆæœ¬ä¸º $Xï¼š
- Phase 2æˆæœ¬: $2.5X
- è´¨é‡æå‡: 9å€ç—›ç‚¹äº‹ä»¶ Ã— 684% ç‰¹å¼‚æ€§æå‡
- **ROI**: (9 Ã— 6.84) / 2.5 = **24.6x**

**ç»“è®º**: Phase 2çš„é¢å¤–æŠ•å…¥å¸¦æ¥äº†è¿‘25å€çš„å›æŠ¥ï¼ˆä»¥è´¨é‡äº§å‡ºè®¡ï¼‰ã€‚

---

## ğŸ“ ç»“è®ºä¸å»ºè®®

### å…³é”®å‘ç°

1. **æˆæœ¬å¯æ§**: æ¯ä¸ªå¸–å­çš„å®Œæ•´å¤„ç†æˆæœ¬çº¦ä¸º ${summary['estimated_cost_usd'] / max(summary['stages_summary'].get('extract', {}).get('items_processed', 1), 1):.4f} USD
2. **è´¨é‡æ˜¾è‘—**: è¯„è®ºæ„ŸçŸ¥æå–å¸¦æ¥äº†9å€çš„ç—›ç‚¹äº‹ä»¶å‘ç°ç‡
3. **ROIä¼˜ç§€**: è´¨é‡æå‡å¹…åº¦è¿œè¶…æˆæœ¬å¢é•¿å¹…åº¦

### ä¼˜åŒ–å»ºè®®

#### çŸ­æœŸï¼ˆç«‹å³å¯åšï¼‰
1. **æ‰¹é‡æŠ˜æ‰£**: æ£€æŸ¥SiliconFlowæ˜¯å¦æœ‰æ‰¹é‡APIæŠ˜æ‰£
2. **ç¼“å­˜ç­–ç•¥**: å¯¹ç›¸ä¼¼å¸–å­å¤ç”¨æå–ç»“æœ
3. **å¹¶è¡Œå¤„ç†**: åœ¨extracté˜¶æ®µä½¿ç”¨å¤šçº¿ç¨‹/åç¨‹

#### ä¸­æœŸï¼ˆéœ€è¦å¼€å‘ï¼‰
1. **æ¨¡å‹é€‰æ‹©**: åœ¨æ—©æœŸé˜¶æ®µä½¿ç”¨æ›´ä¾¿å®œçš„æ¨¡å‹è¿›è¡Œç­›é€‰
2. **è‡ªé€‚åº”é‡‡æ ·**: æ ¹æ®è¯„è®ºè´¨é‡åŠ¨æ€è°ƒæ•´top_nå‚æ•°
3. **å¢é‡æ›´æ–°**: åªå¤„ç†æ–°å¢/å˜åŒ–çš„å¸–å­

#### é•¿æœŸï¼ˆéœ€è¦æ¶æ„è°ƒæ•´ï¼‰
1. **æœ¬åœ°æ¨¡å‹**: è€ƒè™‘éƒ¨ç½²å¼€æºæ¨¡å‹é™ä½APIæˆæœ¬
2. **åˆ†å±‚å¤„ç†**: ä¸åŒè´¨é‡çš„å¸–å­ä½¿ç”¨ä¸åŒæˆæœ¬çš„ç­–ç•¥
3. **ç»“æœç¼“å­˜**: å»ºç«‹ç¼“å­˜å±‚é¿å…é‡å¤è®¡ç®—

---

## ğŸ“Š åŸå§‹æ•°æ®

å®Œæ•´çš„æ€§èƒ½æŒ‡æ ‡å·²ä¿å­˜åœ¨: `{metrics_file}`

å¦‚éœ€æŸ¥çœ‹è¯¦ç»†æ•°æ®ï¼Œè¯·ä½¿ç”¨:
```bash
cat {metrics_file} | jq .
```

---

*æœ¬æŠ¥å‘Šç”± Phase 3 æˆæœ¬æ€§èƒ½åˆ†æè„šæœ¬è‡ªåŠ¨ç”Ÿæˆ*
"""

    return report


def main():
    parser = argparse.ArgumentParser(description="Generate Phase 3 cost & performance report")
    parser.add_argument("--metrics", type=str, required=True,
                       help="Path to metrics JSON file")
    parser.add_argument("--output", type=str, default=None,
                       help="Output report path (default: docs/reports/phase3_cost_performance_YYYYMMDD.md)")

    args = parser.parse_args()

    if not os.path.exists(args.metrics):
        print(f"âŒ Metrics file not found: {args.metrics}")
        sys.exit(1)

    print("ğŸ“Š Generating cost & performance report...")
    print(f"   Metrics file: {args.metrics}")

    # Generate report
    report_content = generate_cost_performance_report(args.metrics, args.output)

    # Save report
    if args.output is None:
        timestamp = datetime.now().strftime("%Y%m%d")
        args.output = f"docs/reports/phase3_cost_performance_{timestamp}.md"

    os.makedirs(os.path.dirname(args.output), exist_ok=True)
    with open(args.output, 'w', encoding='utf-8') as f:
        f.write(report_content)

    print(f"\nâœ… Report generated successfully!")
    print(f"   Output: {args.output}")


if __name__ == "__main__":
    main()
```

**Step 2: èµ‹äºˆæ‰§è¡Œæƒé™**

Run: `chmod +x scripts/generate_cost_performance_report.py`

**Step 3: æäº¤**

```bash
git add scripts/generate_cost_performance_report.py
git commit -m "feat: add cost & performance report generator"
```

---

## Task 6: åˆ›å»ºPhase 3ä¸»æ‰§è¡Œè„šæœ¬

**Files:**
- Create: `scripts/phase3_master.py`

**Step 1: åˆ›å»ºä¸»æ‰§è¡Œè„šæœ¬**

```python
#!/usr/bin/env python3
"""
Phase 3 Master Script
æ‰§è¡Œå®Œæ•´çš„Phase 3æµç¨‹ï¼šæ•°æ®å¤„ç† â†’ æŠ¥å‘Šç”Ÿæˆ â†’ è´¨é‡è¯„ä¼° â†’ æˆæœ¬åˆ†æ
"""
import sys
import os
import argparse
import json
import subprocess
from pathlib import Path
from datetime import datetime

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))


def run_command(cmd: list, description: str) -> bool:
    """è¿è¡Œå‘½ä»¤å¹¶æ˜¾ç¤ºè¿›åº¦"""
    print(f"\n{'='*60}")
    print(f"ğŸ”„ {description}")
    print(f"{'='*60}")

    result = subprocess.run(cmd, capture_output=False)
    success = result.returncode == 0

    if success:
        print(f"âœ… {description} - å®Œæˆ")
    else:
        print(f"âŒ {description} - å¤±è´¥")

    return success


def main():
    parser = argparse.ArgumentParser(description="Phase 3 Master: Complete Evaluation Pipeline")
    parser.add_argument("--limit-posts", type=int, default=100,
                       help="Number of posts to process (default: 100)")
    parser.add_argument("--skip-pipeline", action="store_true",
                       help="Skip pipeline execution (use existing metrics)")
    parser.add_argument("--metrics-file", type=str, default=None,
                       help="Path to existing metrics file (if --skip-pipeline)")
    parser.add_argument("--min-score", type=float, default=0.8,
                       help="Minimum opportunity score for reports (default: 0.8)")
    parser.add_argument("--report-limit", type=int, default=10,
                       help="Number of opportunity reports to generate (default: 10)")

    args = parser.parse_args()

    print("="*60)
    print("PHASE 3: å…¨é¢è¿è¡Œã€è¯„ä¼°ä¸ä¼˜åŒ–")
    print("="*60)
    print(f"å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"å¤„ç†è§„æ¨¡: {args.limit_posts} ä¸ªå¸–å­")

    metrics_file = args.metrics_file
    steps_completed = []
    steps_failed = []

    # Step 1: Run full pipeline with monitoring
    if not args.skip_pipeline:
        if run_command(
            ["python3", "scripts/run_phase3_full_pipeline.py", "--limit-posts", str(args.limit_posts)],
            "æ­¥éª¤ 1/4: è¿è¡Œå®Œæ•´æ•°æ®å¤„ç†æµæ°´çº¿"
        ):
            steps_completed.append("æ•°æ®å¤„ç†æµæ°´çº¿")
            # Find the latest metrics file
            reports_dir = project_root / "docs" / "reports"
            if reports_dir.exists():
                metrics_files = list(reports_dir.glob("phase3_metrics_*.json"))
                if metrics_files:
                    metrics_file = str(max(metrics_files, key=os.path.getctime))
                    print(f"   ğŸ“ Metrics file: {metrics_file}")
        else:
            steps_failed.append("æ•°æ®å¤„ç†æµæ°´çº¿")
            print("âŒ æµæ°´çº¿æ‰§è¡Œå¤±è´¥ï¼Œç»ˆæ­¢åç»­æ­¥éª¤")
            sys.exit(1)
    else:
        if not metrics_file or not os.path.exists(metrics_file):
            print(f"âŒ Metrics file not found: {metrics_file}")
            sys.exit(1)
        print(f"   ğŸ“ Using existing metrics: {metrics_file}")
        steps_completed.append("æ•°æ®å¤„ç†æµæ°´çº¿ (è·³è¿‡)")

    # Step 2: Generate opportunity reports
    if run_command(
        ["python3", "pain_point_analyzer.py", "--limit", str(args.report_limit), "--min-score", str(args.min_score)],
        "æ­¥éª¤ 2/4: ç”Ÿæˆå•†ä¸šæœºä¼šè¯„ä¼°æŠ¥å‘Š"
    ):
        steps_completed.append("å•†ä¸šæœºä¼šæŠ¥å‘Šç”Ÿæˆ")
    else:
        steps_failed.append("å•†ä¸šæœºä¼šæŠ¥å‘Šç”Ÿæˆ")

    # Step 3: Evaluate opportunity reports
    if run_command(
        ["python3", "scripts/evaluate_opportunity_reports.py"],
        "æ­¥éª¤ 3/4: è¯„ä¼°å•†ä¸šæœºä¼šæŠ¥å‘Šè´¨é‡"
    ):
        steps_completed.append("å•†ä¸šæœºä¼šæŠ¥å‘Šè´¨é‡è¯„ä¼°")
    else:
        steps_failed.append("å•†ä¸šæœºä¼šæŠ¥å‘Šè´¨é‡è¯„ä¼°")

    # Step 4: Generate cost & performance report
    if metrics_file and os.path.exists(metrics_file):
        if run_command(
            ["python3", "scripts/generate_cost_performance_report.py", "--metrics", metrics_file],
            "æ­¥éª¤ 4/4: ç”Ÿæˆæˆæœ¬ä¸æ€§èƒ½åˆ†ææŠ¥å‘Š"
        ):
            steps_completed.append("æˆæœ¬ä¸æ€§èƒ½åˆ†ææŠ¥å‘Š")
        else:
            steps_failed.append("æˆæœ¬ä¸æ€§èƒ½åˆ†ææŠ¥å‘Š")
    else:
        print("âš ï¸  è·³è¿‡æˆæœ¬åˆ†æï¼ˆæ— metricsæ–‡ä»¶ï¼‰")
        steps_failed.append("æˆæœ¬ä¸æ€§èƒ½åˆ†ææŠ¥å‘Š (è·³è¿‡)")

    # Summary
    print("\n" + "="*60)
    print("PHASE 3 æ‰§è¡Œæ‘˜è¦")
    print("="*60)
    print(f"å®Œæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

    print(f"\nâœ… å®Œæˆçš„æ­¥éª¤ ({len(steps_completed)}):")
    for step in steps_completed:
        print(f"   â€¢ {step}")

    if steps_failed:
        print(f"\nâŒ å¤±è´¥çš„æ­¥éª¤ ({len(steps_failed)}):")
        for step in steps_failed:
            print(f"   â€¢ {step}")

    print(f"\nğŸ“ ç”Ÿæˆçš„æŠ¥å‘Šä½äº: docs/reports/")
    print(f"   â€¢ phase3_results_*.json - æµæ°´çº¿æ‰§è¡Œç»“æœ")
    print(f"   â€¢ phase3_metrics_*.json - æ€§èƒ½æŒ‡æ ‡")
    print(f"   â€¢ phase3_evaluation_*.md - å•†ä¸šæœºä¼šæŠ¥å‘Šè´¨é‡è¯„ä¼°")
    print(f"   â€¢ phase3_cost_performance_*.md - æˆæœ¬ä¸æ€§èƒ½åˆ†æ")
    print(f"   â€¢ pain_analysis_reports/ - å•†ä¸šæœºä¼šè¯„ä¼°æŠ¥å‘Š")

    print("\nğŸ‰ Phase 3 æ‰§è¡Œå®Œæˆï¼")

    # Return exit code based on failures
    sys.exit(0 if not steps_failed else 1)


if __name__ == "__main__":
    main()
```

**Step 2: èµ‹äºˆæ‰§è¡Œæƒé™**

Run: `chmod +x scripts/phase3_master.py`

**Step 3: æäº¤**

```bash
git add scripts/phase3_master.py
git commit -m "feat: add Phase 3 master execution script"
```

---

## Task 7: æ‰§è¡ŒPhase 3ï¼ˆå°è§„æ¨¡æµ‹è¯•ï¼‰

**Files:**
- Test: All created scripts

**Step 1: å‡†å¤‡æµ‹è¯•ç¯å¢ƒ**

Run: `ls -la pain_analysis_reports/` ï¼ˆæ£€æŸ¥æ˜¯å¦æœ‰ç°æœ‰æŠ¥å‘Šï¼‰

Run: `sqlite3 data/wise_collection.db "SELECT COUNT(*) FROM filtered_posts WHERE pain_score >= 0.3"` ï¼ˆæ£€æŸ¥å¯å¤„ç†æ•°æ®ï¼‰

**Step 2: è¿è¡Œå°è§„æ¨¡æµ‹è¯•ï¼ˆ5ä¸ªå¸–å­ï¼‰**

Run: `python3 scripts/phase3_master.py --limit-posts 5 --report-limit 3`

Expected: æ‰€æœ‰4ä¸ªæ­¥éª¤æˆåŠŸå®Œæˆï¼Œç”ŸæˆæŠ¥å‘Šæ–‡ä»¶

**Step 3: æ£€æŸ¥ç”Ÿæˆçš„æŠ¥å‘Š**

Run: `ls -lah docs/reports/ | grep phase3`

Expected: çœ‹åˆ°phase3ç›¸å…³çš„æŠ¥å‘Šæ–‡ä»¶

**Step 4: æ‰‹åŠ¨éªŒè¯æŠ¥å‘Šè´¨é‡**

Run: `cat docs/reports/phase3_evaluation_*.md` ï¼ˆæŸ¥çœ‹è¯„ä¼°æŠ¥å‘Šï¼‰

Run: `ls pain_analysis_reports/*.md | head -3` ï¼ˆæŸ¥çœ‹ç”Ÿæˆçš„æœºä¼šæŠ¥å‘Šï¼‰

**Step 5: æäº¤æµ‹è¯•ç»“æœ**

```bash
git add docs/reports/
git commit -m "test: add Phase 3 small-scale test results"
```

---

## Task 8: æ‰§è¡ŒPhase 3ï¼ˆä¸­ç­‰è§„æ¨¡ï¼Œ100ä¸ªå¸–å­ï¼‰

**Files:**
- Execution: Production run

**Step 1: è¿è¡Œä¸­ç­‰è§„æ¨¡æµ‹è¯•**

Run: `python3 scripts/phase3_master.py --limit-posts 100 --report-limit 10`

Expected Time: çº¦30-60åˆ†é’Ÿï¼ˆå–å†³äºæ•°æ®é‡å’ŒAPIå“åº”é€Ÿåº¦ï¼‰

**Step 2: ç›‘æ§æ‰§è¡Œè¿›åº¦**

è§‚å¯Ÿè¾“å‡ºæ—¥å¿—ï¼Œç¡®ä¿æ¯ä¸ªé˜¶æ®µæ­£å¸¸æ‰§è¡Œ

**Step 3: å®Œæˆåæ£€æŸ¥ç»“æœ**

Run: `cat docs/reports/phase3_evaluation_*.md`

Run: `cat docs/reports/phase3_cost_performance_*.md`

**Step 4: ç”Ÿæˆæœ€ç»ˆæ‘˜è¦æŠ¥å‘Š**

æ‰‹åŠ¨åˆ›å»º `docs/reports/phase3_final_summary.md`ï¼ŒåŒ…å«ï¼š
- æ‰§è¡Œæ‘˜è¦
- å…³é”®å‘ç°
- ROIåˆ†æ
- å»ºè®®å’Œä¸‹ä¸€æ­¥

**Step 5: æäº¤æœ€ç»ˆç»“æœ**

```bash
git add docs/reports/ pain_analysis_reports/
git commit -m "docs: add Phase 3 full evaluation results (100 posts)"
```

---

## Task 9: æ–‡æ¡£å’Œæ€»ç»“

**Files:**
- Create: `docs/reports/phase3_final_summary.md`

**Step 1: åˆ›å»ºæœ€ç»ˆæ€»ç»“æŠ¥å‘Š**

```markdown
# Phase 3: å…¨é¢è¿è¡Œã€è¯„ä¼°ä¸ä¼˜åŒ– - æœ€ç»ˆæ€»ç»“æŠ¥å‘Š

**æ‰§è¡Œæ—¥æœŸ**: 2025-12-25
**æ•°æ®å¤„ç†è§„æ¨¡**: 100ä¸ªå¸–å­
**æ‰§è¡ŒçŠ¶æ€**: âœ… å®Œæˆ

---

## ğŸ¯ ç›®æ ‡å›é¡¾

Phase 3çš„æ ¸å¿ƒç›®æ ‡ï¼š
1. âœ… åœ¨å…¨é‡æ•°æ®ä¸Šéƒ¨ç½²æ–°æµç¨‹
2. âœ… è¯„ä¼°"å•†ä¸šæœºä¼šæŠ¥å‘Š"çš„ä»·å€¼
3. âœ… é‡åŒ–æ–°æµç¨‹å¸¦æ¥çš„æˆæœ¬å˜åŒ–

---

## ğŸ“Š ä¸»è¦æˆæœ

### 1. å…¨é‡æ•°æ®å¤„ç†

- **å¤„ç†è§„æ¨¡**: 100ä¸ªReddit/HNå¸–å­
- **ç—›ç‚¹äº‹ä»¶**: [æ•°é‡]ä¸ªï¼ˆPhase 2å¢å¼ºåï¼‰
- **èšç±»æ•°é‡**: [æ•°é‡]ä¸ª
- **å•†ä¸šæœºä¼š**: [æ•°é‡]ä¸ª

### 2. å•†ä¸šä»·å€¼è¯„ä¼°

#### Top 3 å•†ä¸šæœºä¼šï¼ˆæ ¹æ®æœ€ç»ˆæŠ¥å‘Šå¡«å†™ï¼‰

1. **[æœºä¼šåç§°]** - è¯„åˆ†: [åˆ†æ•°]
   - ç›®æ ‡ç”¨æˆ·: [ç”¨æˆ·ç¾¤ä½“]
   - å¸‚åœºè¯æ®: [è¯„è®ºå¼•ç”¨æƒ…å†µ]

2. **[æœºä¼šåç§°]** - è¯„åˆ†: [åˆ†æ•°]
   - ç›®æ ‡ç”¨æˆ·: [ç”¨æˆ·ç¾¤ä½“]
   - å¸‚åœºè¯æ®: [è¯„è®ºå¼•ç”¨æƒ…å†µ]

3. **[æœºä¼šåç§°]** - è¯„åˆ†: [åˆ†æ•°]
   - ç›®æ ‡ç”¨æˆ·: [ç”¨æˆ·ç¾¤ä½“]
   - å¸‚åœºè¯æ®: [è¯„è®ºå¼•ç”¨æƒ…å†µ]

#### æŠ¥å‘Šè´¨é‡æŒ‡æ ‡

| æŒ‡æ ‡ | æ•°å€¼ | è¯„ä»· |
|------|------|------|
| åŒ…å«è¯„è®ºè¯æ® | [X]% | [ä¼˜ç§€/éœ€æ”¹è¿›] |
| å¹³å‡é—®é¢˜æè¿°é•¿åº¦ | [X] å­—ç¬¦ | [ä¼˜ç§€/éœ€æ”¹è¿›] |
| åŒ…å«MVPå»ºè®® | [X]% | [ä¼˜ç§€/éœ€æ”¹è¿›] |
| æ˜ç¡®ç›®æ ‡ç”¨æˆ· | [X]% | [ä¼˜ç§€/éœ€æ”¹è¿›] |
| åŒ…å«é£é™©åˆ†æ | [X]% | [ä¼˜ç§€/éœ€æ”¹è¿›] |

### 3. æˆæœ¬ä¸æ€§èƒ½åˆ†æ

#### Tokenæ¶ˆè€—

| é˜¶æ®µ | Tokenæ¶ˆè€— | å æ¯” | æˆæœ¬ |
|------|----------|------|------|
| ç—›ç‚¹æŠ½å– | [æ•°é‡] | [X]% | $[é‡‘é¢] |
| å‘é‡åŒ– | [æ•°é‡] | [X]% | $[é‡‘é¢] |
| èšç±» | [æ•°é‡] | [X]% | $[é‡‘é¢] |
| æœºä¼šæ˜ å°„ | [æ•°é‡] | [X]% | $[é‡‘é¢] |
| å¯è¡Œæ€§è¯„åˆ† | [æ•°é‡] | [X]% | $[é‡‘é¢] |
| **æ€»è®¡** | **[æ•°é‡]** | **100%** | **$[é‡‘é¢]** |

#### æ€§èƒ½æŒ‡æ ‡

- **æ€»è¿è¡Œæ—¶é—´**: [X] åˆ†é’Ÿ
- **å¤„ç†ååé‡**: [X] å¸–å­/åˆ†é’Ÿ
- **LLMè°ƒç”¨æ¬¡æ•°**: [X] æ¬¡
- **æ¯å¸–å­æˆæœ¬**: $[X] USD

---

## ğŸ“ˆ Phase 1 vs Phase 2 vs Phase 3 å¯¹æ¯”

### è´¨é‡æå‡

| æŒ‡æ ‡ | Phase 1 | Phase 2 | Phase 3 (å®é™…) |
|------|---------|---------|---------------|
| ç—›ç‚¹äº‹ä»¶/å¸–å­ | 0.2 | 1.8 | [X] |
| é—®é¢˜æè¿°é•¿åº¦ | 8.6 | 67.4 | [X] |
| æå–ç½®ä¿¡åº¦ | 0.087 | 0.514 | [X] |

### æˆæœ¬å˜åŒ–

| é¡¹ç›® | Phase 1 | Phase 2 | Phase 3 (å®é™…) |
|------|---------|---------|---------------|
| Token/å¸–å­ | ~600 | ~1,500 | [X] |
| æˆæœ¬/å¸–å­ | $[X] | $[X] | $[X] |
| æ€»æˆæœ¬ (100å¸–å­) | $[X] | $[X] | $[X] |

### ROIåˆ†æ

**Phase 2 ROI**: 24.6x (åŸºäºPhase 2æ•°æ®)
**Phase 3 ROI**: [è®¡ç®—åŸºäºå®é™…æ•°æ®]

---

## âœ… å¯éªŒè¯æˆæœ

### 1. å…¨é‡æœºä¼šæŠ¥å‘Š
- ğŸ“ ä½ç½®: `pain_analysis_reports/`
- ğŸ“Š æ•°é‡: [X] ä»½æŠ¥å‘Š
- â­ è´¨é‡: [è¯„ä»·]

### 2. æ€§èƒ½ä¸æˆæœ¬åˆ†ææŠ¥å‘Š
- ğŸ“ ä½ç½®: `docs/reports/phase3_cost_performance_*.md`
- ğŸ“Š åŒ…å«: Tokenæ¶ˆè€—ã€æˆæœ¬åˆ†æã€æ€§èƒ½æŒ‡æ ‡ã€ROIè®¡ç®—

### 3. å•†ä¸šä»·å€¼è¯„ä¼°æŠ¥å‘Š
- ğŸ“ ä½ç½®: `docs/reports/phase3_evaluation_*.md`
- ğŸ“Š åŒ…å«: è´¨é‡æŒ‡æ ‡ã€Topæœºä¼šã€æ”¹è¿›å»ºè®®

---

## ğŸ¯ å…³é”®å‘ç°

### 1. å•†ä¸šæœºä¼šè´¨é‡
[å¡«å†™åŸºäºå®é™…æ•°æ®çš„å‘ç°]

### 2. æˆæœ¬æ•ˆç›Š
[å¡«å†™åŸºäºå®é™…æ•°æ®çš„å‘ç°]

### 3. æŠ¥å‘Šå¯æ“ä½œæ€§
[å¡«å†™åŸºäºå®é™…æ•°æ®çš„å‘ç°]

---

## ğŸ’¡ å»ºè®®ä¸ä¸‹ä¸€æ­¥

### ç«‹å³å¯è¡Œ
1. [åŸºäºå®é™…æ•°æ®çš„å»ºè®®]
2. [åŸºäºå®é™…æ•°æ®çš„å»ºè®®]
3. [åŸºäºå®é™…æ•°æ®çš„å»ºè®®]

### ä¸­æœŸä¼˜åŒ–
1. [åŸºäºå®é™…æ•°æ®çš„å»ºè®®]
2. [åŸºäºå®é™…æ•°æ®çš„å»ºè®®]

### é•¿æœŸè§„åˆ’
1. [åŸºäºå®é™…æ•°æ®çš„å»ºè®®]
2. [åŸºäºå®é™…æ•°æ®çš„å»ºè®®]

---

## ğŸ“ ç»“è®º

Phase 3æˆåŠŸå®Œæˆäº†å…¨é¢è¯„ä¼°ï¼š

âœ… **æ•°æ®å¤„ç†**: åœ¨[æ•°é‡]ä¸ªå¸–å­ä¸ŠéªŒè¯äº†æ–°æµç¨‹
âœ… **å•†ä¸šä»·å€¼**: ç”Ÿæˆäº†[æ•°é‡]ä»½é«˜è´¨é‡çš„å•†ä¸šæœºä¼šæŠ¥å‘Š
âœ… **æˆæœ¬åˆ†æ**: é‡åŒ–äº†æ–°æµç¨‹çš„æˆæœ¬å˜åŒ–ï¼ŒROIä¸º[X]å€

**æ€»ä½“è¯„ä»·**: [æˆåŠŸ/éƒ¨åˆ†æˆåŠŸ/éœ€è¦æ”¹è¿›]

[è¡¥å……è¯¦ç»†è¯„ä»·]

---

*æŠ¥å‘Šç”Ÿæˆæ—¶é—´: [datetime]*
*æŠ¥å‘Šç”Ÿæˆè€…: Phase 3 è‡ªåŠ¨åŒ–æµæ°´çº¿*
```

**Step 2: æäº¤æœ€ç»ˆæ–‡æ¡£**

```bash
git add docs/reports/phase3_final_summary.md
git commit -m "docs: add Phase 3 final summary report"
```

---

## ğŸ“‹ éªŒæ”¶æ ‡å‡†

Phase 3å®Œæˆéœ€æ»¡è¶³ä»¥ä¸‹æ ‡å‡†ï¼š

### åŠŸèƒ½æ€§
- âœ… æ€§èƒ½ç›‘æ§è„šæœ¬æ­£å¸¸å·¥ä½œ
- âœ… å®Œæ•´æµæ°´çº¿å¯ä»¥æˆåŠŸè¿è¡Œ
- âœ… å•†ä¸šæœºä¼šæŠ¥å‘ŠæˆåŠŸç”Ÿæˆ
- âœ… è¯„ä¼°è„šæœ¬è¾“å‡ºå‡†ç¡®çš„è´¨é‡æŒ‡æ ‡
- âœ… æˆæœ¬æŠ¥å‘ŠåŒ…å«å®Œæ•´çš„Tokenå’Œæˆæœ¬åˆ†æ

### è´¨é‡æ€§
- âœ… è‡³å°‘ç”Ÿæˆ5ä»½ä»¥ä¸Šçš„å•†ä¸šæœºä¼šæŠ¥å‘Š
- âœ… æŠ¥å‘Šè´¨é‡è¯„ä¼°æ˜¾ç¤ºï¼š
  - >70%æŠ¥å‘ŠåŒ…å«è¯„è®ºè¯æ®
  - å¹³å‡é—®é¢˜æè¿°é•¿åº¦>50å­—ç¬¦
  - >50%æŠ¥å‘ŠåŒ…å«MVPå»ºè®®

### æ–‡æ¡£æ€§
- âœ… æ€§èƒ½æŠ¥å‘Šæ–‡æ¡£å®Œæ•´
- âœ… æˆæœ¬åˆ†ææ•°æ®å‡†ç¡®
- âœ… æœ€ç»ˆæ€»ç»“æŠ¥å‘Šæ¸…æ™°

---

## ğŸ¯ æ‰§è¡Œé€‰é¡¹

å®Œæˆè®¡åˆ’åï¼Œæä¾›æ‰§è¡Œé€‰é¡¹ï¼š

**Plan complete and saved to `docs/plans/2025-12-25-phase3-full-evaluation.md`.**

**Two execution options:**

**1. Subagent-Driven (this session)** - æˆ‘åˆ†æ´¾ä»»åŠ¡ç»™å­ä»£ç†ï¼Œé€æ­¥æ‰§è¡Œå¹¶å®¡æŸ¥
   - é€‚åˆï¼šéœ€è¦é€æ­¥éªŒè¯å’Œè°ƒè¯•
   - æ—¶é—´ï¼šè¾ƒæ…¢ï¼Œä½†æ›´å¯æ§

**2. Direct Execution (I implement directly)** - æˆ‘ç›´æ¥æŒ‰è®¡åˆ’é€æ­¥å®æ–½
   - é€‚åˆï¼šå¿«é€Ÿå®æ–½ï¼Œä»»åŠ¡æ˜ç¡®
   - æ—¶é—´ï¼šæ›´å¿«

**Which approach?**
