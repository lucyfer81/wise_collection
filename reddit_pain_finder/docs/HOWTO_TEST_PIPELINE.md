# å¦‚ä½•æµ‹è¯•å…¨æµç¨‹Pipeline (How to Test Full Pipeline)

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0
**æœ€åæ›´æ–°**: 2026-01-13

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### æ–¹æ³•1: ä½¿ç”¨è‡ªåŠ¨åŒ–æµ‹è¯•è„šæœ¬ (æ¨è)

```bash
# è¿è¡Œå®Œæ•´æµ‹è¯• (é»˜è®¤limitsï¼Œé€‚åˆå¿«é€ŸéªŒè¯)
./scripts/run_full_pipeline_test.sh

# è¿è¡Œå®Œæ•´æµ‹è¯• (å¤„ç†æ‰€æœ‰æ•°æ®)
./scripts/run_full_pipeline_test.sh --process-all

# è¿è¡Œå¢é‡æµ‹è¯• (åªå¤„ç†æ–°æ•°æ®)
./scripts/run_full_pipeline_test.sh --incremental
```

**è„šæœ¬ä¼šè‡ªåŠ¨**:
1. âœ… æ£€æŸ¥ç¯å¢ƒ (Pythonç‰ˆæœ¬ã€ä¾èµ–ã€é…ç½®æ–‡ä»¶)
2. âœ… å¤‡ä»½å½“å‰æ•°æ®
3. âœ… æ˜¾ç¤ºå½“å‰æ•°æ®åº“çŠ¶æ€
4. âœ… è¿è¡Œå®Œæ•´pipeline
5. âœ… æ˜¾ç¤ºç»“æœç»Ÿè®¡
6. âœ… éªŒè¯å’Œæ¨è

---

### æ–¹æ³•2: æ‰‹åŠ¨è¿è¡ŒPipeline

#### Step 1: æ£€æŸ¥ç¯å¢ƒ

```bash
# æ£€æŸ¥Pythonç‰ˆæœ¬ (éœ€è¦ >= 3.10)
python --version

# æ£€æŸ¥ä¾èµ–
python -c "import chromadb; print('ChromaDB OK')"
python -c "import yaml; print('PyYAML OK')"

# æ£€æŸ¥é…ç½®æ–‡ä»¶
ls -lh config/llm.yaml
ls -lh config/thresholds.yaml

# æ£€æŸ¥æ•°æ®åº“
ls -lh data/wise_collection.db
ls -lh data/chroma_db/
```

#### Step 2: å¤‡ä»½æ•°æ® (é‡è¦!)

```bash
# åˆ›å»ºå¤‡ä»½ç›®å½•
mkdir -p backups/backup_$(date +%Y%m%d_%H%M%S)

# å¤‡ä»½æ•°æ®åº“
cp data/wise_collection.db backups/backup_$(date +%Y%m%d_%H%M%S)/

# å¤‡ä»½Chroma (å¦‚æœå­˜åœ¨)
tar -czf backups/backup_$(date +%Y%m%d_%H%M%S)/chroma_db.tar.gz -C data/ chroma_db

echo "Backup completed!"
```

#### Step 3: æŸ¥çœ‹å½“å‰çŠ¶æ€

```bash
# æŸ¥çœ‹æ•°æ®åº“ç»Ÿè®¡
sqlite3 data/wise_collection.db <<EOF
SELECT
    'Raw posts: ' || COUNT(*) as stat FROM posts
UNION ALL
SELECT '    Filtered: ' || COUNT(*) FROM filtered_posts
UNION ALL
SELECT '    Pain events: ' || COUNT(*) FROM pain_events
UNION ALL
SELECT '    Clusters: ' || COUNT(*) FROM clusters
UNION ALL
SELECT '    Opportunities: ' || COUNT(*) FROM opportunities;
EOF

# æŸ¥çœ‹lifecycleçŠ¶æ€
sqlite3 data/wise_collection.db <<EOF
SELECT
    lifecycle_stage,
    COUNT(*) as count,
    ROUND(COUNT(*) * 100.0 / (SELECT COUNT(*) FROM pain_events), 1) || '%' as percentage
FROM pain_events
GROUP BY lifecycle_stage;
EOF
```

#### Step 4: è¿è¡ŒPipeline

```bash
# é€‰é¡¹A: è¿è¡Œå®Œæ•´pipeline (æ¨è)
python run_pipeline.py --stage all --process-all --save-results --enable-monitoring

# é€‰é¡¹B: è¿è¡Œpipeline (ä½¿ç”¨é»˜è®¤limitsï¼Œæ›´å¿«)
python run_pipeline.py --stage all --save-results --enable-monitoring

# é€‰é¡¹C: è¿è¡Œpipeline (å¢é‡æ¨¡å¼ï¼Œåªå¤„ç†æ–°æ•°æ®)
python run_pipeline.py --stage all --save-results --enable-monitoring
```

**å‚æ•°è¯´æ˜**:
- `--stage all`: è¿è¡Œæ‰€æœ‰stage (fetch â†’ filter â†’ extract â†’ embed â†’ cluster â†’ map â†’ score â†’ decision)
- `--process-all`: å¤„ç†æ‰€æœ‰æ•°æ® (ä¸ä½¿ç”¨é»˜è®¤limits)
- `--save-results`: ä¿å­˜è¿è¡Œç»“æœåˆ°JSONæ–‡ä»¶
- `--enable-monitoring`: å¯ç”¨æ€§èƒ½ç›‘æ§ (LLMè°ƒç”¨æ¬¡æ•°ã€tokenä½¿ç”¨ã€æˆæœ¬)

#### Step 5: ç›‘æ§è¿è¡Œè¿›åº¦

**åœ¨å¦ä¸€ä¸ªç»ˆç«¯çª—å£ä¸­**:

```bash
# å®æ—¶æŸ¥çœ‹æ—¥å¿—
tail -f logs/pipeline.log

# æˆ–æŸ¥çœ‹æœ€å50è¡Œ
tail -n 50 logs/pipeline.log

# ç›‘æ§æ•°æ®åº“å¤§å°å˜åŒ–
watch -n 30 'du -sh data/wise_collection.db data/chroma_db/'

# ç›‘æ§è¿›ç¨‹
ps aux | grep run_pipeline

# ç›‘æ§ç³»ç»Ÿèµ„æº
htop
# æˆ–
top
```

#### Step 6: æ£€æŸ¥ç»“æœ

```bash
# æŸ¥çœ‹æœ€æ–°ç»“æœæ–‡ä»¶
ls -lt pipeline_results_*.json | head -1
LATEST=$(ls -t pipeline_results_*.json | head -1)

# æŸ¥çœ‹ç»“æœæ‘˜è¦
cat $LATEST | jq '.final_summary'

# æŸ¥çœ‹å„stageç»Ÿè®¡
cat $LATEST | jq '.stage_results'

# æŸ¥çœ‹æ€§èƒ½æŒ‡æ ‡
cat $LATEST | jq '.performance'

# æŸ¥çœ‹top opportunities
cat $LATEST | jq '.final_summary.top_opportunities'
```

---

## ğŸ“Š éªŒè¯æ£€æŸ¥æ¸…å•

### 1. æ•°æ®å®Œæ•´æ€§æ£€æŸ¥

```bash
# æ£€æŸ¥postsæ˜¯å¦å¢åŠ 
sqlite3 data/wise_collection.db "SELECT COUNT(*) FROM posts;"

# æ£€æŸ¥filtered_postsæ˜¯å¦å¢åŠ 
sqlite3 data/wise_collection.db "SELECT COUNT(*) FROM filtered_posts;"

# æ£€æŸ¥pain_eventsæ˜¯å¦å¢åŠ 
sqlite3 data/wise_collection.db "SELECT COUNT(*) FROM pain_events;"

# æ£€æŸ¥clustersæ˜¯å¦æ›´æ–°
sqlite3 data/wise_collection.db "SELECT COUNT(*) FROM clusters;"

# æ£€æŸ¥opportunitiesæ˜¯å¦å¢åŠ 
sqlite3 data/wise_collection.db "SELECT COUNT(*) FROM opportunities;"
```

### 2. LifecycleçŠ¶æ€æ£€æŸ¥

```bash
# æ£€æŸ¥active vs orphanæ¯”ä¾‹
sqlite3 data/wise_collection.db <<EOF
SELECT
    'Active (in clusters): ' || COUNT(*) FILTER (WHERE lifecycle_stage = 'active') as stat
FROM pain_events
UNION ALL
SELECT
    'Orphans (will be deleted): ' || COUNT(*) FILTER (WHERE lifecycle_stage = 'orphan')
FROM pain_events
UNION ALL
SELECT
    'Retention rate: ' || ROUND(COUNT(*) FILTER (WHERE lifecycle_stage = 'active') * 100.0 / COUNT(*), 1) || '%'
FROM pain_events;
EOF
```

**é¢„æœŸç»“æœ**:
- Active eventsåº”è¯¥å 60-70%
- Orphan eventsåº”è¯¥å 30-40%
- Retention rateåº”è¯¥åœ¨60-70%

### 3. Chromaæ•°æ®ä¸€è‡´æ€§

```bash
# è¿è¡Œä¸€è‡´æ€§æµ‹è¯•
python tests/test_05_data_consistency.py
```

**é¢„æœŸç»“æœ**: âœ… All checks passed

### 4. æ€§èƒ½æŒ‡æ ‡æ£€æŸ¥

```bash
# ä»ç»“æœæ–‡ä»¶ä¸­æå–æ€§èƒ½æ•°æ®
LATEST=$(ls -t pipeline_results_*.json | head -1)
python <<EOF
import json

with open('$LATEST', 'r') as f:
    results = json.load(f)

perf = results.get('performance', {})
print(f"Total duration: {perf.get('total_duration_minutes', 0):.1f} minutes")
print(f"LLM calls: {perf.get('total_llm_calls', 0):,}")
print(f"Total tokens: {perf.get('total_tokens', 0):,}")
print(f"Estimated cost: ${perf.get('estimated_cost_usd', 0):.4f} USD")

# å„stageè€—æ—¶
stages = perf.get('stages_summary', {})
print("\nStage breakdown:")
for stage, stats in stages.items():
    print(f"  {stage}: {stats.get('duration_seconds', 0):.1f}s ({stats.get('items_processed', 0)} items)")
EOF
```

### 5. Top OpportunitieséªŒè¯

```bash
# æŸ¥çœ‹å¾—åˆ†>=7.0çš„opportunities
sqlite3 data/wise_collection.db <<EOF
SELECT
    o.opportunity_name,
    o.total_score,
    o.recommendation,
    c.cluster_name
FROM opportunities o
JOIN clusters c ON o.cluster_id = c.id
WHERE o.total_score >= 7.0
ORDER BY o.total_score DESC
LIMIT 10;
EOF
```

---

## ğŸ› æ•…éšœæ’æŸ¥

### é—®é¢˜1: ImportError: No module named 'chromadb'

**è§£å†³æ–¹æ¡ˆ**:
```bash
pip install chromadb
```

### é—®é¢˜2: Database is locked

**è§£å†³æ–¹æ¡ˆ**:
```bash
# æ£€æŸ¥æ˜¯å¦æœ‰å…¶ä»–è¿›ç¨‹åœ¨ä½¿ç”¨
ps aux | grep python

# åœæ­¢å…¶ä»–pipelineè¿›ç¨‹
pkill -f run_pipeline

# ç­‰å¾…å‡ ç§’åé‡è¯•
sleep 5
python run_pipeline.py --stage all
```

### é—®é¢˜3: LLM API rate limit exceeded

**è§£å†³æ–¹æ¡ˆ**:
```bash
# ä¿®æ”¹config/llm.yamlï¼Œé™ä½å¹¶å‘
# æˆ–ç­‰å¾…ä¸€æ®µæ—¶é—´åé‡è¯•

# æŸ¥çœ‹å¤±è´¥æ—¥å¿—
grep -i "rate limit" logs/pipeline.log
```

### é—®é¢˜4: Pipelineè¿è¡Œæ—¶é—´è¿‡é•¿ (>2å°æ—¶)

**è§£å†³æ–¹æ¡ˆ**:
```bash
# ä½¿ç”¨é»˜è®¤limitsè€Œä¸æ˜¯--process-all
python run_pipeline.py --stage all --save-results

# æˆ–åªè¿è¡Œç‰¹å®šstage
python run_pipeline.py --stage extract
python run_pipeline.py --stage cluster
```

### é—®é¢˜5: å†…å­˜ä¸è¶³ (Out of memory)

**è§£å†³æ–¹æ¡ˆ**:
```bash
# ç›‘æ§å†…å­˜ä½¿ç”¨
/usr/bin/time -v python run_pipeline.py --stage all

# å¦‚æœè¶…è¿‡4GBï¼Œè€ƒè™‘å‡å°‘å¹¶å‘æˆ–batch size

# æˆ–åˆ†stageè¿è¡Œ
for stage in fetch filter extract embed cluster; do
    python run_pipeline.py --stage $stage
done
```

---

## ğŸ“ˆ æ€§èƒ½åŸºå‡†

### é¢„æœŸè¿è¡Œæ—¶é—´ (ä½¿ç”¨å½“å‰ä»£ç )

| æ¨¡å¼ | æ•°æ®é‡ | é¢„æœŸæ—¶é—´ | è¯´æ˜ |
|------|--------|----------|------|
| **æµ‹è¯•æ¨¡å¼** (é»˜è®¤limits) | ~100 posts | ~30-45åˆ†é’Ÿ | å¿«é€ŸéªŒè¯ |
| **é¦–æ¬¡è¿è¡Œ** (--process-all) | å…¨éƒ¨æ•°æ® | ~2-3å°æ—¶ | å¤„ç†æ‰€æœ‰å†å²æ•°æ® |
| **åç»­è¿è¡Œ** (å¢é‡) | æ–°æ•°æ® | ~30-60åˆ†é’Ÿ | åªå¤„ç†æ–°æ•°æ® |

### å„Stageé¢„æœŸè€—æ—¶

| Stage | é¢„æœŸæ—¶é—´ | è¯´æ˜ |
|-------|----------|------|
| Fetch | 5-10 min | å–å†³äºç½‘ç»œå’Œsubreddits |
| Filter | 5-10 min | LLMè°ƒç”¨ |
| Extract | 30-40 min | **æœ€æ…¢** (å¤§é‡LLMè°ƒç”¨) |
| Embed | 3-5 min | æœ¬åœ°embedding |
| Cluster | 15-20 min | ChromaæŸ¥è¯¢ + LLMéªŒè¯ |
| Map | 10-15 min | LLMè°ƒç”¨ |
| Score | 10-15 min | LLMè°ƒç”¨ |
| Decision | 3-5 min | LLMè°ƒç”¨ |
| Cleanup | <1 min | æ•°æ®åº“æ“ä½œ |

---

## ğŸ¯ æˆåŠŸæ ‡å‡†

è¿è¡Œpipelineåï¼Œåº”è¯¥çœ‹åˆ°ï¼š

### 1. æ—¥å¿—è¾“å‡º

```
âœ… Stage 1 completed: Found X posts
âœ… Stage 2 completed: Y/X posts passed
âœ… Stage 3 completed: Extracted Z pain events
âœ… Stage 4 completed: Created Z embeddings
âœ… Stage 5 completed:
   Events processed: N
   New clusters: M
   Updated clusters: K
âœ… Stage 6 completed: Mapped O opportunities
âœ… Stage 7 completed: Scored P opportunities
âœ… Stage 8 completed: Generated S candidates
âœ… Stage 9 completed:
   Active events: A
   Orphan events: O
   Retention rate: R%
```

### 2. æ•°æ®å¢é•¿

```
Before:
  Raw posts: 2330
  Pain events: 2242
  Clusters: 36

After (å‡è®¾æ–°å¢100 posts):
  Raw posts: 2430 (+100)
  Pain events: ~2342 (+100)
  Clusters: 36-38 (å¯èƒ½æ–°å¢)
```

### 3. LifecycleçŠ¶æ€

```
Active events: ~1500-1600 (60-70%)
Orphan events: ~700-900 (30-40%)
Retention rate: 60-70%
```

### 4. æ–°å‘ç°çš„æœºä¼š

```
Top opportunities (score >= 7.0):
  1. [Opportunity name] - Score: 8.5
  2. [Opportunity name] - Score: 7.8
  3. ...
```

---

## ğŸ“ æµ‹è¯•æŠ¥å‘Šæ¨¡æ¿

å®Œæˆæµ‹è¯•åï¼Œå»ºè®®è®°å½•ï¼š

```markdown
## Pipelineæµ‹è¯•æŠ¥å‘Š

**æµ‹è¯•æ—¥æœŸ**: YYYY-MM-DD
**æµ‹è¯•æ¨¡å¼**: [é»˜è®¤/å…¨é‡/å¢é‡]
**æµ‹è¯•äººå‘˜**: [Your name]

### ç¯å¢ƒä¿¡æ¯
- Pythonç‰ˆæœ¬:
- åˆ†æ”¯:
- Commit:

### è¿è¡Œç»“æœ
- æ€»è¿è¡Œæ—¶é—´: Xåˆ†é’Ÿ
- æ–°å¢posts: X
- æ–°å¢pain_events: Y
- æ–°å¢clusters: Z

### æ€§èƒ½æŒ‡æ ‡
- LLMè°ƒç”¨æ¬¡æ•°: N
- Tokenä½¿ç”¨é‡: T
- é¢„ä¼°æˆæœ¬: $X.XX

### å‘ç°çš„é—®é¢˜
- [å¦‚æœ‰]

### Top Opportunities
1. [Opportunity 1]
2. [Opportunity 2]

### ä¸‹ä¸€æ­¥
- [ ] Phase 5: æ›´æ–°run_pipeline.py
- [ ] Phase 6: æ€§èƒ½ä¼˜åŒ–
- [ ] å…¶ä»–
```

---

## ğŸ” é«˜çº§è°ƒè¯•æŠ€å·§

### 1. å•ç‹¬è¿è¡ŒæŸä¸ªStage

```bash
# åªè¿è¡Œfetch stage
python run_pipeline.py --stage fetch --limit-sources 5

# åªè¿è¡Œfilter stage
python run_pipeline.py --stage filter --limit-posts 10

# åªè¿è¡Œextract stage
python run_pipeline.py --stage extract --limit-posts 10

# åªè¿è¡Œcluster stage
python run_pipeline.py --stage cluster

# åªè¿è¡Œlifecycle cleanup
python run_pipeline.py --stage lifecycle_cleanup
```

### 2. æŸ¥çœ‹è¯¦ç»†æ—¥å¿—

```bash
# ä¿®æ”¹logging levelä¸ºDEBUG
# åœ¨run_pipeline.pyä¸­ä¿®æ”¹:
# logging.basicConfig(level=logging.DEBUG)

# é‡æ–°è¿è¡Œ
python run_pipeline.py --stage all 2>&1 | tee pipeline_debug.log
```

### 3. ä½¿ç”¨Python Profiler

```bash
# æ€§èƒ½åˆ†æ
python -m cProfile -o pipeline.prof run_pipeline.py --stage all

# æŸ¥çœ‹ç»“æœ
python -m pstats pipeline.prof
# è¿›å…¥äº¤äº’ç•Œé¢åï¼š
# > stats 10  # æŸ¥çœ‹top 10æœ€æ…¢çš„å‡½æ•°
# > callers run_stage_cluster  # æŸ¥çœ‹è°è°ƒç”¨äº†cluster stage
```

### 4. æ•°æ®åº“æŸ¥è¯¢åˆ†æ

```bash
# å¯ç”¨SQLiteæŸ¥è¯¢æ—¥å¿—
export SQLITE_TRACE="1"
python run_pipeline.py --stage cluster

# æˆ–åœ¨ä»£ç ä¸­æ·»åŠ :
# import sqlite3
# sqlite3.connect('...', check_same_thread=False).set_trace_callback(print)
```

---

## âœ… å®Œæˆæµ‹è¯•å

### 1. æ¸…ç†ä¸´æ—¶æ–‡ä»¶

```bash
# æŸ¥çœ‹ä¸´æ—¶æ–‡ä»¶
ls -lh pipeline_results_*.json
ls -lh docs/reports/pipeline_metrics_*.json

# ä¿ç•™æœ€è¿‘çš„ï¼Œåˆ é™¤æ—§çš„
ls -t pipeline_results_*.json | tail -n +6 | xargs rm -
```

### 2. æäº¤ä»£ç 

```bash
# å¦‚æœæœ‰ä¿®æ”¹
git status
git add .
git commit -m "test: Run full pipeline test"
```

### 3. å‡†å¤‡ä¸‹ä¸€æ­¥

æ ¹æ®æµ‹è¯•ç»“æœï¼Œå†³å®šï¼š

**å¦‚æœæµ‹è¯•é€šè¿‡** âœ…:
- ç»§ç»­Phase 5: æ›´æ–°run_pipeline.py
- æˆ–å…ˆåœ¨çœŸå®ç¯å¢ƒè¿è¡Œå‡ æ¬¡éªŒè¯

**å¦‚æœæµ‹è¯•å¤±è´¥** âŒ:
- æŸ¥çœ‹æ—¥å¿—æ‰¾å‡ºé—®é¢˜
- å•ç‹¬è¿è¡Œå¤±è´¥çš„stageè°ƒè¯•
- å‚è€ƒæœ¬æ–‡æ¡£çš„"æ•…éšœæ’æŸ¥"éƒ¨åˆ†

---

## ğŸ“ è·å–å¸®åŠ©

å¦‚æœé‡åˆ°é—®é¢˜ï¼š

1. æŸ¥çœ‹æ—¥å¿—: `cat logs/pipeline.log`
2. æŸ¥çœ‹æœ¬æ–‡æ¡£çš„"æ•…éšœæ’æŸ¥"éƒ¨åˆ†
3. è¿è¡Œæµ‹è¯•å¥—ä»¶éªŒè¯: `python tests/test_*.py`
4. æ£€æŸ¥å·²çŸ¥é—®é¢˜: `docs/PHASE_5_6_DEV_GUIDE.md`

---

**æ–‡æ¡£ä½œè€…**: Claude Sonnet 4.5
**æœ€åæ›´æ–°**: 2026-01-13
