#!/usr/bin/env python3
"""
ç—›ç‚¹åº”ç”¨åˆ†æå™¨

é’ˆå¯¹æ¯ä¸ªç—›ç‚¹èšç±»ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Šï¼ŒåŒ…å«ï¼š
1. ç—›ç‚¹åˆ†æ
2. åº”ç”¨è®¾è®¡æ–¹æ¡ˆ
3. å¯æ‰§è¡Œæœºä¼šæ¸…å•

æ¯ä¸ªèšç±»ç”Ÿæˆä¸€ä¸ªç‹¬ç«‹çš„markdownæ–‡ä»¶
"""

import os
import sqlite3
import json
import requests
from datetime import datetime
from typing import Dict, List, Any, Optional
import re
from pathlib import Path
import logging
import sys

# å¯¼å…¥ç»Ÿä¸€æ•°æ®åº“ç®¡ç†å™¨
try:
    from utils.db import WiseCollectionDB
except ImportError as e:
    print(f"âŒ æ— æ³•å¯¼å…¥æ•°æ®åº“ç®¡ç†å™¨: {e}")
    sys.exit(1)

# åŠ è½½.envæ–‡ä»¶
def load_env():
    """åŠ è½½.envæ–‡ä»¶"""
    env_path = Path(__file__).parent / '.env'
    if env_path.exists():
        with open(env_path, 'r') as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith('#'):
                    key, value = line.split('=', 1)
                    os.environ[key.strip()] = value.strip()

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),  # è¾“å‡ºåˆ°æ§åˆ¶å°
        logging.FileHandler('pain_point_analyzer.log', encoding='utf-8')  # åŒæ—¶è¾“å‡ºåˆ°æ–‡ä»¶
    ]
)

logger = logging.getLogger(__name__)

# åŠ è½½ç¯å¢ƒå˜é‡
logger.info("å¼€å§‹åŠ è½½ç¯å¢ƒå˜é‡...")
load_env()
logger.info("ç¯å¢ƒå˜é‡åŠ è½½å®Œæˆ")


class PainPointAnalyzer:
    def __init__(self, unified_db: bool = True):
        """åˆå§‹åŒ–åˆ†æå™¨"""
        logger.info("åˆå§‹åŒ– PainPointAnalyzer...")

        # åˆå§‹åŒ–ç»Ÿä¸€æ•°æ®åº“ç®¡ç†å™¨
        logger.info("åˆå§‹åŒ–æ•°æ®åº“ç®¡ç†å™¨...")
        self.db = WiseCollectionDB(unified=unified_db)
        self.unified_db = unified_db

        if unified_db:
            logger.info(f"ä½¿ç”¨ç»Ÿä¸€æ•°æ®åº“æ¨¡å¼: {self.db.get_database_path()}")
        else:
            logger.info("ä½¿ç”¨å¤šæ•°æ®åº“æ¨¡å¼")

        self.base_url = os.getenv('Siliconflow_Base_URL', 'https://api.siliconflow.cn/v1')
        self.api_key = os.getenv('Siliconflow_KEY')
        self.model = os.getenv('Siliconflow_AI_Model_Default', 'deepseek-ai/DeepSeek-V3.2')

        logger.info(f"é…ç½®ä¿¡æ¯: base_url={self.base_url}, model={self.model}")
        logger.info(f"API key {'å·²è®¾ç½®' if self.api_key else 'æœªè®¾ç½®'}")

        if not self.api_key:
            logger.error("SiliconFlow API key not found in environment variables")
            raise ValueError("SiliconFlow API key not found in environment variables")

        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir = "pain_analysis_reports"
        os.makedirs(self.output_dir, exist_ok=True)
        logger.info(f"è¾“å‡ºç›®å½•å·²åˆ›å»º: {self.output_dir}")

        print(f"ğŸ”§ åˆå§‹åŒ–åˆ†æå™¨")
        print(f"   â€¢ æ•°æ®åº“æ¨¡å¼: {'ç»Ÿä¸€æ•°æ®åº“' if unified_db else 'å¤šæ•°æ®åº“æ–‡ä»¶'}")
        if unified_db:
            print(f"   â€¢ æ•°æ®åº“è·¯å¾„: {self.db.get_database_path()}")
        print(f"   â€¢ APIæ¨¡å‹: {self.model}")
        print(f"   â€¢ è¾“å‡ºç›®å½•: {self.output_dir}")

    def get_db_connection(self, db_type: str = "clusters"):
        """è·å–æ•°æ®åº“è¿æ¥ - ä½¿ç”¨ç»Ÿä¸€æ•°æ®åº“ç®¡ç†å™¨"""
        logger.debug(f"è·å–æ•°æ®åº“è¿æ¥ï¼Œç±»å‹: {db_type}")
        return self.db.get_connection(db_type)

    def call_llm(self, prompt: str, temperature: float = 0.3, max_retries: int = 3) -> str:
        """è°ƒç”¨LLM"""
        logger.info(f"å¼€å§‹è°ƒç”¨LLM: model={self.model}, temperature={temperature}, max_retries={max_retries}")
        logger.debug(f"prompté•¿åº¦: {len(prompt)} å­—ç¬¦")

        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }

        data = {
            "model": self.model,
            "messages": [
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä½èµ„æ·±çš„äº§å“åˆ†æå¸ˆå’ŒæŠ€æœ¯é¡¾é—®ï¼Œä¸“é—¨åˆ†æç”¨æˆ·ç—›ç‚¹å¹¶è®¾è®¡åˆ›æ–°çš„è§£å†³æ–¹æ¡ˆã€‚"},
                {"role": "user", "content": prompt}
            ],
            "temperature": temperature,
            "max_tokens": 4000
        }

        for attempt in range(max_retries):
            try:
                print(f"  ğŸ¤– è°ƒç”¨LLM (å°è¯• {attempt + 1}/{max_retries})...")
                logger.info(f"å°è¯•ç¬¬ {attempt + 1}/{max_retries} æ¬¡LLMè°ƒç”¨")

                url = f"{self.base_url}/chat/completions"
                logger.debug(f"è¯·æ±‚URL: {url}")

                response = requests.post(
                    url,
                    headers=headers,
                    json=data,
                    timeout=180  # å¢åŠ åˆ°3åˆ†é’Ÿ
                )

                logger.debug(f"å“åº”çŠ¶æ€ç : {response.status_code}")

                response.raise_for_status()
                result = response.json()

                if 'choices' not in result or len(result['choices']) == 0:
                    logger.error("LLMå“åº”ä¸­æ²¡æœ‰choiceså­—æ®µ")
                    return "LLMå“åº”æ ¼å¼é”™è¯¯: æ²¡æœ‰choices"

                content = result['choices'][0]['message']['content'].strip()
                logger.debug(f"LLMå“åº”é•¿åº¦: {len(content)} å­—ç¬¦")
                print(f"  âœ… LLMå“åº”æˆåŠŸ")
                logger.info("LLMè°ƒç”¨æˆåŠŸ")
                return content

            except requests.exceptions.Timeout:
                error_msg = f"LLMè°ƒç”¨è¶…æ—¶ (å°è¯• {attempt + 1}/{max_retries})"
                logger.warning(error_msg)
                print(f"  âš ï¸ {error_msg}")
                if attempt < max_retries - 1:
                    continue
                logger.error(f"LLMè°ƒç”¨è¶…æ—¶ï¼Œå·²é‡è¯•{max_retries}æ¬¡")
                return f"LLMè°ƒç”¨è¶…æ—¶: å·²é‡è¯•{max_retries}æ¬¡"

            except requests.exceptions.HTTPError as e:
                error_msg = f"HTTPé”™è¯¯: {e}"
                logger.error(error_msg)
                logger.error(f"å“åº”å†…å®¹: {response.text if 'response' in locals() else 'N/A'}")
                print(f"  âŒ {error_msg}")
                if attempt < max_retries - 1:
                    print(f"  ğŸ”„ æ­£åœ¨é‡è¯•...")
                    continue
                return f"LLM HTTPé”™è¯¯: {str(e)}"

            except Exception as e:
                error_msg = f"LLMè°ƒç”¨å¤±è´¥: {e}"
                logger.error(error_msg)
                import traceback
                logger.error(traceback.format_exc())
                print(f"  âŒ {error_msg}")
                if attempt < max_retries - 1:
                    print(f"  ğŸ”„ æ­£åœ¨é‡è¯•...")
                    continue
                return f"LLMè°ƒç”¨å¤±è´¥: {str(e)}"

    def get_top_clusters(self, min_score: float = 0.8, limit: int = 10) -> List[Dict]:
        """è·å–é«˜åˆ†èšç±» - ä½¿ç”¨ç»Ÿä¸€æ•°æ®åº“"""
        logger.info(f"è·å–é«˜åˆ†èšç±»: min_score={min_score}, limit={limit}")
        clusters = []

        try:
            with self.get_db_connection("clusters") as conn:
                cursor = conn.cursor()

                logger.debug("æ‰§è¡Œèšç±»æŸ¥è¯¢SQL...")

                cursor.execute("""
                    SELECT c.id, c.cluster_name, c.cluster_description, c.avg_pain_score,
                           c.cluster_size, c.pain_event_ids,
                           COUNT(o.id) as opportunity_count,
                           MAX(o.total_score) as max_opportunity_score,
                           GROUP_CONCAT(o.opportunity_name, ' | ') as opportunity_names
                    FROM clusters c
                    LEFT JOIN opportunities o ON c.id = o.cluster_id
                    GROUP BY c.id
                    HAVING opportunity_count > 0 AND max_opportunity_score >= ?
                    ORDER BY max_opportunity_score DESC, c.avg_pain_score DESC
                    LIMIT ?
                """, (min_score, limit))

                logger.debug(f"æŸ¥è¯¢æ‰§è¡Œå®Œæˆï¼Œå¼€å§‹å¤„ç†ç»“æœ...")
                rows = cursor.fetchall()
                logger.info(f"æŸ¥è¯¢åˆ° {len(rows)} ä¸ªèšç±»")

                for i, row in enumerate(rows, 1):
                    logger.debug(f"å¤„ç†ç¬¬ {i}/{len(rows)} ä¸ªèšç±»: {row['cluster_name'][:50]}...")
                    # è·å–è¯¥èšç±»çš„æ‰€æœ‰æœºä¼š
                    logger.debug(f"è·å–èšç±» {row['id']} çš„æœºä¼šæ•°æ®...")
                    cursor.execute("""
                        SELECT opportunity_name, description, total_score, recommendation,
                               current_tools, missing_capability, why_existing_fail,
                               target_users, killer_risks
                        FROM opportunities
                        WHERE cluster_id = ?
                        ORDER BY total_score DESC
                    """, (row['id'],))

                    opportunities = []
                    opp_rows = cursor.fetchall()
                    logger.debug(f"èšç±» {row['id']} æœ‰ {len(opp_rows)} ä¸ªæœºä¼š")

                    for opp_row in opp_rows:
                        opportunities.append({
                            'name': opp_row['opportunity_name'],
                            'description': opp_row['description'],
                            'score': opp_row['total_score'],
                            'recommendation': opp_row['recommendation'],
                            'current_tools': opp_row['current_tools'],
                            'missing_capability': opp_row['missing_capability'],
                            'why_existing_fail': opp_row['why_existing_fail'],
                            'target_users': opp_row['target_users'],
                            'killer_risks': json.loads(opp_row['killer_risks']) if opp_row['killer_risks'] else []
                        })

                    # è·å–ç—›ç‚¹äº‹ä»¶æ ·æœ¬
                    try:
                        pain_event_ids = json.loads(row['pain_event_ids'])
                        logger.debug(f"èšç±» {row['id']} ç—›ç‚¹äº‹ä»¶IDs: {len(pain_event_ids)} ä¸ª")
                        sample_pains = self.get_sample_pain_events(pain_event_ids[:5])
                    except json.JSONDecodeError as e:
                        logger.warning(f"èšç±» {row['id']} pain_event_ids JSONè§£æå¤±è´¥: {e}")
                        sample_pains = []

                    clusters.append({
                        'id': row['id'],
                        'name': row['cluster_name'],
                        'description': row['cluster_description'],
                        'avg_pain_score': row['avg_pain_score'],
                        'cluster_size': row['cluster_size'],
                        'opportunity_count': row['opportunity_count'],
                        'max_opportunity_score': row['max_opportunity_score'],
                        'opportunities': opportunities,
                        'sample_pains': sample_pains
                    })

            logger.info(f"æˆåŠŸè·å– {len(clusters)} ä¸ªèšç±»æ•°æ®")
            return clusters

        except Exception as e:
            logger.error(f"è·å–èšç±»æ•°æ®å¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return []

    def get_sample_pain_events(self, pain_event_ids: List[int]) -> List[Dict]:
        """è·å–ç—›ç‚¹äº‹ä»¶æ ·æœ¬ - ä½¿ç”¨ç»Ÿä¸€æ•°æ®åº“"""
        logger.debug(f"è·å– {len(pain_event_ids)} ä¸ªç—›ç‚¹äº‹ä»¶æ ·æœ¬: {pain_event_ids}")
        pains = []

        if not pain_event_ids:
            logger.warning("pain_event_ids ä¸ºç©ºï¼Œè¿”å›ç©ºåˆ—è¡¨")
            return []

        try:
            with self.get_db_connection("pain") as conn:
                cursor = conn.cursor()

                placeholders = ','.join(['?' for _ in pain_event_ids])
                logger.debug(f"æ‰§è¡Œç—›ç‚¹äº‹ä»¶æŸ¥è¯¢ï¼ŒIDs: {pain_event_ids}")

                cursor.execute(f"""
                    SELECT problem, current_workaround, frequency, emotional_signal, mentioned_tools
                    FROM pain_events
                    WHERE id IN ({placeholders})
                """, pain_event_ids)

                rows = cursor.fetchall()
                logger.debug(f"æŸ¥è¯¢åˆ° {len(rows)} ä¸ªç—›ç‚¹äº‹ä»¶")

                for row in rows:
                    pains.append({
                        'problem': row['problem'],
                        'workaround': row['current_workaround'],
                        'frequency': row['frequency'],
                        'emotion': row['emotional_signal'],
                        'tools': row['mentioned_tools']
                    })

            logger.debug(f"æˆåŠŸè·å– {len(pains)} ä¸ªç—›ç‚¹äº‹ä»¶")
            return pains

        except Exception as e:
            logger.error(f"è·å–ç—›ç‚¹äº‹ä»¶å¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return []

    def generate_basic_analysis(self, cluster: Dict) -> str:
        """ç”ŸæˆåŸºç¡€åˆ†æï¼ˆå½“LLMè°ƒç”¨å¤±è´¥æ—¶ï¼‰"""
        pain_context = "\n".join([
            f"â€¢ {pain['problem']}" + chr(10) + f"  å½“å‰è§£å†³æ–¹æ¡ˆ: {pain['workaround']}" + chr(10) + f"  å‘ç”Ÿé¢‘ç‡: {pain['frequency']}" + chr(10) + f"  æƒ…ç»ªä¿¡å·: {pain['emotion']}"
            for pain in cluster['sample_pains']
        ])

        opp_analysis = ""
        for opp in cluster['opportunities'][:3]:
            opp_analysis += f"""
### {opp['name']} (è¯„åˆ†: {opp['score']:.2f})

**é—®é¢˜æè¿°**: {opp['description']}

**å…³é”®æœºä¼šåˆ†æ**:
- å¸‚åœºéœ€æ±‚: é€šè¿‡{cluster['cluster_size']}ä¸ªç›¸å…³å¸–å­éªŒè¯äº†å¼ºçƒˆéœ€æ±‚
- ç›®æ ‡ç”¨æˆ·: {opp['target_users'] or 'ä¸­å°ä¼ä¸šã€ä¸ªäººå¼€å‘è€…ã€è‡ªç”±èŒä¸šè€…'}
- ç«äº‰ä¼˜åŠ¿: {opp['missing_capability'] or 'å¡«è¡¥ç°æœ‰å·¥å…·çš„åŠŸèƒ½ç©ºç™½'}

**MVPåŠŸèƒ½å»ºè®®**:
1. æ ¸å¿ƒåŠŸèƒ½å®ç°{opp['current_tools'] and f"ï¼Œæ•´åˆ{opp['current_tools']}çš„å·¥ä½œæµ"}
2. ç®€åŒ–ç”¨æˆ·ç•Œé¢ï¼Œé™ä½å­¦ä¹ æˆæœ¬
3. å¿«é€Ÿéƒ¨ç½²å’Œé›†æˆèƒ½åŠ›

**å•†ä¸šåŒ–å»ºè®®**:
- å…è´¹åŸºç¡€ç‰ˆå¸å¼•åˆå§‹ç”¨æˆ·
- Proç‰ˆæœ¬æœˆè´¹$10-20
- ä¼ä¸šå®šåˆ¶ç‰ˆæ”¯æŒ
"""

        return f"""## ç—›ç‚¹æ·±åº¦åˆ†æ

### æ ¸å¿ƒé—®é¢˜
{cluster['description']}

### å½±å“èŒƒå›´
- å—å½±å“ç”¨æˆ·ç¾¤ä½“: {cluster['cluster_size']}ä¸ªçœŸå®ç”¨æˆ·åé¦ˆ
- ç—›ç‚¹å¼ºåº¦: {cluster['avg_pain_score']:.2f}/1.0

### å…¸å‹ç—›ç‚¹äº‹ä»¶
{pain_context}

## å¸‚åœºæœºä¼šè¯„ä¼°

### å¸‚åœºè§„æ¨¡
åŸºäºRedditè®¨è®ºçƒ­åº¦ï¼Œè¯¥é—®é¢˜å½±å“äº†å¤§é‡ç”¨æˆ·ï¼Œå…·æœ‰æ˜ç¡®çš„ä»˜è´¹æ„æ„¿ã€‚

### æœºä¼šæ•°é‡
å·²è¯†åˆ«{cluster['opportunity_count']}ä¸ªå…·ä½“æœºä¼šï¼Œæœ€é«˜è¯„åˆ†{cluster['max_opportunity_score']:.2f}

## äº§å“è®¾è®¡æ–¹æ¡ˆ

{opp_analysis}

## å¯æ‰§è¡Œè¡ŒåŠ¨è®¡åˆ’

### ç«‹å³è¡ŒåŠ¨ï¼ˆ1ä¸ªæœˆå†…ï¼‰
1. éªŒè¯ç›®æ ‡ç”¨æˆ·éœ€æ±‚ï¼Œè¿›è¡Œæ·±åº¦ç”¨æˆ·è®¿è°ˆ
2. å¼€å‘æœ€å°å¯è¡Œäº§å“(MVP)åŸå‹
3. å»ºç«‹ç”¨æˆ·åé¦ˆæ¸ é“

### çŸ­æœŸç›®æ ‡ï¼ˆ3ä¸ªæœˆå†…ï¼‰
1. å‘å¸ƒMVPç‰ˆæœ¬å¹¶è·å–é¦–æ‰¹100ä¸ªç”¨æˆ·
2. åŸºäºåé¦ˆè¿­ä»£äº§å“åŠŸèƒ½
3. æ¢ç´¢ç›ˆåˆ©æ¨¡å¼

### æˆåŠŸæŒ‡æ ‡
- ç”¨æˆ·ç•™å­˜ç‡ > 60%
- æœˆæ´»è·ƒç”¨æˆ·å¢é•¿ > 20%
- NPSå¾—åˆ† > 40
"""

    def analyze_cluster(self, cluster: Dict) -> str:
        """åˆ†æå•ä¸ªèšç±»å¹¶ç”Ÿæˆå®Œæ•´æŠ¥å‘Š"""

        # æ„å»ºåˆ†æprompt
        pain_context = "\n".join([
            f"â€¢ {pain['problem']} (å½“å‰è§£å†³æ–¹æ¡ˆ: {pain['workaround']}, é¢‘ç‡: {pain['frequency']}, æƒ…ç»ª: {pain['emotion']})"
            for pain in cluster['sample_pains']
        ])

        opportunities_context = "\n".join([
            f"â€¢ {opp['name']} (è¯„åˆ†: {opp['score']:.2f})"
            f"  æè¿°: {opp['description'][:100]}..."
            for opp in cluster['opportunities'][:3]
        ])

        prompt = f"""
è¯·åˆ†æä»¥ä¸‹ç—›ç‚¹èšç±»å¹¶ç”Ÿæˆç»¼åˆæŠ¥å‘Šï¼š

## èšç±»ä¿¡æ¯
- **èšç±»åç§°**: {cluster['name']}
- **èšç±»æè¿°**: {cluster['description']}
- **ç—›ç‚¹æ•°é‡**: {cluster['cluster_size']}
- **å¹³å‡ç—›ç‚¹å¼ºåº¦**: {cluster['avg_pain_score']:.2f}
- **æœºä¼šæ•°é‡**: {cluster['opportunity_count']}

## å…¸å‹ç—›ç‚¹æ ·æœ¬
{pain_context}

## å·²è¯†åˆ«çš„æœºä¼š
{opportunities_context}

## åˆ†æè¦æ±‚
è¯·æŒ‰ç…§ä»¥ä¸‹ç»“æ„ç”Ÿæˆè¯¦ç»†åˆ†ææŠ¥å‘Šï¼š

### 1. ç—›ç‚¹æ·±åº¦åˆ†æ
- æ ¸å¿ƒé—®é¢˜æœ¬è´¨
- å½±å“èŒƒå›´å’Œä¸¥é‡ç¨‹åº¦
- ç”¨æˆ·ç‰¹å¾å’Œä½¿ç”¨åœºæ™¯
- ç°æœ‰è§£å†³æ–¹æ¡ˆçš„ä¸è¶³

### 2. å¸‚åœºæœºä¼šè¯„ä¼°
- å¸‚åœºè§„æ¨¡ä¼°ç®—
- ç”¨æˆ·ä»˜è´¹æ„æ„¿
- ç«äº‰æ ¼å±€åˆ†æ
- è¿›å…¥å£å’è¯„ä¼°

### 3. äº§å“è®¾è®¡æ–¹æ¡ˆ
- MVPåŠŸèƒ½å®šä¹‰
- æŠ€æœ¯æ¶æ„å»ºè®®
- ç”¨æˆ·ä½“éªŒè®¾è®¡è¦ç‚¹
- å·®å¼‚åŒ–ç«äº‰ç­–ç•¥

### 4. å•†ä¸šåŒ–è·¯å¾„
- ç›ˆåˆ©æ¨¡å¼è®¾è®¡
- è·å®¢ç­–ç•¥
- å®šä»·ç­–ç•¥
- å‘å±•è·¯çº¿å›¾

### 5. å¯æ‰§è¡Œè¡ŒåŠ¨è®¡åˆ’
- è¿‘æœŸè¡ŒåŠ¨é¡¹ï¼ˆ1-3ä¸ªæœˆï¼‰
- ä¸­æœŸç›®æ ‡ï¼ˆ3-6ä¸ªæœˆï¼‰
- å…³é”®æˆåŠŸæŒ‡æ ‡
- é£é™©åº”å¯¹æªæ–½

è¯·ç¡®ä¿åˆ†ææ·±å…¥ã€å…·ä½“ä¸”å¯æ“ä½œã€‚ä½¿ç”¨markdownæ ¼å¼è¾“å‡ºã€‚
"""

        print(f"ğŸ¤– æ­£åœ¨åˆ†æèšç±»: {cluster['name'][:50]}...")

        # å°è¯•è°ƒç”¨LLM
        analysis = self.call_llm(prompt, temperature=0.4)

        # å¦‚æœLLMè°ƒç”¨å¤±è´¥ï¼Œä½¿ç”¨åŸºç¡€åˆ†æ
        if "LLMè°ƒç”¨" in analysis:
            print(f"  âš ï¸ ä½¿ç”¨åŸºç¡€åˆ†ææ›¿ä»£")
            analysis = self.generate_basic_analysis(cluster)

        return analysis

    def generate_cluster_report(self, cluster: Dict, analysis: str) -> str:
        """ç”Ÿæˆèšç±»æŠ¥å‘Šæ–‡ä»¶"""
        logger.info(f"ç”Ÿæˆèšç±»æŠ¥å‘Š: {cluster['name'][:50]}...")

        # æ¸…ç†æ–‡ä»¶å
        safe_name = re.sub(r'[^\w\s-]', '', cluster['name']).strip()
        safe_name = re.sub(r'[-\s]+', '_', safe_name)
        filename = f"{safe_name}_opportunity_analysis.md"
        filepath = os.path.join(self.output_dir, filename)

        logger.debug(f"æŠ¥å‘Šæ–‡ä»¶è·¯å¾„: {filepath}")

        # æ„å»ºå®Œæ•´æŠ¥å‘Š
        report_content = f"""# {cluster['name']} - æœºä¼šåˆ†ææŠ¥å‘Š

> **ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
> **èšç±»ID**: {cluster['id']}
> **ç—›ç‚¹æ•°é‡**: {cluster['cluster_size']}
> **å¹³å‡ç—›ç‚¹å¼ºåº¦**: {cluster['avg_pain_score']:.2f}
> **æœºä¼šæ•°é‡**: {cluster['opportunity_count']}

---

## ğŸ“Š èšç±»æ¦‚è§ˆ

**èšç±»æè¿°**: {cluster['description']}

### ğŸ¯ é¡¶çº§æœºä¼š
{chr(10).join([f"- **{opp['name']}** (è¯„åˆ†: {opp['score']:.2f})" for opp in cluster['opportunities'][:5]])}

---

## ğŸ” æ·±åº¦åˆ†æ

{analysis}

---

## ğŸ“‹ åŸå§‹æ•°æ®

### å…¸å‹ç—›ç‚¹äº‹ä»¶
{chr(10).join([f"**é—®é¢˜**: {pain['problem']}" + chr(10) + f"- å½“å‰æ–¹æ¡ˆ: {pain['workaround']}" + chr(10) + f"- å‘ç”Ÿé¢‘ç‡: {pain['frequency']}" + chr(10) + f"- æƒ…ç»ªä¿¡å·: {pain['emotion']}" + chr(10) for pain in cluster['sample_pains']])}

### å·²è¯†åˆ«æœºä¼šè¯¦æƒ…
{chr(10).join([f"**{opp['name']}** (è¯„åˆ†: {opp['score']:.2f})" + chr(10) + f"- æè¿°: {opp['description']}" + chr(10) + f"- æ¨èå»ºè®®: {opp['recommendation']}" + chr(10) + (f"- ç›®æ ‡ç”¨æˆ·: {opp['target_users']}" if opp['target_users'] else "") + chr(10) for opp in cluster['opportunities']])}

---

*æœ¬æŠ¥å‘Šç”± Reddit Pain Point Finder è‡ªåŠ¨ç”Ÿæˆ*
"""

        # å†™å…¥æ–‡ä»¶
        try:
            logger.debug(f"å¼€å§‹å†™å…¥æŠ¥å‘Šæ–‡ä»¶: {filepath}")
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(report_content)
            logger.info(f"æŠ¥å‘Šç”ŸæˆæˆåŠŸ: {filepath}")
            print(f"âœ… æŠ¥å‘Šå·²ç”Ÿæˆ: {filepath}")
            return filepath
        except Exception as e:
            logger.error(f"æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {filepath}, é”™è¯¯: {e}")
            import traceback
            logger.error(traceback.format_exc())
            print(f"âŒ æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")
            return None

    def generate_summary_index(self, report_files: List[str]) -> str:
        """ç”Ÿæˆæ€»ç»“ç´¢å¼•æ–‡ä»¶"""
        logger.info(f"ç”Ÿæˆæ€»ç»“ç´¢å¼•ï¼ŒåŒ…å« {len(report_files)} ä¸ªæŠ¥å‘Š")

        index_content = f"""# ç—›ç‚¹æœºä¼šåˆ†ææŠ¥å‘Šç´¢å¼•

> **ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
> **åˆ†ææ•°é‡**: {len(report_files)}

---

## ğŸ“ˆ åˆ†ææ¦‚è§ˆ

æœ¬æ¬¡å…±åˆ†æäº† {len(report_files)} ä¸ªé«˜ä»·å€¼ç—›ç‚¹èšç±»ï¼Œæ¯ä¸ªèšç±»éƒ½åŒ…å«è¯¦ç»†çš„ç—›ç‚¹åˆ†æã€åº”ç”¨è®¾è®¡æ–¹æ¡ˆå’Œå¯æ‰§è¡Œè¡ŒåŠ¨è®¡åˆ’ã€‚

---

## ğŸ“‹ æŠ¥å‘Šåˆ—è¡¨

{chr(10).join([f"- [{os.path.basename(f)}]({f})" for f in report_files])}

---

## ğŸ¯ ä¸‹ä¸€æ­¥è¡ŒåŠ¨å»ºè®®

1. **ä¼˜å…ˆçº§æ’åº**: æ ¹æ®æœºä¼šè¯„åˆ†å’Œå¸‚åœºè§„æ¨¡ç¡®å®šäº§å“å¼€å‘ä¼˜å…ˆçº§
2. **ç”¨æˆ·éªŒè¯**: é’ˆå¯¹Top 3æœºä¼šè¿›è¡Œç”¨æˆ·è®¿è°ˆå’Œéœ€æ±‚éªŒè¯
3. **MVPå¼€å‘**: é€‰æ‹©æœ€é«˜ä»·å€¼çš„æœºä¼šå¯åŠ¨MVPå¼€å‘
4. **æŒç»­ç›‘æ§**: å®šæœŸæ›´æ–°Redditæ•°æ®ï¼Œè·Ÿè¸ªæ–°çš„ç—›ç‚¹è¶‹åŠ¿

---

*ä½¿ç”¨æ–¹æ³•: ç‚¹å‡»ä¸Šæ–¹é“¾æ¥æŸ¥çœ‹å…·ä½“çš„æœºä¼šåˆ†ææŠ¥å‘Š*
"""

        index_path = os.path.join(self.output_dir, "README.md")
        try:
            logger.debug(f"å¼€å§‹å†™å…¥ç´¢å¼•æ–‡ä»¶: {index_path}")
            with open(index_path, 'w', encoding='utf-8') as f:
                f.write(index_content)
            logger.info(f"ç´¢å¼•æ–‡ä»¶ç”ŸæˆæˆåŠŸ: {index_path}")
            print(f"ğŸ“‘ ç´¢å¼•æ–‡ä»¶å·²ç”Ÿæˆ: {index_path}")
            return index_path
        except Exception as e:
            logger.error(f"ç´¢å¼•æ–‡ä»¶ç”Ÿæˆå¤±è´¥: {index_path}, é”™è¯¯: {e}")
            import traceback
            logger.error(traceback.format_exc())
            print(f"âŒ ç´¢å¼•æ–‡ä»¶ç”Ÿæˆå¤±è´¥: {e}")
            return None

    def run_analysis(self, min_score: float = 0.8, limit: int = 10):
        """è¿è¡Œå®Œæ•´åˆ†æ"""
        logger.info(f"å¼€å§‹è¿è¡Œå®Œæ•´åˆ†æ: min_score={min_score}, limit={limit}")
        print(f"\nğŸš€ å¼€å§‹ç—›ç‚¹æœºä¼šåˆ†æ...")
        print(f"   â€¢ æœ€ä½æœºä¼šè¯„åˆ†: {min_score}")
        print(f"   â€¢ æœ€å¤§åˆ†ææ•°é‡: {limit}")
        print("="*60)

        # è·å–èšç±»æ•°æ®
        logger.info("å¼€å§‹è·å–èšç±»æ•°æ®...")
        clusters = self.get_top_clusters(min_score, limit)
        if not clusters:
            logger.warning("æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„èšç±»æ•°æ®")
            print("âŒ æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„èšç±»æ•°æ®")
            return

        logger.info(f"æˆåŠŸè·å– {len(clusters)} ä¸ªèšç±»")
        print(f"ğŸ“Š æ‰¾åˆ° {len(clusters)} ä¸ªé«˜ä»·å€¼èšç±»")

        # åˆ†ææ¯ä¸ªèšç±»
        report_files = []
        for i, cluster in enumerate(clusters, 1):
            logger.info(f"å¼€å§‹åˆ†æç¬¬ {i}/{len(clusters)} ä¸ªèšç±»: {cluster['name'][:50]}...")
            print(f"\n[{i}/{len(clusters)}] åˆ†æèšç±»: {cluster['name'][:50]}...")

            # æ‰§è¡Œåˆ†æ
            logger.debug("æ‰§è¡Œèšç±»åˆ†æ...")
            analysis = self.analyze_cluster(cluster)

            # ç”ŸæˆæŠ¥å‘Š
            logger.debug("ç”Ÿæˆèšç±»æŠ¥å‘Š...")
            report_path = self.generate_cluster_report(cluster, analysis)
            if report_path:
                report_files.append(report_path)
                logger.info(f"æŠ¥å‘Šå·²æ·»åŠ åˆ°åˆ—è¡¨: {report_path}")

            logger.info(f"èšç±» {i} åˆ†æå®Œæˆ")
            print(f"âœ… å®Œæˆ: {cluster['name'][:50]}")

        # ç”Ÿæˆç´¢å¼•æ–‡ä»¶
        if report_files:
            logger.info("å¼€å§‹ç”Ÿæˆæ€»ç»“ç´¢å¼•...")
            self.generate_summary_index(report_files)

        logger.info(f"åˆ†æå®Œæˆï¼Œç”Ÿæˆäº† {len(report_files)} ä¸ªæŠ¥å‘Š")
        print(f"\nğŸ‰ åˆ†æå®Œæˆï¼")
        print(f"   â€¢ ç”ŸæˆæŠ¥å‘Š: {len(report_files)} ä»½")
        print(f"   â€¢ è¾“å‡ºç›®å½•: {self.output_dir}")
        print(f"   â€¢ æŸ¥çœ‹ç´¢å¼•: {self.output_dir}/README.md")


def main():
    """ä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description="Redditç—›ç‚¹æœºä¼šåˆ†æå™¨")
    parser.add_argument("--min-score", type=float, default=0.8, help="æœ€ä½æœºä¼šè¯„åˆ†")
    parser.add_argument("--limit", type=int, default=15, help="æœ€å¤§åˆ†ææ•°é‡")
    parser.add_argument("--legacy-db", action="store_true", help="ä½¿ç”¨æ—§çš„å¤šæ•°æ®åº“æ¨¡å¼")
    parser.add_argument("--dry-run", action="store_true", help="è¯•è¿è¡Œæ¨¡å¼ï¼ˆä»…è·å–æ•°æ®ï¼Œä¸ç”ŸæˆæŠ¥å‘Šï¼‰")

    args = parser.parse_args()

    logger.info("=" * 50)
    logger.info("ç—›ç‚¹åˆ†æå™¨å¼€å§‹è¿è¡Œ")
    logger.info(f"æ•°æ®åº“æ¨¡å¼: {'å¤šæ•°æ®åº“æ–‡ä»¶' if args.legacy_db else 'ç»Ÿä¸€æ•°æ®åº“'}")
    logger.info(f"æœ€ä½è¯„åˆ†: {args.min_score}, æœ€å¤§æ•°é‡: {args.limit}")
    logger.info("=" * 50)

    try:
        logger.info("åˆå§‹åŒ– PainPointAnalyzer...")
        analyzer = PainPointAnalyzer(unified_db=not args.legacy_db)

        if args.dry_run:
            # è¯•è¿è¡Œï¼šä»…è·å–æ•°æ®å¹¶æ˜¾ç¤º
            logger.info("è¯•è¿è¡Œæ¨¡å¼ï¼šè·å–èšç±»æ•°æ®...")
            clusters = analyzer.get_top_clusters(min_score=args.min_score, limit=args.limit)
            logger.info(f"æ‰¾åˆ° {len(clusters)} ä¸ªèšç±»")

            print(f"\nğŸ“Š è¯•è¿è¡Œç»“æœï¼š")
            print(f"æ‰¾åˆ° {len(clusters)} ä¸ªç¬¦åˆæ¡ä»¶çš„èšç±»ï¼š")
            for i, cluster in enumerate(clusters, 1):
                print(f"  {i}. {cluster['name']} (è¯„åˆ†: {cluster['max_opportunity_score']:.2f}, æœºä¼šæ•°: {cluster['opportunity_count']})")
            return

        logger.info("å¼€å§‹è¿è¡Œåˆ†æ...")
        analyzer.run_analysis(min_score=args.min_score, limit=args.limit)
        logger.info("ç¨‹åºæ‰§è¡Œå®Œæˆ")
    except Exception as e:
        logger.error(f"ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        logger.error(traceback.format_exc())
        print(f"âŒ ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        traceback.print_exc()


if __name__ == "__main__":
    main()