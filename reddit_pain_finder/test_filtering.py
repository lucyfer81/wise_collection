#!/usr/bin/env python3
"""
æµ‹è¯•è¿‡æ»¤è§„åˆ™åŠŸèƒ½
"""
import sys
import logging
from pathlib import Path

# æ·»åŠ æ¨¡å—è·¯å¾„
sys.path.append(str(Path(__file__).parent))

from pipeline.score_viability import ViabilityScorer
from utils.db import db
import json

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def test_filtering_rules():
    """æµ‹è¯•è¿‡æ»¤è§„åˆ™åŠŸèƒ½"""
    print("ğŸ§ª æµ‹è¯•è¿‡æ»¤è§„åˆ™åŠŸèƒ½...")

    try:
        # åˆå§‹åŒ–è¯„åˆ†å™¨
        scorer = ViabilityScorer()

        # æ£€æŸ¥è¿‡æ»¤è§„åˆ™æ˜¯å¦å¯ç”¨
        if not scorer.filtering_rules.get("enabled", False):
            print("âŒ è¿‡æ»¤è§„åˆ™æœªå¯ç”¨ï¼Œè¯·æ£€æŸ¥é…ç½®æ–‡ä»¶")
            return False

        print(f"âœ… è¿‡æ»¤è§„åˆ™å·²å¯ç”¨")
        print(f"   - æœ€å°èšç±»å¤§å°: {scorer.filtering_rules.get('min_cluster_size')}")
        print(f"   - æœ€å°ç‹¬ç«‹ä½œè€…: {scorer.filtering_rules.get('min_unique_authors')}")
        print(f"   - æœ€å°å­ç‰ˆå—æ•°: {scorer.filtering_rules.get('min_cross_subreddit_count')}")
        print(f"   - æœ€å°é¢‘ç‡è¯„åˆ†: {scorer.filtering_rules.get('min_avg_frequency_score')}")

        # è·å–ä¸€äº›èšç±»è¿›è¡Œæµ‹è¯•
        with db.get_connection("clusters") as conn:
            cursor = conn.execute("""
                SELECT c.*, COUNT(o.id) as opportunity_count
                FROM clusters c
                LEFT JOIN opportunities o ON c.id = o.cluster_id
                GROUP BY c.id
                ORDER BY c.id
                LIMIT 10
            """)
            clusters = [dict(row) for row in cursor.fetchall()]

        if not clusters:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°èšç±»æ•°æ®")
            return False

        print(f"\nğŸ“Š æ‰¾åˆ° {len(clusters)} ä¸ªèšç±»è¿›è¡Œæµ‹è¯•")

        # æµ‹è¯•æ¯ä¸ªèšç±»
        passed_count = 0
        skipped_count = 0

        for i, cluster in enumerate(clusters, 1):
            print(f"\n[{i}/{len(clusters)}] æµ‹è¯•èšç±»: {cluster['cluster_name'][:50]}...")
            print(f"   èšç±»å¤§å°: {cluster['cluster_size']}")
            print(f"   æœºä¼šæ•°é‡: {cluster['opportunity_count']}")

            # åº”ç”¨è¿‡æ»¤è§„åˆ™
            should_skip, skip_reason = scorer.should_skip_solution_design(cluster)

            if should_skip:
                print(f"   âŒ è·³è¿‡: {skip_reason}")
                skipped_count += 1
            else:
                print(f"   âœ… é€šè¿‡")
                passed_count += 1

            # è®¡ç®—å¹¶æ˜¾ç¤ºè¯¦ç»†æŒ‡æ ‡
            pain_event_ids = json.loads(cluster.get("pain_event_ids", "[]"))
            if pain_event_ids:
                unique_authors = scorer._calculate_unique_authors(pain_event_ids)
                cross_subreddits = scorer._calculate_cross_subreddit_count(pain_event_ids)
                avg_frequency = scorer._calculate_avg_frequency_score(pain_event_ids)

                print(f"   ğŸ“ˆ è¯¦ç»†æŒ‡æ ‡:")
                print(f"      - ç‹¬ç«‹ä½œè€…: {unique_authors}")
                print(f"      - å­ç‰ˆå—æ•°: {cross_subreddits}")
                print(f"      - é¢‘ç‡è¯„åˆ†: {avg_frequency:.1f}")

        print(f"\nğŸ“‹ æµ‹è¯•æ€»ç»“:")
        print(f"   - æ€»èšç±»æ•°: {len(clusters)}")
        print(f"   - é€šè¿‡è¿‡æ»¤: {passed_count}")
        print(f"   - è¢«è·³è¿‡: {skipped_count}")
        print(f"   - è·³è¿‡ç‡: {skipped_count/len(clusters)*100:.1f}%")

        # æµ‹è¯•é¢‘ç‡è¯„åˆ†æ˜ å°„
        print(f"\nğŸ¯ æµ‹è¯•é¢‘ç‡è¯„åˆ†æ˜ å°„:")
        test_frequencies = [
            ["daily", "weekly", "monthly"],
            ["sometimes", "often", "rarely"],
            ["æ¯å¤©", "æ¯å‘¨", "å¾ˆå°‘"],
            ["", "unknown", "invalid"]
        ]

        for freq_list in test_frequencies:
            score = scorer._frequency_to_score(freq_list)
            print(f"   {freq_list} -> {score:.1f}")

        return True

    except Exception as e:
        print(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_config_loading():
    """æµ‹è¯•é…ç½®åŠ è½½"""
    print("\nğŸ”§ æµ‹è¯•é…ç½®åŠ è½½...")

    try:
        scorer = ViabilityScorer()

        # æ£€æŸ¥é…ç½®æ˜¯å¦æ­£ç¡®åŠ è½½
        print(f"âœ… é…ç½®åŠ è½½æˆåŠŸ")
        print(f"   è¿‡æ»¤è§„åˆ™: {scorer.filtering_rules}")

        # æ£€æŸ¥é¢‘ç‡æ˜ å°„
        freq_mapping = scorer.filtering_rules.get("frequency_score_mapping", {})
        if freq_mapping:
            print(f"âœ… é¢‘ç‡æ˜ å°„é…ç½®: {len(freq_mapping)} ä¸ªæ˜ å°„")
        else:
            print("âš ï¸ é¢‘ç‡æ˜ å°„é…ç½®ä¸ºç©º")

        return True

    except Exception as e:
        print(f"âŒ é…ç½®åŠ è½½å¤±è´¥: {e}")
        return False

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹æµ‹è¯•è¿‡æ»¤è§„åˆ™åŠŸèƒ½\n")

    success = True

    # æµ‹è¯•é…ç½®åŠ è½½
    success &= test_config_loading()

    # æµ‹è¯•è¿‡æ»¤è§„åˆ™
    success &= test_filtering_rules()

    if success:
        print("\nâœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
        print("\nğŸ’¡ ä½¿ç”¨å»ºè®®:")
        print("   1. è¿è¡Œ python pipeline/score_viability.py --limit 50 æ¥åº”ç”¨è¿‡æ»¤è§„åˆ™")
        print("   2. æ£€æŸ¥æ—¥å¿—ä¸­çš„è·³è¿‡åŸå› æ¥è°ƒæ•´é˜ˆå€¼")
        print("   3. å¯ä»¥é€šè¿‡ä¿®æ”¹ config/thresholds.yaml æ¥è°ƒæ•´è¿‡æ»¤è§„åˆ™")
    else:
        print("\nâŒ æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")
        return 1

    return 0

if __name__ == "__main__":
    sys.exit(main())