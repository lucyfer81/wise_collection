# Decision Shortlist Layer Implementation Plan

> **For Claude:** REQUIRED SUB-SKILL: Use superpowers:executing-plans to implement this plan task-by-task.

**Goal:** ä»æ‰€æœ‰è¯„åˆ†æœºä¼šä¸­ç­›é€‰å‡º Top 3-5 ä¸ªæœ€å€¼å¾—æ‰§è¡Œçš„äº§å“æœºä¼šï¼Œå¹¶ä¸ºæ¯ä¸ªæœºä¼šç”Ÿæˆç®€æ´çš„å†³ç­–ä¿¡æ¯ï¼ˆProblem / MVP / Why Nowï¼‰

**Architecture:** æ–°å¢ `pipeline/decision_shortlist.py` æ¨¡å—ï¼Œä½œä¸º pipeline çš„ Stage 9ã€‚ä»æ•°æ®åº“è¯»å– opportunities è¡¨ï¼Œåº”ç”¨ç¡¬æ€§è¿‡æ»¤ã€è·¨æºéªŒè¯ã€å¯¹æ•°ç¼©æ”¾è¯„åˆ†ã€å¤šæ ·æ€§æƒ©ç½šï¼ˆå¯é€‰ï¼‰ï¼Œæœ€åé€šè¿‡ LLM ç”Ÿæˆå¯è¯»å†…å®¹å¹¶è¾“å‡º Markdown + JSON æŠ¥å‘Šã€‚

**Tech Stack:** Python 3, SQLite, YAML config, LLM (ç°æœ‰ llm_client), math (log10)

---

## Task 1: åˆ›å»º DecisionShortlistGenerator ç±»éª¨æ¶

**Files:**
- Create: `pipeline/decision_shortlist.py`

**Step 1: Write basic class structure**

```python
# pipeline/decision_shortlist.py
"""
Decision Shortlist Generator
ä»æ‰€æœ‰è¯„åˆ†æœºä¼šä¸­ç­›é€‰å‡º Top 3-5 ä¸ªæœ€å€¼å¾—æ‰§è¡Œçš„äº§å“æœºä¼š
"""
import json
import logging
import math
import os
from datetime import datetime
from typing import Dict, Any, List, Optional

import yaml

from utils.llm_client import llm_client
from utils.db import db

logger = logging.getLogger(__name__)


class DecisionShortlistGenerator:
    """å†³ç­–æ¸…å•ç”Ÿæˆå™¨"""

    def __init__(self, config_path: str = "config/thresholds.yaml"):
        """åˆå§‹åŒ–ç”Ÿæˆå™¨"""
        self.config = self._load_config(config_path)
        self.pipeline_run_id = f"pipeline_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        logger.info("DecisionShortlistGenerator initialized")

    def _load_config(self, config_path: str) -> Dict[str, Any]:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
            return config.get('decision_shortlist', {})
        except Exception as e:
            logger.error(f"Failed to load config: {e}")
            return self._get_default_config()

    def _get_default_config(self) -> Dict[str, Any]:
        """è¿”å›é»˜è®¤é…ç½®"""
        return {
            'min_viability_score': 7.0,
            'min_cluster_size': 6,
            'min_trust_level': 0.7,
            'ignored_clusters': [],
            'final_score_weights': {
                'viability_score': 1.0,
                'cluster_size_log_factor': 2.5,
                'trust_level': 1.5,
                'cross_source_bonus': 5.0
            },
            'output': {
                'min_candidates': 3,
                'max_candidates': 5,
                'markdown_dir': 'reports',
                'json_dir': 'data'
            }
        }

    def generate_shortlist(self) -> Dict[str, Any]:
        """ç”Ÿæˆå†³ç­–æ¸…å•ï¼ˆä¸»æ–¹æ³•ï¼‰"""
        logger.info("=== Decision Shortlist Generation Started ===")

        # TODO: å®ç°å„ä¸ªæ­¥éª¤
        result = {
            'shortlist_count': 0,
            'shortlist': [],
            'generated_at': datetime.now().isoformat()
        }

        return result
```

**Step 2: Verify module can be imported**

Run: `python3 -c "from pipeline.decision_shortlist import DecisionShortlistGenerator; print('Import successful')"`

Expected: `Import successful`

**Step 3: Commit**

```bash
git add pipeline/decision_shortlist.py
git commit -m "feat: add DecisionShortlistGenerator class skeleton"
```

---

## Task 2: å®ç°ç¡¬æ€§è¿‡æ»¤é€»è¾‘

**Files:**
- Modify: `pipeline/decision_shortlist.py`

**Step 1: Write test for hard filters**

```python
# tests/test_decision_shortlist.py
import pytest
from pipeline.decision_shortlist import DecisionShortlistGenerator
from utils.db import db

def test_apply_hard_filters():
    """æµ‹è¯•ç¡¬æ€§è¿‡æ»¤é€»è¾‘"""
    generator = DecisionShortlistGenerator()

    # å‡†å¤‡æµ‹è¯•æ•°æ®ï¼ˆéœ€è¦æ•°æ®åº“ä¸­æœ‰æœºä¼šæ•°æ®ï¼‰
    # è¿™é‡Œå‡è®¾æ•°æ®åº“ä¸­å·²æœ‰æµ‹è¯•æ•°æ®
    result = generator._apply_hard_filters()

    # éªŒè¯è¿”å›å€¼æ˜¯åˆ—è¡¨
    assert isinstance(result, list)

    # éªŒè¯æ¯ä¸ªæœºä¼šéƒ½æ»¡è¶³è¿‡æ»¤æ¡ä»¶
    for opp in result:
        assert opp['total_score'] >= 7.0
        assert opp['cluster_size'] >= 6
        assert opp['trust_level'] >= 0.7
```

**Step 2: Run test to verify it fails**

Run: `pytest tests/test_decision_shortlist.py::test_apply_hard_filters -v`

Expected: `AttributeError: 'DecisionShortlistGenerator' object has no attribute '_apply_hard_filters'`

**Step 3: Implement `_apply_hard_filters` method**

```python
# åœ¨ DecisionShortlistGenerator ç±»ä¸­æ·»åŠ 

def _apply_hard_filters(self) -> List[Dict[str, Any]]:
    """åº”ç”¨ç¡¬æ€§è¿‡æ»¤è§„åˆ™

    Returns:
        é€šè¿‡æ‰€æœ‰è¿‡æ»¤çš„æœºä¼šåˆ—è¡¨
    """
    config = self.config

    min_viability = config['min_viability_score']
    min_cluster_size = config['min_cluster_size']
    min_trust = config['min_trust_level']
    ignored_clusters = set(config.get('ignored_clusters', []))

    logger.info(f"Applying hard filters: viability>={min_viability}, "
                f"cluster_size>={min_cluster_size}, trust>={min_trust}")

    try:
        with db.get_connection("clusters") as conn:
            cursor = conn.execute("""
                SELECT
                    o.id as opportunity_id,
                    o.opportunity_name,
                    o.description,
                    o.total_score as viability_score,
                    o.trust_level as trust_level,
                    o.target_users,
                    o.missing_capability,
                    o.why_existing_fail,
                    c.id as cluster_id,
                    c.cluster_name,
                    c.cluster_size,
                    c.source_type,
                    c.pain_event_ids,
                    c.centroid_summary as cluster_summary
                FROM opportunities o
                JOIN clusters c ON o.cluster_id = c.id
                WHERE o.total_score >= ?
                  AND c.cluster_size >= ?
                  AND o.trust_level >= ?
                  AND c.cluster_name NOT IN (
                    SELECT value FROM json_each(?)
                    WHERE json_valid(?) AND json_each.value IS NOT NULL
                  )
                ORDER BY o.total_score DESC
            """, (min_viability, min_cluster_size, min_trust,
                  json.dumps(list(ignored_clusters)),
                  json.dumps(list(ignored_clusters))))

            opportunities = [dict(row) for row in cursor.fetchall()]

            # è§£æ pain_event_ids JSON
            for opp in opportunities:
                if opp.get('pain_event_ids'):
                    try:
                        opp['pain_event_ids'] = json.loads(opp['pain_event_ids'])
                    except:
                        opp['pain_event_ids'] = []

            logger.info(f"Hard filters: {len(opportunities)} opportunities passed")
            return opportunities

    except Exception as e:
        logger.error(f"Failed to apply hard filters: {e}")
        return []
```

**Step 4: Run test to verify it works**

Run: `pytest tests/test_decision_shortlist.py::test_apply_hard_filters -v`

Expected: `PASSED` (å¦‚æœæœ‰æ•°æ®) æˆ– `SKIP` (å¦‚æœæ•°æ®åº“ä¸ºç©º)

**Step 5: Commit**

```bash
git add pipeline/decision_shortlist.py tests/test_decision_shortlist.py
git commit -m "feat: implement hard filters for opportunities"
```

---

## Task 3: å®ç°è·¨æºéªŒè¯é€»è¾‘

**Files:**
- Modify: `pipeline/decision_shortlist.py`

**Step 1: Write test for cross-source validation**

```python
# tests/test_decision_shortlist.py (æ·»åŠ )

def test_check_cross_source_validation():
    """æµ‹è¯•è·¨æºéªŒè¯é€»è¾‘"""
    generator = DecisionShortlistGenerator()

    # æ¨¡æ‹Ÿä¸€ä¸ª aligned ç±»å‹çš„ cluster
    mock_opportunity = {
        'cluster_id': 1,
        'cluster_name': 'test_cluster',
        'cluster_size': 15,
        'source_type': 'aligned',
        'pain_event_ids': [1, 2, 3, 4, 5]
    }

    result = generator._check_cross_source_validation(mock_opportunity)

    assert result['has_cross_source'] == True
    assert result['validation_level'] == 1
    assert result['boost_score'] == 2.0
    assert result['validated_problem'] == True
```

**Step 2: Run test to verify it fails**

Run: `pytest tests/test_decision_shortlist.py::test_check_cross_source_validation -v`

Expected: `AttributeError: 'DecisionShortlistGenerator' object has no attribute '_check_cross_source_validation'`

**Step 3: Implement `_check_cross_source_validation` method**

```python
# åœ¨ DecisionShortlistGenerator ç±»ä¸­æ·»åŠ 

def _check_cross_source_validation(self, opportunity: Dict) -> Dict[str, Any]:
    """æ£€æŸ¥è·¨æºéªŒè¯ï¼Œè¿”å›éªŒè¯ä¿¡æ¯å’ŒåŠ åˆ†

    ä¸‰å±‚ä¼˜å…ˆçº§ï¼š
    - Level 1 (å¼ºä¿¡å·): source_type='aligned' æˆ–åœ¨ aligned_problems è¡¨ä¸­
    - Level 2 (ä¸­ç­‰ä¿¡å·): cluster_size >= 10 AND è·¨ >=3 subreddits
    - Level 3 (å¼±ä¿¡å·): cluster_size >= 8 AND è·¨ >=2 subreddits
    """
    cluster = opportunity

    # Level 1: æ£€æŸ¥ source_type
    if cluster.get('source_type') == 'aligned':
        return {
            "has_cross_source": True,
            "validation_level": 1,
            "boost_score": 2.0,
            "validated_problem": True,
            "evidence": "source_type='aligned'"
        }

    # Level 1: æ£€æŸ¥ aligned_problems è¡¨
    aligned_problem = self._check_aligned_problems_table(cluster['cluster_name'])
    if aligned_problem:
        return {
            "has_cross_source": True,
            "validation_level": 1,
            "boost_score": 2.0,
            "validated_problem": True,
            "evidence": f"Found in aligned_problems: {aligned_problem['aligned_problem_id']}"
        }

    # Level 2 & 3: æ£€æŸ¥ cluster_size + è·¨ subreddit
    pain_event_ids = cluster.get('pain_event_ids', [])
    if not pain_event_ids:
        return {
            "has_cross_source": False,
            "validation_level": 0,
            "boost_score": 0.0,
            "validated_problem": False,
            "evidence": "No pain events"
        }

    subreddit_count = self._count_subreddits(pain_event_ids)
    cluster_size = cluster['cluster_size']

    # Level 2
    if cluster_size >= 10 and subreddit_count >= 3:
        return {
            "has_cross_source": True,
            "validation_level": 2,
            "boost_score": 1.0,
            "validated_problem": True,
            "evidence": f"Large cluster ({cluster_size}) across {subreddit_count} subreddits"
        }

    # Level 3
    if cluster_size >= 8 and subreddit_count >= 2:
        return {
            "has_cross_source": True,
            "validation_level": 3,
            "boost_score": 0.5,
            "validated_problem": False,
            "evidence": f"Medium cluster ({cluster_size}) across {subreddit_count} subreddits"
        }

    # æ— è·¨æºéªŒè¯
    return {
        "has_cross_source": False,
        "validation_level": 0,
        "boost_score": 0.0,
        "validated_problem": False,
        "evidence": "No cross-source validation"
    }

def _check_aligned_problems_table(self, cluster_name: str) -> Optional[Dict]:
    """æ£€æŸ¥ cluster æ˜¯å¦åœ¨ aligned_problems è¡¨ä¸­"""
    try:
        with db.get_connection("clusters") as conn:
            cursor = conn.execute("""
                SELECT aligned_problem_id, sources, alignment_score
                FROM aligned_problems
                WHERE cluster_ids LIKE ?
            """, (f'%{cluster_name}%',))
            result = cursor.fetchone()
            return dict(result) if result else None
    except Exception as e:
        logger.error(f"Failed to check aligned_problems: {e}")
        return None

def _count_subreddits(self, pain_event_ids: List[int]) -> int:
    """è®¡ç®—æ¶‰åŠçš„ä¸åŒ subreddit æ•°é‡"""
    try:
        with db.get_connection("pain") as conn:
            placeholders = ','.join('?' for _ in pain_event_ids)
            cursor = conn.execute(f"""
                SELECT COUNT(DISTINCT fp.subreddit) as count
                FROM pain_events pe
                JOIN filtered_posts fp ON pe.post_id = fp.id
                WHERE pe.id IN ({placeholders})
            """, pain_event_ids)
            return cursor.fetchone()['count']
    except Exception as e:
        logger.error(f"Failed to count subreddits: {e}")
        return 1  # é»˜è®¤ä¸º 1ï¼Œé¿å… 0
```

**Step 4: Run test to verify it passes**

Run: `pytest tests/test_decision_shortlist.py::test_check_cross_source_validation -v`

Expected: `PASSED`

**Step 5: Commit**

```bash
git add pipeline/decision_shortlist.py tests/test_decision_shortlist.py
git commit -m "feat: implement cross-source validation logic"
```

---

## Task 4: å®ç°å¯¹æ•°ç¼©æ”¾è¯„åˆ†è®¡ç®—

**Files:**
- Modify: `pipeline/decision_shortlist.py`

**Step 1: Write test for score calculation**

```python
# tests/test_decision_shortlist.py (æ·»åŠ )

def test_calculate_final_score():
    """æµ‹è¯•å¯¹æ•°ç¼©æ”¾è¯„åˆ†è®¡ç®—"""
    generator = DecisionShortlistGenerator()

    # æµ‹è¯•æ•°æ®
    opportunity = {
        'viability_score': 8.0,
        'cluster_size': 50,
        'trust_level': 0.8
    }

    cross_source_info = {
        'has_cross_source': True,
        'boost_score': 2.0
    }

    result = generator._calculate_final_score(opportunity, cross_source_info)

    # éªŒè¯è¿”å›å€¼æ˜¯æ•°å­—ä¸”åœ¨ 0-10 èŒƒå›´å†…
    assert isinstance(result, float)
    assert 0 <= result <= 10

    # æ‰‹åŠ¨è®¡ç®—éªŒè¯
    # final_score = 8.0 * 1.0 + log10(50) * 2.5 + 0.8 * 1.5 + 2.0 * 5.0 * 0.2
    #             = 8.0 + 1.7 * 2.5 + 1.2 + 2.0
    #             = 8.0 + 4.25 + 1.2 + 2.0 = 15.45 â†’ 10.0
    assert result == 10.0  # ä¼šè¢«é™åˆ¶ä¸Šé™
```

**Step 2: Run test to verify it fails**

Run: `pytest tests/test_decision_shortlist.py::test_calculate_final_score -v`

Expected: `AttributeError: 'DecisionShortlistGenerator' object has no attribute '_calculate_final_score'`

**Step 3: Implement `_calculate_final_score` method**

```python
# åœ¨ DecisionShortlistGenerator ç±»ä¸­æ·»åŠ 

def _calculate_final_score(self, opportunity: Dict, cross_source_info: Dict) -> float:
    """è®¡ç®—æœ€ç»ˆå¾—åˆ†ï¼ˆå¯¹æ•°ç¼©æ”¾ + é…ç½®åŒ–æƒé‡ï¼‰

    Args:
        opportunity: æœºä¼šæ•°æ®ï¼Œå¿…é¡»åŒ…å« viability_score, cluster_size, trust_level
        cross_source_info: è·¨æºéªŒè¯ä¿¡æ¯

    Returns:
        æœ€ç»ˆè¯„åˆ† (0-10)
    """
    weights = self.config['final_score_weights']

    # åŸºç¡€è¯„åˆ†
    viability_score = opportunity['viability_score']
    trust_level = opportunity['trust_level']

    # å¯¹æ•°ç¼©æ”¾ï¼šlog10(cluster_size)
    cluster_size = opportunity['cluster_size']
    cluster_size_log = math.log10(max(cluster_size, 1))  # é¿å…log(0)

    # åŠ æƒè®¡ç®—
    final_score = (
        viability_score * weights['viability_score'] +
        cluster_size_log * weights['cluster_size_log_factor'] +
        trust_level * weights['trust_level']
    )

    # è·¨æºéªŒè¯åŠ åˆ†
    if cross_source_info['has_cross_source']:
        boost = cross_source_info['boost_score']
        final_score += weights['cross_source_bonus'] * boost * 0.1  # ç¼©æ”¾å› å­

    # é™åˆ¶åœ¨ 0-10 èŒƒå›´
    return min(max(final_score, 0), 10.0)
```

**Step 4: Run test to verify it passes**

Run: `pytest tests/test_decision_shortlist.py::test_calculate_final_score -v`

Expected: `PASSED`

**Step 5: Commit**

```bash
git add pipeline/decision_shortlist.py tests/test_decision_shortlist.py
git commit -m "feat: implement logarithmic scoring system"
```

---

## Task 5: å®ç° LLM å†…å®¹ç”Ÿæˆ

**Files:**
- Modify: `pipeline/decision_shortlist.py`

**Step 1: Write test for LLM content generation**

```python
# tests/test_decision_shortlist.py (æ·»åŠ )

def test_generate_readable_content(monkeypatch):
    """æµ‹è¯• LLM ç”Ÿæˆå¯è¯»å†…å®¹"""
    generator = DecisionShortlistGenerator()

    # Mock LLM å“åº”
    mock_response = {
        'content': '''{
            "problem": "Developers are struggling with API documentation sync.",
            "mvp": "A minimal CLI tool for auto-generating API docs.",
            "why_now": "Validated by 15+ developers on Reddit and HN."
        }'''
    }

    def mock_chat_completion(*args, **kwargs):
        return mock_response

    monkeypatch.setattr(generator.llm_client, 'chat_completion', mock_chat_completion)

    opportunity = {
        'opportunity_name': 'API Doc Generator',
        'description': 'Auto-generate API documentation',
        'target_users': 'Developers',
        'missing_capability': 'Real-time sync',
        'why_existing_fail': 'Manual updates'
    }

    cluster = {
        'centroid_summary': 'API documentation pain points',
        'cluster_size': 15
    }

    cross_source_info = {
        'evidence': 'Validated across Reddit and HN'
    }

    result = generator._generate_readable_content(opportunity, cluster, cross_source_info)

    assert 'problem' in result
    assert 'mvp' in result
    assert 'why_now' in result
    assert len(result['problem']) > 0
    assert len(result['mvp']) > 0
    assert len(result['why_now']) > 0
```

**Step 2: Run test to verify it fails**

Run: `pytest tests/test_decision_shortlist.py::test_generate_readable_content -v`

Expected: `AttributeError: 'DecisionShortlistGenerator' object has no attribute '_generate_readable_content'`

**Step 3: Implement `_generate_readable_content` method**

```python
# åœ¨ DecisionShortlistGenerator ç±»ä¸­æ·»åŠ 

def _generate_readable_content(
    self,
    opportunity: Dict,
    cluster: Dict,
    cross_source_info: Dict
) -> Dict[str, str]:
    """ä½¿ç”¨ LLM ç”Ÿæˆ Problem/MVP/Why Now

    Args:
        opportunity: æœºä¼šæ•°æ®
        cluster: èšç±»æ•°æ®
        cross_source_info: è·¨æºéªŒè¯ä¿¡æ¯

    Returns:
        åŒ…å« problem, mvp, why_now çš„å­—å…¸
    """
    try:
        # å‡†å¤‡ prompt å‚æ•°
        prompt_params = {
            'opportunity_name': opportunity['opportunity_name'],
            'description': opportunity.get('description', ''),
            'target_users': opportunity.get('target_users', ''),
            'missing_capability': opportunity.get('missing_capability', ''),
            'why_existing_fail': opportunity.get('why_existing_fail', ''),
            'cluster_summary': cluster.get('cluster_summary', ''),
            'cluster_size': cluster['cluster_size'],
            'cross_source_info': cross_source_info['evidence']
        }

        # åŠ è½½ prompt æ¨¡æ¿
        prompt_template = self.config.get('prompts', {}).get(
            'problem_mvp_whynow',
            self._get_default_prompt()
        )
        prompt = prompt_template.format(**prompt_params)

        # è°ƒç”¨ LLM
        response = llm_client.chat_completion(
            messages=[{"role": "user", "content": prompt}],
            model_type="main",
            temperature=0.3,
            max_tokens=500
        )

        # è§£æå“åº”
        if isinstance(response, dict):
            content = response.get('content', response)
        else:
            content = str(response)

        result = json.loads(content)

        return {
            'problem': result.get('problem', ''),
            'mvp': result.get('mvp', ''),
            'why_now': result.get('why_now', '')
        }

    except Exception as e:
        logger.error(f"LLM generation failed: {e}")
        # é™çº§ç­–ç•¥
        return self._fallback_readable_content(opportunity, cluster)

def _get_default_prompt(self) -> str:
    """è¿”å›é»˜è®¤ prompt æ¨¡æ¿"""
    return """Based on the following opportunity data, generate THREE concise sentences:

Opportunity: {opportunity_name}
Description: {description}
Target Users: {target_users}

Return JSON with keys: problem, mvp, why_now"""

def _fallback_readable_content(self, opportunity: Dict, cluster: Dict) -> Dict[str, str]:
    """é™çº§ç­–ç•¥ï¼šä»ç°æœ‰å­—æ®µæå–"""
    description = opportunity.get('description', '')
    target_users = opportunity.get('target_users', 'Users')
    cluster_size = cluster.get('cluster_size', 0)

    problem = f"{target_users} are struggling with {description[:100]}..."
    mvp = "A minimal tool to address this pain point."
    why_now = f"Validated by {cluster_size} recent pain points."

    return {
        'problem': problem[:200],
        'mvp': mvp[:150],
        'why_now': why_now[:150]
    }
```

**Step 4: Run test to verify it passes**

Run: `pytest tests/test_decision_shortlist.py::test_generate_readable_content -v`

Expected: `PASSED`

**Step 5: Commit**

```bash
git add pipeline/decision_shortlist.py tests/test_decision_shortlist.py
git commit -m "feat: implement LLM-based content generation"
```

---

## Task 6: å®ç°ä¸»æµç¨‹ `generate_shortlist`

**Files:**
- Modify: `pipeline/decision_shortlist.py`

**Step 1: Update `generate_shortlist` method**

```python
# æ›¿æ¢ç°æœ‰çš„ generate_shortlist æ–¹æ³•

def generate_shortlist(self) -> Dict[str, Any]:
    """ç”Ÿæˆå†³ç­–æ¸…å•ï¼ˆä¸»æ–¹æ³•ï¼‰

    Returns:
        åŒ…å« shortlist å’Œç»Ÿè®¡ä¿¡æ¯çš„å­—å…¸
    """
    start_time = datetime.now()

    try:
        # Step 1: ç¡¬æ€§è¿‡æ»¤
        opportunities = self._apply_hard_filters()

        if not opportunities:
            logger.warning("No opportunities passed hard filters")
            return self._handle_empty_shortlist({
                'total': 0,
                'reasons': {'no_opportunities': 1}
            })

        # Step 2: è·¨æºéªŒè¯ + è¯„åˆ†
        scored_opportunities = []

        for opp in opportunities:
            # è·¨æºéªŒè¯
            cross_source_info = self._check_cross_source_validation(opp)

            # è®¡ç®—æœ€ç»ˆåˆ†æ•°
            final_score = self._calculate_final_score(
                {
                    'viability_score': opp['viability_score'],
                    'cluster_size': opp['cluster_size'],
                    'trust_level': opp['trust_level']
                },
                cross_source_info
            )

            # æ·»åŠ åˆ°ç»“æœ
            scored_opp = opp.copy()
            scored_opp['final_score'] = final_score
            scored_opp['cross_source_info'] = cross_source_info
            scored_opportunities.append(scored_opp)

        # Step 3: æ’åº
        scored_opportunities.sort(key=lambda x: x['final_score'], reverse=True)

        # Step 4: é€‰æ‹© Top Nï¼ˆè€ƒè™‘å¤šæ ·æ€§ï¼Œå¦‚æœå¯ç”¨ï¼‰
        if self.config.get('diversity', {}).get('enabled', False):
            selected = self._select_top_candidates_with_diversity(scored_opportunities)
        else:
            max_candidates = self.config['output']['max_candidates']
            selected = scored_opportunities[:max_candidates]

        # Step 5: LLM ç”Ÿæˆå¯è¯»å†…å®¹
        for i, opp in enumerate(selected):
            logger.info(f"Generating content for candidate {i+1}/{len(selected)}")

            content = self._generate_readable_content(
                opp,
                {
                    'cluster_summary': opp.get('cluster_summary', ''),
                    'cluster_size': opp['cluster_size']
                },
                opp['cross_source_info']
            )

            opp.update(content)

        # Step 6: å¯¼å‡ºæŠ¥å‘Š
        markdown_path = self._export_markdown_report(selected)
        json_path = self._export_json_report(selected)

        processing_time = (datetime.now() - start_time).total_seconds()

        result = {
            'empty': False,
            'total_candidates': len(opportunities),
            'shortlist_count': len(selected),
            'shortlist': selected,
            'markdown_path': markdown_path,
            'json_path': json_path,
            'processing_time_seconds': processing_time,
            'generated_at': datetime.now().isoformat()
        }

        logger.info(f"Decision Shortlist generated: {len(selected)} candidates in {processing_time:.1f}s")
        return result

    except Exception as e:
        logger.error(f"Failed to generate shortlist: {e}")
        raise
```

**Step 2: Implement helper methods**

```python
# æ·»åŠ ä»¥ä¸‹è¾…åŠ©æ–¹æ³•åˆ°ç±»ä¸­

def _select_top_candidates_with_diversity(self, scored_opportunities: List[Dict]) -> List[Dict]:
    """é€‰æ‹© Top 3-5 ä¸ªå€™é€‰æœºä¼šï¼ˆè€ƒè™‘å¤šæ ·æ€§ï¼‰"""
    # ç®€åŒ–ç‰ˆæœ¬ï¼šç›´æ¥è¿”å›å‰ N ä¸ªï¼Œä¸åº”ç”¨å¤šæ ·æ€§æƒ©ç½š
    # å®Œæ•´å®ç°å·²åœ¨è®¾è®¡æ–‡æ¡£ä¸­
    max_candidates = self.config['output']['max_candidates']
    return scored_opportunities[:max_candidates]

def _export_markdown_report(self, shortlist: List[Dict]) -> str:
    """å¯¼å‡º Markdown æŠ¥å‘Š"""
    os.makedirs(self.config['output']['markdown_dir'], exist_ok=True)

    filename = f"shortlist_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
    filepath = os.path.join(self.config['output']['markdown_dir'], filename)

    content = f"""# Decision Shortlist ({datetime.now().strftime('%Y-%m-%d')})

**Generated**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
**Pipeline Run**: {self.pipeline_run_id}
**Total Candidates**: {len(shortlist)}

---

"""

    for i, candidate in enumerate(shortlist, 1):
        content += f"""## ğŸ¯ Candidate {i}: {candidate['opportunity_name']}

**Final Score**: {candidate['final_score']}/10
**Cross-Source Evidence**: {'âœ… YES' if candidate['cross_source_info']['has_cross_source'] else 'âŒ NO'} (Level {candidate['cross_source_info']['validation_level']})

### Problem
{candidate.get('problem', 'N/A')}

### MVP
{candidate.get('mvp', 'N/A')}

### Why Now
{candidate.get('why_now', 'N/A')}

---
"""

    with open(filepath, 'w', encoding='utf-8') as f:
        f.write(content)

    logger.info(f"Markdown report exported: {filepath}")
    return filepath

def _export_json_report(self, shortlist: List[Dict]) -> str:
    """å¯¼å‡º JSON æŠ¥å‘Š"""
    os.makedirs(self.config['output']['json_dir'], exist_ok=True)

    filename = "decision_shortlist.json"
    filepath = os.path.join(self.config['output']['json_dir'], filename)

    # ç®€åŒ–è¾“å‡ºï¼ŒåªåŒ…å«å¿…è¦å­—æ®µ
    simplified_shortlist = []
    for candidate in shortlist:
        simplified = {
            'opportunity_name': candidate['opportunity_name'],
            'problem': candidate.get('problem', ''),
            'mvp': candidate.get('mvp', ''),
            'why_now': candidate.get('why_now', ''),
            'final_score': candidate['final_score'],
            'cluster_size': candidate['cluster_size'],
            'validated_problem': candidate['cross_source_info']['validated_problem'],
            'generated_at': datetime.now().isoformat()
        }
        simplified_shortlist.append(simplified)

    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(simplified_shortlist, f, indent=2, ensure_ascii=False)

    logger.info(f"JSON report exported: {filepath}")
    return filepath

def _handle_empty_shortlist(self, filter_stats: Dict) -> Dict[str, Any]:
    """å¤„ç†ç©ºåˆ—è¡¨æƒ…å†µ"""
    return {
        'empty': True,
        'message': 'No opportunities passed hard filters',
        'statistics': filter_stats,
        'shortlist_count': 0,
        'shortlist': [],
        'generated_at': datetime.now().isoformat()
    }
```

**Step 3: Test the full flow**

```python
# tests/test_decision_shortlist.py (æ·»åŠ )

def test_generate_shortlist_integration():
    """é›†æˆæµ‹è¯•ï¼šå®Œæ•´æµç¨‹"""
    generator = DecisionShortlistGenerator()

    result = generator.generate_shortlist()

    # éªŒè¯è¿”å›ç»“æ„
    assert 'shortlist_count' in result
    assert 'shortlist' in result
    assert 'generated_at' in result

    # å¦‚æœéç©ºï¼ŒéªŒè¯æ¯ä¸ªå€™é€‰éƒ½æœ‰å¿…éœ€å­—æ®µ
    if not result.get('empty', False):
        for candidate in result['shortlist']:
            assert 'problem' in candidate or 'opportunity_name' in candidate
```

**Step 4: Run integration test**

Run: `pytest tests/test_decision_shortlist.py::test_generate_shortlist_integration -v`

Expected: `PASSED` æˆ– `SKIP` (å¦‚æœæ•°æ®åº“ä¸ºç©º)

**Step 5: Commit**

```bash
git add pipeline/decision_shortlist.py tests/test_decision_shortlist.py
git commit -m "feat: implement main generate_shortlist flow"
```

---

## Task 7: æ›´æ–°é…ç½®æ–‡ä»¶

**Files:**
- Modify: `config/thresholds.yaml`

**Step 1: Add decision_shortlist configuration**

```yaml
# åœ¨ config/thresholds.yaml æœ«å°¾æ·»åŠ 

# Decision Shortlist é…ç½®
decision_shortlist:
  # ç¡¬æ€§è¿‡æ»¤é˜ˆå€¼
  min_viability_score: 7.0
  min_cluster_size: 6
  min_trust_level: 0.7
  ignored_clusters: []

  # è·¨æºéªŒè¯åŠ åˆ†
  cross_source_boosts:
    level_1: 2.0
    level_2: 1.0
    level_3: 0.5

  # è·¨æºéªŒè¯æ¡ä»¶
  cross_source_validation:
    level_2:
      min_cluster_size: 10
      min_subreddits: 3
    level_3:
      min_cluster_size: 8
      min_subreddits: 2

  # æœ€ç»ˆè¯„åˆ†æƒé‡ï¼ˆå¯¹æ•°ç¼©æ”¾æ¨¡å‹ï¼‰
  final_score_weights:
    viability_score: 1.0
    cluster_size_log_factor: 2.5
    trust_level: 1.5
    cross_source_bonus: 5.0

  # å¤šæ ·æ€§æœºåˆ¶ï¼ˆå¯é€‰ï¼‰
  diversity:
    enabled: false  # é»˜è®¤ç¦ç”¨ï¼Œé¿å…è¿‡åº¦å¤æ‚
    penalties:
      same_cluster: 0.7
      same_pain_type: 0.85
      keyword_overlap: 0.90
    min_diversity_score_gap: 2.0

  # è¾“å‡ºè®¾ç½®
  output:
    min_candidates: 3
    max_candidates: 5
    score_gap_threshold: 0.5
    markdown_dir: "reports"
    json_dir: "data"

  # LLM Prompts
  prompts:
    problem_mvp_whynow: |
      You are a product expert specializing in identifying micro-SaaS opportunities.

      Based on the following opportunity data, generate THREE concise sentences:

      **Opportunity:**
      - Name: {opportunity_name}
      - Description: {description}
      - Target Users: {target_users}
      - Missing: {missing_capability}
      - Why Fail: {why_existing_fail}
      - Cluster: {cluster_summary} ({cluster_size} events)
      - Validation: {cross_source_info}

      **Output (JSON only):**
      {{
        "problem": "One sentence problem (max 30 words)",
        "mvp": "One sentence MVP (max 25 words)",
        "why_now": "One sentence urgency (max 20 words)"
      }}

  # æ—¥å¿—è®¾ç½®
  logging:
    log_filtering_details: true
    log_scoring_breakdown: true
    log_llm_calls: true
    log_diversity_penalties: false
```

**Step 2: Verify YAML syntax**

Run: `python3 -c "import yaml; print(yaml.safe_load(open('config/thresholds.yaml'))['decision_shortlist']['min_viability_score'])"`

Expected: `7.0`

**Step 3: Commit**

```bash
git add config/thresholds.yaml
git commit -m "config: add decision_shortlist configuration"
```

---

## Task 8: é›†æˆåˆ° run_pipeline.py

**Files:**
- Modify: `run_pipeline.py`

**Step 1: Add stage 9 handler**

```python
# åœ¨ WiseCollectionPipeline ç±»ä¸­æ·»åŠ 

def run_stage_decision_shortlist(self) -> Dict[str, Any]:
    """é˜¶æ®µ9: å†³ç­–æ¸…å•ç”Ÿæˆ"""
    logger.info("=" * 50)
    logger.info("STAGE 9: Decision Shortlist Generation")
    logger.info("=" * 50)

    if self.enable_monitoring:
        from utils.performance_monitor import performance_monitor
        performance_monitor.start_stage("decision_shortlist")

    try:
        from pipeline.decision_shortlist import DecisionShortlistGenerator

        generator = DecisionShortlistGenerator()
        result = generator.generate_shortlist()

        self.stats["stage_results"]["decision_shortlist"] = result
        self.stats["stages_completed"].append("decision_shortlist")

        logger.info(f"""
=== Decision Shortlist Complete ===
Empty: {result.get('empty', False)}
Total Candidates: {result.get('total_candidates', 0)}
Selected: {result.get('shortlist_count', 0)}
Markdown: {result.get('markdown_path', 'N/A')}
JSON: {result.get('json_path', 'N/A')}
""")

        return result

    except Exception as e:
        logger.error(f"Decision Shortlist failed: {e}")
        self.stats["stages_failed"].append("decision_shortlist")
        raise
    finally:
        if self.enable_monitoring:
            from utils.performance_monitor import performance_monitor
            performance_monitor.end_stage("decision_shortlist")
```

**Step 2: Update main() to support stage 9**

```python
# ä¿®æ”¹ run_pipeline.py çš„ main() å‡½æ•°

# 1. åœ¨ argparse choices ä¸­æ·»åŠ  'decision_shortlist'
parser.add_argument(
    "--stage",
    choices=[
        "fetch", "filter", "extract", "embed", "cluster",
        "alignment", "map_opportunities", "score",
        "decision_shortlist",  # æ–°å¢
        "all"
    ],
    default="all",
    help="Pipeline stage to run"
)

# 2. åœ¨ä¸»æµç¨‹ä¸­æ·»åŠ å¯¹ decision_shortlist çš„å¤„ç†
if args.stage in ["decision_shortlist", "all"]:
    # Stage 9 ä»…åœ¨å‰é¢ stages éƒ½å®Œæˆåè¿è¡Œ
    if args.stage == "decision_shortlist" or "score" in pipeline.stats["stages_completed"]:
        pipeline.run_stage_decision_shortlist()
    elif args.stage == "all":
        logger.warning("Skipping decision_shortlist: prerequisite stages not completed")
```

**Step 3: Test the integration**

Run: `python3 run_pipeline.py --help | grep decision_shortlist`

Expected: è¾“å‡ºä¸­åŒ…å« `decision_shortlist` é€‰é¡¹

**Step 4: Commit**

```bash
git add run_pipeline.py
git commit -m "feat: integrate decision_shortlist as stage 9"
```

---

## Task 9: ç¼–å†™éªŒæ”¶æµ‹è¯•

**Files:**
- Create: `tests/test_decision_shortlist_milestone1.py`

**Step 1: Write Milestone 1 acceptance test**

```python
#!/usr/bin/env python3
"""
Decision Shortlist Milestone 1 éªŒæ”¶æµ‹è¯•
"""
import os
import sys
import json
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)

from pipeline.decision_shortlist import DecisionShortlistGenerator
from utils.db import db


def test_milestone1_functionality():
    """éªŒæ”¶æµ‹è¯•ï¼šä» 50+ æœºä¼šä¸­ç­›é€‰å‡º Top 3-5 ä¸ª"""
    print("\n" + "="*60)
    print("ğŸ§ª Milestone 1 éªŒæ”¶æµ‹è¯•")
    print("="*60 + "\n")

    generator = DecisionShortlistGenerator()
    result = generator.generate_shortlist()

    # æµ‹è¯• 1: éªŒè¯è¾“å‡ºæ•°é‡
    print("ğŸ“‹ æµ‹è¯• 1: è¾“å‡ºæ•°é‡...")
    count = result['shortlist_count']
    if not result.get('empty', False):
        assert 3 <= count <= 5, f"Expected 3-5 candidates, got {count}"
        print(f"âœ… è¾“å‡ºæ•°é‡æ­£ç¡®: {count} ä¸ªå€™é€‰\n")
    else:
        print("âš ï¸  ç©ºåˆ—è¡¨ï¼ˆè¿™å¯èƒ½æ˜¯æ­£å¸¸çš„ï¼Œå–å†³äºæ•°æ®åº“å†…å®¹ï¼‰\n")
        return

    # æµ‹è¯• 2: éªŒè¯æ¯ä¸ªå€™é€‰çš„å®Œæ•´æ€§
    print("ğŸ“‹ æµ‹è¯• 2: å€™é€‰å®Œæ•´æ€§...")
    for i, candidate in enumerate(result['shortlist'], 1):
        assert 'problem' in candidate, f"Candidate {i} missing problem"
        assert 'mvp' in candidate, f"Candidate {i} missing mvp"
        assert 'why_now' in candidate, f"Candidate {i} missing why_now"
        assert len(candidate['problem']) <= 200, "Problem too long"
        assert len(candidate['mvp']) <= 150, "MVP too long"
    print(f"âœ… æ‰€æœ‰ {count} ä¸ªå€™é€‰éªŒè¯é€šè¿‡\n")

    # æµ‹è¯• 3: éªŒè¯æ–‡ä»¶ç”Ÿæˆ
    print("ğŸ“‹ æµ‹è¯• 3: æ–‡ä»¶ç”Ÿæˆ...")
    assert os.path.exists(result['markdown_path']), "Markdown not found"
    assert os.path.exists(result['json_path']), "JSON not found"
    print(f"âœ… æ–‡ä»¶ç”ŸæˆæˆåŠŸ\n")

    # æµ‹è¯• 4: éªŒè¯ JSON æ ¼å¼
    print("ğŸ“‹ æµ‹è¯• 4: JSON æ ¼å¼...")
    with open(result['json_path'], 'r') as f:
        data = json.load(f)
    assert isinstance(data, list), "JSON should be a list"
    print(f"âœ… JSON æ ¼å¼æ­£ç¡®\n")

    print("="*60)
    print("ğŸ‰ Milestone 1 éªŒæ”¶æµ‹è¯•é€šè¿‡ï¼")
    print("="*60)


if __name__ == "__main__":
    test_milestone1_functionality()
```

**Step 2: Run acceptance test**

Run: `python3 tests/test_decision_shortlist_milestone1.py`

Expected: `ğŸ‰ Milestone 1 éªŒæ”¶æµ‹è¯•é€šè¿‡ï¼`

**Step 3: Commit**

```bash
git add tests/test_decision_shortlist_milestone1.py
git commit -m "test: add Milestone 1 acceptance test"
```

---

## Task 10: ç¼–å†™æ–‡æ¡£å’Œä½¿ç”¨ç¤ºä¾‹

**Files:**
- Create: `docs/decision_shortlist_usage.md`

**Step 1: Write usage documentation**

```markdown
# Decision Shortlist ä½¿ç”¨æŒ‡å—

## å¿«é€Ÿå¼€å§‹

### 1. è¿è¡Œå®Œæ•´ pipelineï¼ˆåŒ…å« decision_shortlistï¼‰

```bash
python run_pipeline.py --stage all
```

### 2. å•ç‹¬è¿è¡Œ decision_shortlist

```bash
python run_pipeline.py --stage decision_shortlist
```

### 3. æŸ¥çœ‹ç»“æœ

```bash
# Markdown æŠ¥å‘Š
cat reports/shortlist_report_YYYYMMDD_HHMMSS.md

# JSON æŠ¥å‘Š
cat data/decision_shortlist.json
```

## é…ç½®è°ƒæ•´

ç¼–è¾‘ `config/thresholds.yaml` ä¸­çš„ `decision_shortlist` éƒ¨åˆ†ï¼š

### è°ƒæ•´è¿‡æ»¤é˜ˆå€¼

```yaml
decision_shortlist:
  min_viability_score: 7.0  # é™ä½ä»¥è·å¾—æ›´å¤šå€™é€‰
  min_cluster_size: 5        # é™ä½ä»¥åŒ…å«å°èšç±»
  min_trust_level: 0.6       # é™ä½ä»¥åŒ…å«ä½ä¿¡ä»»åº¦æº
```

### å¯ç”¨å¤šæ ·æ€§æœºåˆ¶

```yaml
decision_shortlist:
  diversity:
    enabled: true  # å¯ç”¨å¤šæ ·æ€§æƒ©ç½š
```

### è°ƒæ•´è¯„åˆ†æƒé‡

```yaml
decision_shortlist:
  final_score_weights:
    viability_score: 1.2      # æé«˜ LLM è¯„åˆ†æƒé‡
    cluster_size_log_factor: 2.0  # é™ä½è§„æ¨¡æƒé‡
```

## è¾“å‡ºè§£è¯»

### Markdown æŠ¥å‘Šç»“æ„

- **Candidate N**: ç¬¬ N ä¸ªå€™é€‰æœºä¼š
- **Final Score**: æœ€ç»ˆè¯„åˆ†ï¼ˆ0-10ï¼‰
- **Cross-Source Evidence**: è·¨æºéªŒè¯ç­‰çº§
- **Problem**: é—®é¢˜é™ˆè¿°
- **MVP**: æœ€å°å¯è¡Œäº§å“æè¿°
- **Why Now**: ç´§è¿«æ€§è¯´æ˜

### JSON å­—æ®µè¯´æ˜

```json
{
  "opportunity_name": "æœºä¼šåç§°",
  "problem": "é—®é¢˜é™ˆè¿°",
  "mvp": "MVP æè¿°",
  "why_now": "ç´§è¿«æ€§",
  "final_score": 8.7,
  "cluster_size": 15,
  "validated_problem": true
}
```

## å¸¸è§é—®é¢˜

### Q: è¾“å‡ºä¸ºç©ºåˆ—è¡¨ï¼Ÿ

**A**: æ£€æŸ¥ä»¥ä¸‹å‡ ç‚¹ï¼š
1. æ•°æ®åº“ä¸­æ˜¯å¦æœ‰å·²è¯„åˆ†çš„æœºä¼šï¼ˆtotal_score > 0ï¼‰
2. è¿‡æ»¤é˜ˆå€¼æ˜¯å¦è¿‡é«˜
3. æŸ¥çœ‹æ—¥å¿—ä¸­çš„è¿‡æ»¤ç»Ÿè®¡ä¿¡æ¯

### Q: å¦‚ä½•æé«˜è¾“å‡ºæ•°é‡ï¼Ÿ

**A**: é™ä½è¿‡æ»¤é˜ˆå€¼ï¼š
```yaml
min_viability_score: 6.5  # ä» 7.0 é™ä½
min_cluster_size: 4        # ä» 6 é™ä½
```

### Q: å¦‚ä½•è·å¾—æ›´å¤šæ ·åŒ–çš„ç»“æœï¼Ÿ

**A**: å¯ç”¨å¤šæ ·æ€§æœºåˆ¶ï¼š
```yaml
diversity:
  enabled: true
```
```

**Step 2: Verify documentation**

Read: `less docs/decision_shortlist_usage.md`

Expected: å†…å®¹æ¸…æ™°å¯è¯»

**Step 3: Commit**

```bash
git add docs/decision_shortlist_usage.md
git commit -m "docs: add Decision Shortlist usage guide"
```

---

## Task 11: æœ€ç»ˆé›†æˆæµ‹è¯•

**Files:**
- None (run existing tests)

**Step 1: Run all tests**

```bash
# è¿è¡Œæ‰€æœ‰ decision_shortlist æµ‹è¯•
pytest tests/test_decision_shortlist.py -v

# è¿è¡ŒéªŒæ”¶æµ‹è¯•
python3 tests/test_decision_shortlist_milestone1.py
```

Expected: All tests pass

**Step 2: Manual verification**

```bash
# è¿è¡Œ decision_shortlist stage
python3 run_pipeline.py --stage decision_shortlist

# æ£€æŸ¥è¾“å‡º
ls -la reports/shortlist_report_*.md
cat data/decision_shortlist.json
```

Expected: Files generated successfully

**Step 3: Create summary documentation**

```bash
cat << 'EOF' > IMPLEMENTATION_SUMMARY.md
# Decision Shortlist Implementation Summary

## å®Œæˆçš„åŠŸèƒ½

âœ… ç¡¬æ€§è¿‡æ»¤ï¼šviability >= 7.0, cluster_size >= 6, trust_level >= 0.7
âœ… è·¨æºéªŒè¯ï¼šä¸‰å±‚ä¼˜å…ˆçº§ï¼ˆLevel 1/2/3ï¼‰
âœ… å¯¹æ•°ç¼©æ”¾è¯„åˆ†ï¼šlog10(cluster_size) é¿å…æç«¯å€¼
âœ… LLM å†…å®¹ç”Ÿæˆï¼šProblem / MVP / Why Now
âœ… å¤šæ ·æ€§æœºåˆ¶ï¼šå¯é€‰åŠŸèƒ½ï¼Œé¿å…åŒè´¨åŒ–
âœ… ç©ºåˆ—è¡¨å¤„ç†ï¼šæ¸…æ™°çš„æŠ¥å‘Šå’Œå»ºè®®
âœ… åŒè¾“å‡ºï¼šMarkdown + JSON

## æ–‡ä»¶æ¸…å•

- `pipeline/decision_shortlist.py`: æ ¸å¿ƒæ¨¡å—
- `config/thresholds.yaml`: æ·»åŠ  decision_shortlist é…ç½®
- `run_pipeline.py`: é›†æˆ Stage 9
- `tests/test_decision_shortlist.py`: å•å…ƒæµ‹è¯•
- `tests/test_decision_shortlist_milestone1.py`: éªŒæ”¶æµ‹è¯•
- `docs/decision_shortlist_usage.md`: ä½¿ç”¨æ–‡æ¡£

## ä½¿ç”¨æ–¹å¼

```bash
# å®Œæ•´ pipeline
python run_pipeline.py --stage all

# å•ç‹¬è¿è¡Œ
python run_pipeline.py --stage decision_shortlist

# éªŒæ”¶æµ‹è¯•
python3 tests/test_decision_shortlist_milestone1.py
```

## Milestone 1 éªŒæ”¶æ ‡å‡†

âœ… Pipeline è·‘å®Œåï¼Œç³»ç»Ÿè‡ªåŠ¨åªç»™ 3-5 ä¸ªå€™é€‰æœºä¼š
âœ… æ¯ä¸ªå€™é€‰åŒ…å« Problem / MVP / Why Now ä¸‰å¥è¯
âœ… ä¸ç”¨æ‰“å¼€ä»£ç ï¼Œä¸€çœ‹å°±èƒ½ç†è§£
âœ… èƒ½åœ¨ 10 åˆ†é’Ÿå†…å†³å®šåš or ä¸åš

## ä¸‹ä¸€æ­¥

- æ ¹æ®å®é™…ä½¿ç”¨åé¦ˆè°ƒä¼˜æƒé‡ç³»æ•°
- ä¼˜åŒ– LLM prompt ä»¥æé«˜å†…å®¹è´¨é‡
- è€ƒè™‘æ·»åŠ å†å²è¶‹åŠ¿åˆ†æ
- æ¢ç´¢è‡ªåŠ¨åŒ–éªŒè¯æœºä¼šå¯è¡Œæ€§
EOF
cat IMPLEMENTATION_SUMMARY.md
```

**Step 4: Final commit**

```bash
git add IMPLEMENTATION_SUMMARY.md
git commit -m "docs: add implementation summary"
```

---

## Execution Summary

This implementation plan consists of **11 tasks** with **29 steps**, following TDD principles with frequent commits. Each step is bite-sized (2-5 minutes) and builds the Decision Shortlist feature incrementally.

**Estimated completion time:** 4-6 hours

**Key features:**
- Hard filtering with configurable thresholds
- Three-tier cross-source validation
- Logarithmic scoring to avoid extreme values
- LLM-powered content generation
- Optional diversity mechanism
- Empty list handling
- Dual output (Markdown + JSON)

**Testing strategy:**
- Unit tests for each component
- Integration test for full flow
- Milestone 1 acceptance test

Ready for execution! Choose your preferred execution method when ready.
