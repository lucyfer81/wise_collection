#!/usr/bin/env python3
"""
Wise Collection - Main Pipeline Runner
ä¸»è¦çš„pipelineæ‰§è¡Œè„šæœ¬ - ä¸€é”®è¿è¡Œæ•´ä¸ªæ•°æ®æ”¶é›†æµç¨‹
"""
import os
import sys
import argparse
import logging
import json
import time
from datetime import datetime
from typing import Dict, Any, Optional, List

# è®¾ç½®é¡¹ç›®æ ¹ç›®å½•
project_root = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, project_root)

# å¯¼å…¥pipelineæ¨¡å—
from pipeline.fetch import RedditSourceFetcher
from pipeline.filter_signal import PainSignalFilter
from pipeline.extract_pain import PainPointExtractor
from pipeline.embed import PainEventEmbedder
from pipeline.cluster import PainEventClusterer
from pipeline.score_viability import ViabilityScorer
from pipeline.map_opportunity import OpportunityMapper
from pipeline.align_cross_sources import CrossSourceAligner
from pipeline.decision_shortlist import DecisionShortlistGenerator

# å¯¼å…¥å·¥å…·æ¨¡å—
from utils.db import db
from utils.llm_client import LLMClient
from utils.performance_monitor import performance_monitor

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/pipeline.log'),
        logging.StreamHandler(sys.stdout)
    ]
)

logger = logging.getLogger(__name__)

class WiseCollectionPipeline:
    """Wise Collectionæ•°æ®æ”¶é›†Pipeline"""

    def __init__(self, enable_monitoring: bool = True):
        """åˆå§‹åŒ–pipeline"""
        self.pipeline_start_time = datetime.now()
        self.enable_monitoring = enable_monitoring
        self.stats = {
            "start_time": self.pipeline_start_time.isoformat(),
            "stages_completed": [],
            "stages_failed": [],
            "stage_results": {},
            "total_runtime_seconds": 0,
            "final_summary": {}
        }

        # ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨
        os.makedirs("logs", exist_ok=True)

        # é‡ç½®æ€§èƒ½ç›‘æ§å™¨
        if self.enable_monitoring:
            performance_monitor.reset()

    def _load_config(self, config_path: str = "config/llm.yaml") -> Dict[str, Any]:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            import yaml
            with open(config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
            return config
        except Exception as e:
            logger.warning(f"Failed to load config from {config_path}: {e}")
            # è¿”å›é»˜è®¤é…ç½®
            return {
                'database': {'path': 'data/wise_collection.db'},
                'llm': {
                    'models': {
                        'main': 'gpt-4',
                        'medium': 'gpt-3.5-turbo',
                        'small': 'gpt-3.5-turbo'
                    }
                }
            }

    def run_stage_fetch(self, limit_sources: Optional[int] = None,
                       sources: Optional[List[str]] = None) -> Dict[str, Any]:
        """é˜¶æ®µ1: æ•°æ®æŠ“å–ï¼ˆæ”¯æŒå¤šæ•°æ®æºï¼‰"""
        logger.info("=" * 50)
        logger.info("STAGE 1: Multi-Source Posts Fetcher")
        logger.info("=" * 50)

        if self.enable_monitoring:
            performance_monitor.start_stage("fetch")

        try:
            from pipeline.fetch import MultiSourceFetcher

            # ä½¿ç”¨æŒ‡å®šçš„æ•°æ®æºï¼Œé»˜è®¤ä¸º reddit + hackernews
            fetch_sources = sources or ['reddit', 'hackernews']
            fetcher = MultiSourceFetcher(sources=fetch_sources)
            result = fetcher.fetch_all(limit_sources=limit_sources)

            self.stats["stages_completed"].append("fetch")
            self.stats["stage_results"]["fetch"] = result

            if self.enable_monitoring:
                performance_monitor.end_stage("fetch", result.get('total_saved', 0))

            logger.info(f"âœ… Stage 1 completed: Found {result['total_saved']} posts from {len(result['sources_processed'])} sources")

            # è¾“å‡ºå„æ•°æ®æºç»Ÿè®¡
            for source, stats in result.get("source_stats", {}).items():
                if "error" not in stats:
                    logger.info(f"   - {source}: {stats.get('total_saved', 0)} posts")
                else:
                    logger.error(f"   - {source}: ERROR - {stats['error']}")

            return result

        except Exception as e:
            logger.error(f"âŒ Stage 1 failed: {e}")
            self.stats["stages_failed"].append("fetch")
            if self.enable_monitoring:
                performance_monitor.end_stage("fetch", 0)
            raise

    def run_stage_filter(self, limit_posts: Optional[int] = None, process_all: bool = False) -> Dict[str, Any]:
        """é˜¶æ®µ2: ä¿¡å·è¿‡æ»¤"""
        logger.info("=" * 50)
        logger.info("STAGE 2: Filtering pain signals")
        logger.info("=" * 50)

        if self.enable_monitoring:
            performance_monitor.start_stage("filter")

        try:
            filter = PainSignalFilter()

            # è·å–æœªè¿‡æ»¤çš„å¸–å­
            # å¦‚æœ process_all=True ä¸”æœªæŒ‡å®š limitï¼Œåˆ™å¤„ç†æ‰€æœ‰æ•°æ®
            if process_all and limit_posts is None:
                limit_posts = 1000000  # å¤„ç†æ‰€æœ‰æ•°æ®
            elif limit_posts is None:
                limit_posts = 1000

            unfiltered_posts = db.get_unprocessed_posts(limit=limit_posts)

            # åˆå§‹åŒ–è®¡æ•°å™¨
            saved_count = 0
            failed_count = 0
            failed_posts = []

            if not unfiltered_posts:
                logger.info("No posts to filter")
                result = {"processed": 0, "filtered": 0, "failed": 0}
                if self.enable_monitoring:
                    performance_monitor.end_stage("filter", 0)
            else:
                logger.info(f"Filtering {len(unfiltered_posts)} posts")
                logger.info("Using incremental save mode - each post is saved immediately after processing")

                # æ”¹è¿›ï¼šé€ä¸ªå¤„ç†å¹¶ç«‹å³ä¿å­˜ï¼Œé¿å…æ‰¹é‡å¤±è´¥å¯¼è‡´æ•°æ®ä¸¢å¤±
                for i, post in enumerate(unfiltered_posts):
                    if i % 100 == 0:
                        logger.info(f"Processed {i}/{len(unfiltered_posts)} posts, saved: {saved_count}, failed: {failed_count}")

                    try:
                        # è¿‡æ»¤å•ä¸ªå¸–å­
                        passed, filter_result = filter.filter_post(post)

                        if passed:
                            # ä¸ºå¸–å­æ·»åŠ è¿‡æ»¤ç»“æœ
                            filtered_post = post.copy()
                            filtered_post.update({
                                "pain_score": filter_result["pain_score"],
                                "pain_keywords": filter_result.get("matched_keywords", []),
                                "pain_patterns": filter_result.get("matched_patterns", []),
                                "emotional_intensity": filter_result.get("emotional_intensity", 0.0),
                                "filter_reason": "pain_signal_passed",
                                "aspiration_keywords": filter_result.get("matched_aspirations", []),
                                "aspiration_score": filter_result.get("aspiration_score", 0.0),
                                "pass_type": filter_result.get("pass_type", "pain"),
                                "engagement_score": filter_result.get("engagement_score", 0.0),
                                "trust_level": filter_result.get("trust_level", 0.5)
                            })

                            # ç«‹å³ä¿å­˜åˆ°æ•°æ®åº“
                            if db.insert_filtered_post(filtered_post):
                                saved_count += 1
                            else:
                                logger.warning(f"Failed to save post {post.get('id')}")
                                failed_count += 1
                                failed_posts.append(post.get('id'))
                        # å¦‚æœæœªé€šè¿‡è¿‡æ»¤ï¼Œä¸ä¿å­˜ï¼ˆè¿™æ˜¯æ­£å¸¸çš„ï¼‰

                    except Exception as e:
                        logger.error(f"Error processing post {post.get('id')}: {e}")
                        failed_count += 1
                        failed_posts.append(post.get('id'))
                        # ç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ªå¸–å­ï¼Œä¸ä¸­æ–­æ•´ä¸ªæµç¨‹
                        continue

                result = {
                    "processed": len(unfiltered_posts),
                    "filtered": saved_count,
                    "failed": failed_count,
                    "failed_posts": failed_posts[:10],  # åªè®°å½•å‰10ä¸ªå¤±è´¥çš„
                    "filter_stats": filter.get_statistics()
                }

                if self.enable_monitoring:
                    performance_monitor.end_stage("filter", saved_count)

            self.stats["stages_completed"].append("filter")
            self.stats["stage_results"]["filter"] = result

            logger.info(f"âœ… Stage 2 completed: Processed {result['processed']} posts, filtered {result['filtered']}, failed {result.get('failed', 0)}")
            if failed_count > 0:
                logger.warning(f"âš ï¸  {failed_count} posts failed to process and will be retried next run")

            return result

        except Exception as e:
            logger.error(f"âŒ Stage 2 failed: {e}")
            self.stats["stages_failed"].append("filter")
            if self.enable_monitoring:
                performance_monitor.end_stage("filter", 0)
            raise

    def run_stage_extract(self, limit_posts: Optional[int] = None, process_all: bool = False) -> Dict[str, Any]:
        """é˜¶æ®µ3: ç—›ç‚¹æŠ½å–"""
        logger.info("=" * 50)
        logger.info("STAGE 3: Extracting pain points")
        logger.info("=" * 50)

        if self.enable_monitoring:
            performance_monitor.start_stage("extract")

        try:
            extractor = PainPointExtractor()

            # å¦‚æœ process_all=True ä¸”æœªæŒ‡å®š limitï¼Œåˆ™å¤„ç†æ‰€æœ‰æ•°æ®
            if process_all and limit_posts is None:
                limit_posts = 1000000  # å¤„ç†æ‰€æœ‰æ•°æ®
            elif limit_posts is None:
                limit_posts = 100

            result = extractor.process_unextracted_posts(limit=limit_posts)

            self.stats["stages_completed"].append("extract")
            self.stats["stage_results"]["extract"] = result

            if self.enable_monitoring:
                performance_monitor.end_stage("extract", result.get('pain_events_saved', 0))

            logger.info(f"âœ… Stage 3 completed: Extracted {result.get('pain_events_saved', 0)} pain events")
            return result

        except Exception as e:
            logger.error(f"âŒ Stage 3 failed: {e}")
            self.stats["stages_failed"].append("extract")
            if self.enable_monitoring:
                performance_monitor.end_stage("extract", 0)
            raise

    def run_stage_embed(self, limit_events: Optional[int] = None, process_all: bool = False) -> Dict[str, Any]:
        """é˜¶æ®µ4: å‘é‡åŒ–"""
        logger.info("=" * 50)
        logger.info("STAGE 4: Creating embeddings")
        logger.info("=" * 50)

        if self.enable_monitoring:
            performance_monitor.start_stage("embed")

        try:
            embedder = PainEventEmbedder()

            # å¦‚æœ process_all=True ä¸”æœªæŒ‡å®š limitï¼Œåˆ™å¤„ç†æ‰€æœ‰æ•°æ®
            if process_all and limit_events is None:
                limit_events = 1000000  # å¤„ç†æ‰€æœ‰æ•°æ®
            elif limit_events is None:
                limit_events = 200

            result = embedder.process_missing_embeddings(limit=limit_events)

            self.stats["stages_completed"].append("embed")
            self.stats["stage_results"]["embed"] = result

            if self.enable_monitoring:
                performance_monitor.end_stage("embed", result.get('embeddings_created', 0))

            logger.info(f"âœ… Stage 4 completed: Created {result['embeddings_created']} embeddings")
            return result

        except Exception as e:
            logger.error(f"âŒ Stage 4 failed: {e}")
            self.stats["stages_failed"].append("embed")
            if self.enable_monitoring:
                performance_monitor.end_stage("embed", 0)
            raise

    def run_stage_cluster(self, limit_events: Optional[int] = None, process_all: bool = False) -> Dict[str, Any]:
        """é˜¶æ®µ5: èšç±»"""
        logger.info("=" * 50)
        logger.info("STAGE 5: Clustering pain events")
        logger.info("=" * 50)

        if self.enable_monitoring:
            performance_monitor.start_stage("cluster")

        try:
            clusterer = PainEventClusterer()

            # å¦‚æœ process_all=True ä¸”æœªæŒ‡å®š limitï¼Œåˆ™å¤„ç†æ‰€æœ‰æ•°æ®ï¼ˆè®¾ç½®ä¸ºå¤§æ•°å€¼ï¼‰
            if process_all and limit_events is None:
                limit_events = 1000000  # å¤„ç†æ‰€æœ‰æ•°æ®
            elif limit_events is None:
                limit_events = 200

            result = clusterer.cluster_pain_events(limit=limit_events)

            self.stats["stages_completed"].append("cluster")
            self.stats["stage_results"]["cluster"] = result

            if self.enable_monitoring:
                performance_monitor.end_stage("cluster", result.get('clusters_created', 0))

            logger.info(f"âœ… Stage 5 completed: Created {result.get('clusters_created', 0)} clusters")
            return result

        except Exception as e:
            logger.error(f"âŒ Stage 5 failed: {e}")
            self.stats["stages_failed"].append("cluster")
            if self.enable_monitoring:
                performance_monitor.end_stage("cluster", 0)
            raise

    def run_stage_cross_source_alignment(self) -> Dict[str, Any]:
        """é˜¶æ®µ5.5: è·¨æºå¯¹é½"""
        logger.info("=" * 50)
        logger.info("STAGE 5.5: Cross-Source Alignment")
        logger.info("=" * 50)

        if self.enable_monitoring:
            performance_monitor.start_stage("alignment")

        try:
            # åˆå§‹åŒ–å¯¹é½å™¨
            llm_client = LLMClient()  # Uses default config path
            aligner = CrossSourceAligner(db, llm_client)

            # æ‰§è¡Œè·¨æºå¯¹é½
            logger.info("Processing cross-source alignment...")
            aligner.process_alignments()

            # è·å–å¯¹é½ç»“æœ
            aligned_problems = db.get_aligned_problems()

            result = {
                "aligned_problems_count": len(aligned_problems),
                "aligned_problems": aligned_problems
            }

            self.stats["stages_completed"].append("alignment")
            self.stats["stage_results"]["alignment"] = result

            if self.enable_monitoring:
                performance_monitor.end_stage("alignment", len(aligned_problems))

            logger.info(f"âœ… Stage 5.5 completed: Found {len(aligned_problems)} aligned problems")

            # æ˜¾ç¤ºå¯¹é½æ‘˜è¦
            if aligned_problems:
                logger.info("\nAlignment Summary:")
                logger.info(f"- Total aligned problems: {len(aligned_problems)}")
                for problem in aligned_problems[:3]:  # æ˜¾ç¤ºå‰3ä¸ª
                    logger.info(f"  {problem['aligned_problem_id']}: {problem['core_problem'][:100]}...")
                    logger.info(f"  Sources: {', '.join(problem['sources'])}")
            else:
                logger.info("No cross-source alignments found in this run")

            return result

        except Exception as e:
            logger.error(f"âŒ Stage 5.5 failed: {e}")
            self.stats["stages_failed"].append("alignment")
            if self.enable_monitoring:
                performance_monitor.end_stage("alignment", 0)
            raise

    def run_stage_map_opportunities(self, limit_clusters: Optional[int] = None, process_all: bool = False) -> Dict[str, Any]:
        """é˜¶æ®µ6: æœºä¼šæ˜ å°„"""
        logger.info("=" * 50)
        logger.info("STAGE 6: Mapping opportunities")
        logger.info("=" * 50)

        if self.enable_monitoring:
            performance_monitor.start_stage("map_opportunities")

        try:
            mapper = OpportunityMapper()

            # å¦‚æœ process_all=True ä¸”æœªæŒ‡å®š limitï¼Œåˆ™å¤„ç†æ‰€æœ‰æ•°æ®
            if process_all and limit_clusters is None:
                limit_clusters = 1000000  # å¤„ç†æ‰€æœ‰æ•°æ®
            elif limit_clusters is None:
                limit_clusters = 50

            result = mapper.map_opportunities_for_clusters(limit=limit_clusters)

            self.stats["stages_completed"].append("map_opportunities")
            self.stats["stage_results"]["map_opportunities"] = result

            if self.enable_monitoring:
                performance_monitor.end_stage("map_opportunities", result.get('opportunities_created', 0))

            logger.info(f"âœ… Stage 6 completed: Mapped {result['opportunities_created']} opportunities")
            return result

        except Exception as e:
            logger.error(f"âŒ Stage 6 failed: {e}")
            self.stats["stages_failed"].append("map_opportunities")
            if self.enable_monitoring:
                performance_monitor.end_stage("map_opportunities", 0)
            raise

    def run_stage_score(self, limit_opportunities: Optional[int] = None, process_all: bool = False) -> Dict[str, Any]:
        """é˜¶æ®µ7: å¯è¡Œæ€§è¯„åˆ†"""
        logger.info("=" * 50)
        logger.info("STAGE 7: Scoring viability")
        logger.info("=" * 50)

        if self.enable_monitoring:
            performance_monitor.start_stage("score")

        try:
            scorer = ViabilityScorer()

            # å¦‚æœ process_all=True ä¸”æœªæŒ‡å®š limitï¼Œåˆ™å¤„ç†æ‰€æœ‰æ•°æ®
            if process_all and limit_opportunities is None:
                limit_opportunities = 1000000  # å¤„ç†æ‰€æœ‰æ•°æ®
            elif limit_opportunities is None:
                limit_opportunities = 100

            result = scorer.score_opportunities(limit=limit_opportunities)

            self.stats["stages_completed"].append("score")
            self.stats["stage_results"]["score"] = result

            if self.enable_monitoring:
                performance_monitor.end_stage("score", result.get('opportunities_scored', 0))

            logger.info(f"âœ… Stage 7 completed: Scored {result['opportunities_scored']} opportunities")
            return result

        except Exception as e:
            logger.error(f"âŒ Stage 7 failed: {e}")
            self.stats["stages_failed"].append("score")
            if self.enable_monitoring:
                performance_monitor.end_stage("score", 0)
            raise

    def run_stage_decision_shortlist(self) -> Dict[str, Any]:
        """é˜¶æ®µ8: å†³ç­–æ¸…å•ç”Ÿæˆ"""
        logger.info("=" * 50)
        logger.info("STAGE 8: Decision Shortlist Generation")
        logger.info("=" * 50)

        if self.enable_monitoring:
            performance_monitor.start_stage("decision_shortlist")

        try:
            generator = DecisionShortlistGenerator()
            result = generator.generate_shortlist()

            self.stats["stages_completed"].append("decision_shortlist")
            self.stats["stage_results"]["decision_shortlist"] = result

            if self.enable_monitoring:
                performance_monitor.end_stage("decision_shortlist", result.get('shortlist_count', 0))

            logger.info(f"âœ… Stage 8 completed: Generated {result['shortlist_count']} candidates")
            if result.get('markdown_report'):
                logger.info(f"ğŸ“ Markdown report: {result['markdown_report']}")
            if result.get('json_report'):
                logger.info(f"ğŸ“Š JSON report: {result['json_report']}")
            return result

        except Exception as e:
            logger.error(f"âŒ Stage 8 failed: {e}")
            self.stats["stages_failed"].append("decision_shortlist")
            if self.enable_monitoring:
                performance_monitor.end_stage("decision_shortlist", 0)
            raise

    def generate_final_report(
        self,
        save_metrics: bool = False,
        metrics_file: Optional[str] = None,
        generate_report: bool = False,
        report_file: Optional[str] = None
    ) -> Dict[str, Any]:
        """ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š"""
        logger.info("=" * 50)
        logger.info("GENERATING FINAL REPORT")
        logger.info("=" * 50)

        try:
            # è¾“å‡ºæ€§èƒ½ç›‘æ§æ‘˜è¦
            if self.enable_monitoring:
                monitor_summary = performance_monitor.get_summary()
                logger.info(f"\nğŸ“Š Performance Summary:")
                logger.info(f"   â€¢ Total Duration: {monitor_summary['total_duration_minutes']} minutes")
                logger.info(f"   â€¢ LLM Calls: {monitor_summary['total_llm_calls']:,}")
                logger.info(f"   â€¢ Total Tokens: {monitor_summary['total_tokens']:,}")
                logger.info(f"   â€¢ Est. Cost: ${monitor_summary['estimated_cost_usd']:.4f} USD")

                # ä¿å­˜metricsåˆ°æ–‡ä»¶
                if save_metrics:
                    if metrics_file is None:
                        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                        metrics_file = f"docs/reports/pipeline_metrics_{timestamp}.json"

                    os.makedirs(os.path.dirname(metrics_file), exist_ok=True)
                    performance_monitor.save_metrics(metrics_file)
                    logger.info(f"ğŸ’¾ Metrics saved to: {metrics_file}")

                    # å¦‚æœéœ€è¦ç”ŸæˆmarkdownæŠ¥å‘Š
                    if generate_report:
                        self.generate_markdown_report(metrics_file, report_file)

            # è·å–æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯
            db_stats = db.get_statistics()

            # è·å–æœ€é«˜åˆ†çš„æœºä¼š
            top_opportunities = []
            try:
                with db.get_connection("clusters") as conn:
                    cursor = conn.execute("""
                        SELECT o.opportunity_name, o.total_score, o.recommendation, c.cluster_name
                        FROM opportunities o
                        JOIN clusters c ON o.cluster_id = c.id
                        WHERE o.total_score > 0
                        ORDER BY o.total_score DESC
                        LIMIT 10
                    """)
                    top_opportunities = [dict(row) for row in cursor.fetchall()]
            except Exception as e:
                logger.warning(f"Failed to get top opportunities: {e}")

            # è®¡ç®—è¿è¡Œæ—¶é—´
            end_time = datetime.now()
            total_runtime = (end_time - self.pipeline_start_time).total_seconds()
            self.stats["total_runtime_seconds"] = total_runtime

            # ç”Ÿæˆæœ€ç»ˆæ‘˜è¦
            final_summary = {
                "pipeline_completed": len(self.stats["stages_failed"]) == 0,
                "stages_completed": len(self.stats["stages_completed"]),
                "stages_failed": len(self.stats["stages_failed"]),
                "total_runtime_minutes": round(total_runtime / 60, 2),
                "database_statistics": db_stats,
                "top_opportunities": top_opportunities,
                "pipeline_efficiency": {
                    "posts_per_minute": self.stats["stage_results"].get("fetch", {}).get("total_saved", 0) / max(total_runtime / 60, 1),
                    "pain_events_per_hour": self.stats["stage_results"].get("extract", {}).get("pain_events_saved", 0) / max(total_runtime / 3600, 1),
                    "opportunities_per_hour": self.stats["stage_results"].get("map_opportunities", {}).get("opportunities_created", 0) / max(total_runtime / 3600, 1)
                }
            }

            # æ·»åŠ æ€§èƒ½ç›‘æ§æ•°æ®åˆ°æ‘˜è¦
            if self.enable_monitoring:
                final_summary["performance"] = monitor_summary

            self.stats["final_summary"] = final_summary

            logger.info("ğŸ‰ PIPELINE COMPLETED SUCCESSFULLY!")
            logger.info(f"ğŸ“Š Final Summary:")
            logger.info(f"   â€¢ Runtime: {final_summary['total_runtime_minutes']} minutes")
            logger.info(f"   â€¢ Stages completed: {final_summary['stages_completed']}/7")
            logger.info(f"   â€¢ Raw posts collected: {db_stats.get('raw_posts_count', 0)}")
            logger.info(f"   â€¢ Pain events extracted: {db_stats.get('pain_events_count', 0)}")
            logger.info(f"   â€¢ Clusters created: {db_stats.get('clusters_count', 0)}")
            logger.info(f"   â€¢ Opportunities identified: {db_stats.get('opportunities_count', 0)}")

            if top_opportunities:
                logger.info(f"   â€¢ Top opportunity: {top_opportunities[0]['opportunity_name']} (Score: {top_opportunities[0]['total_score']:.1f})")

            return final_summary

        except Exception as e:
            logger.error(f"Failed to generate final report: {e}")
            return {"error": str(e)}

    def run_full_pipeline(
        self,
        limit_sources: Optional[int] = None,
        limit_posts: Optional[int] = None,
        limit_events: Optional[int] = None,
        limit_clusters: Optional[int] = None,
        limit_opportunities: Optional[int] = None,
        sources: Optional[List[str]] = None,
        process_all: bool = False,
        stop_on_error: bool = False,
        save_metrics: bool = False,
        metrics_file: Optional[str] = None,
        generate_report: bool = False,
        report_file: Optional[str] = None
    ) -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´pipeline"""
        logger.info("ğŸš€ Starting Wise Collection Multi-Source Pipeline")
        logger.info(f"â° Started at: {self.pipeline_start_time}")

        if self.enable_monitoring:
            logger.info("ğŸ“Š Performance monitoring: ENABLED")
        else:
            logger.info("ğŸ“Š Performance monitoring: DISABLED")

        # ä½¿ç”¨æŒ‡å®šçš„æ•°æ®æºï¼Œé»˜è®¤ä¸º reddit + hackernews
        fetch_sources = sources or ['reddit', 'hackernews']
        logger.info(f"ğŸ“¡ Data sources: {', '.join(fetch_sources)}")

        # æ˜¾ç¤ºå¤„ç†æ¨¡å¼
        if process_all:
            logger.info("ğŸ”„ Processing mode: PROCESS ALL (no limits)")
        else:
            logger.info("ğŸ“Š Processing mode: Default limits")

        stages = [
            ("fetch", lambda: self.run_stage_fetch(limit_sources, fetch_sources)),
            ("filter", lambda: self.run_stage_filter(limit_posts, process_all)),
            ("extract", lambda: self.run_stage_extract(limit_posts, process_all)),
            ("embed", lambda: self.run_stage_embed(limit_events, process_all)),
            ("cluster", lambda: self.run_stage_cluster(limit_events, process_all)),
            ("alignment", lambda: self.run_stage_cross_source_alignment()),
            ("map_opportunities", lambda: self.run_stage_map_opportunities(limit_clusters, process_all)),
            ("score", lambda: self.run_stage_score(limit_opportunities, process_all)),
            ("shortlist", lambda: self.run_stage_decision_shortlist())
        ]

        for stage_name, stage_func in stages:
            try:
                stage_func()
            except Exception as e:
                logger.error(f"Stage '{stage_name}' failed: {e}")
                if stop_on_error:
                    logger.error("Stopping pipeline due to error")
                    break
                else:
                    logger.warning(f"Continuing pipeline despite '{stage_name}' failure")

        # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
        final_report = self.generate_final_report(
            save_metrics=save_metrics,
            metrics_file=metrics_file,
            generate_report=generate_report,
            report_file=report_file
        )

        return final_report

    def run_single_stage(self, stage_name: str, process_all: bool = False, **kwargs) -> Dict[str, Any]:
        """è¿è¡Œå•ä¸ªé˜¶æ®µ"""
        stage_map = {
            "fetch": lambda: self.run_stage_fetch(kwargs.get("limit_sources"), kwargs.get("sources")),
            "filter": lambda: self.run_stage_filter(kwargs.get("limit_posts"), process_all),
            "extract": lambda: self.run_stage_extract(kwargs.get("limit_posts"), process_all),
            "embed": lambda: self.run_stage_embed(kwargs.get("limit_events"), process_all),
            "cluster": lambda: self.run_stage_cluster(kwargs.get("limit_events"), process_all),
            "alignment": lambda: self.run_stage_cross_source_alignment(),
            "map": lambda: self.run_stage_map_opportunities(kwargs.get("limit_clusters"), process_all),
            "score": lambda: self.run_stage_score(kwargs.get("limit_opportunities"), process_all),
            "shortlist": lambda: self.run_stage_decision_shortlist()
        }

        if stage_name not in stage_map:
            raise ValueError(f"Unknown stage: {stage_name}")

        return stage_map[stage_name]()

    def save_results(self, filename: Optional[str] = None):
        """ä¿å­˜pipelineç»“æœ"""
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"pipeline_results_{timestamp}.json"

        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(self.stats, f, indent=2, default=str)

            logger.info(f"ğŸ“ Results saved to: {filename}")
            return filename

        except Exception as e:
            logger.error(f"Failed to save results: {e}")
            return None

    def generate_markdown_report(self, metrics_file: str, output_file: Optional[str] = None) -> str:
        """ç”Ÿæˆè¯¦ç»†çš„æˆæœ¬æ€§èƒ½markdownæŠ¥å‘Š"""
        if not self.enable_monitoring:
            logger.warning("Performance monitoring is disabled, cannot generate report")
            return None

        try:
            from utils.performance_monitor import PerformanceMonitor

            # åŠ è½½metrics
            monitor = PerformanceMonitor.load_metrics(metrics_file)
            summary = monitor.get_summary()
            stages_summary = summary['stages_summary']

            # é˜¶æ®µä¸­æ–‡åç§°æ˜ å°„
            stage_names_cn = {
                'fetch': 'æ•°æ®æŠ“å–',
                'filter': 'ä¿¡å·è¿‡æ»¤',
                'extract': 'ç—›ç‚¹æŠ½å–',
                'embed': 'å‘é‡åŒ–',
                'cluster': 'èšç±»åˆ†æ',
                'alignment': 'è·¨æºå¯¹é½',
                'map_opportunities': 'æœºä¼šæ˜ å°„',
                'score': 'å¯è¡Œæ€§è¯„åˆ†'
            }

            # ç”ŸæˆæŠ¥å‘Š
            timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            metrics_filename = os.path.basename(metrics_file)

            report = f"""# Pipeline æˆæœ¬ä¸æ€§èƒ½åˆ†ææŠ¥å‘Š

> **ç”Ÿæˆæ—¶é—´**: {timestamp}
> **æŒ‡æ ‡æ–‡ä»¶**: {metrics_filename}

---

## ğŸ“Š æ‰§è¡Œæ‘˜è¦

### å…³é”®æŒ‡æ ‡

| æŒ‡æ ‡ | æ•°å€¼ |
|------|------|
| æ€»æ‰§è¡Œæ—¶é—´ | {summary['total_duration_minutes']:.2f} åˆ†é’Ÿ ({summary['total_duration_seconds']:.1f} ç§’) |
| LLMè°ƒç”¨æ¬¡æ•° | {summary['total_llm_calls']:,} æ¬¡ |
| æ€»Tokenä½¿ç”¨é‡ | {summary['total_tokens']:,} |
| é¢„ä¼°æˆæœ¬ | ${summary['estimated_cost_usd']:.4f} USD |
| å¹³å‡æ¯è°ƒç”¨æˆæœ¬ | ${(summary['estimated_cost_usd']/summary['total_llm_calls'] if summary['total_llm_calls'] > 0 else 0):.6f} USD |

### æ•ˆç‡æŒ‡æ ‡

| æŒ‡æ ‡ | æ•°å€¼ |
|------|------|
| å¹³å‡æ¯è°ƒç”¨è€—æ—¶ | {(summary['total_duration_seconds']/summary['total_llm_calls'] if summary['total_llm_calls'] > 0 else 0):.2f} ç§’ |
| å¹³å‡æ¯è°ƒç”¨Tokenæ•° | {(summary['total_tokens']/summary['total_llm_calls'] if summary['total_llm_calls'] > 0 else 0):.0f} |
| æ¯åƒTokenæˆæœ¬ | ${(summary['estimated_cost_usd']/(summary['total_tokens']/1000) if summary['total_tokens'] > 0 else 0):.4f} USD |

---

## ğŸ“ˆ é˜¶æ®µåˆ†è§£

### å„é˜¶æ®µæ€§èƒ½è¯¦æƒ…

| é˜¶æ®µ | æ‰§è¡Œæ—¶é—´(ç§’) | å¤„ç†é¡¹ç›®æ•° | LLMè°ƒç”¨æ¬¡æ•° | Tokenä½¿ç”¨é‡ |
|------|-------------|-----------|------------|-----------|
"""

            # æ·»åŠ å„é˜¶æ®µæ•°æ®
            for stage_name, stats in stages_summary.items():
                stage_name_cn = stage_names_cn.get(stage_name, stage_name)
                duration = stats['duration_seconds']
                items = stats['items_processed']
                tokens = stats['tokens_used']
                llm_calls = stats['llm_calls']

                report += f"| {stage_name_cn} | {duration:.1f} | {items} | {llm_calls} | {tokens:,} |\n"

            # é˜¶æ®µæ•ˆç‡å¯¹æ¯”
            report += "\n### é˜¶æ®µæ•ˆç‡å¯¹æ¯”\n\n"
            for stage_name, stats in stages_summary.items():
                stage_name_cn = stage_names_cn.get(stage_name, stage_name)
                duration = stats['duration_seconds']
                items = stats['items_processed']
                tokens = stats['tokens_used']

                if items > 0:
                    avg_time_per_item = duration / items
                    report += f"**{stage_name_cn}**:\n"
                    report += f"- å¹³å‡æ¯é¡¹ç›®å¤„ç†æ—¶é—´: {avg_time_per_item:.2f} ç§’\n"

                    if tokens > 0:
                        avg_tokens_per_item = tokens / items
                        report += f"- å¹³å‡æ¯é¡¹ç›®Tokenæ•°: {avg_tokens_per_item:.0f}\n"
                    report += "\n"

            # æˆæœ¬åˆ†æ
            total_tokens = summary['total_tokens']
            report += """
---

## ğŸ’° æˆæœ¬åˆ†æ

### Tokenåˆ†å¸ƒ

| é˜¶æ®µ | Tokenä½¿ç”¨é‡ | å æ¯” |
|------|-----------|------|
"""

            for stage_name, stats in stages_summary.items():
                stage_name_cn = stage_names_cn.get(stage_name, stage_name)
                tokens = stats['tokens_used']
                percentage = (tokens / total_tokens * 100) if total_tokens > 0 else 0
                report += f"| {stage_name_cn} | {tokens:,} | {percentage:.1f}% |\n"

            # æˆæœ¬æ„æˆåˆ†æ
            report += "\n### æˆæœ¬æ„æˆåˆ†æ\n\n"
            for stage_name, stats in stages_summary.items():
                stage_name_cn = stage_names_cn.get(stage_name, stage_name)
                tokens = stats['tokens_used']
                percentage = (tokens / total_tokens * 100) if total_tokens > 0 else 0
                stage_cost = summary['estimated_cost_usd'] * (percentage / 100)
                report += f"**{stage_name_cn}**: ${stage_cost:.4f} USD ({percentage:.1f}%)\n"

            report += f"\n**æ€»æˆæœ¬**: ${summary['estimated_cost_usd']:.4f} USD\n"

            # æ€§èƒ½æŒ‡æ ‡
            report += """
---

## âš¡ æ€§èƒ½æŒ‡æ ‡

### ååé‡åˆ†æ

| é˜¶æ®µ | ååé‡ (é¡¹ç›®/åˆ†é’Ÿ) | ååé‡ (Token/ç§’) |
|------|------------------|------------------|
"""

            for stage_name, stats in stages_summary.items():
                stage_name_cn = stage_names_cn.get(stage_name, stage_name)
                duration_minutes = stats['duration_seconds'] / 60
                items = stats['items_processed']
                tokens = stats['tokens_used']

                items_per_minute = (items / duration_minutes) if duration_minutes > 0 else 0
                tokens_per_second = (tokens / stats['duration_seconds']) if stats['duration_seconds'] > 0 else 0

                report += f"| {stage_name_cn} | {items_per_minute:.2f} | {tokens_per_second:.0f} |\n"

            # æ€§èƒ½ç“¶é¢ˆè¯†åˆ«
            report += "\n### æ€§èƒ½ç“¶é¢ˆè¯†åˆ«\n\n"

            # è¯†åˆ«æœ€æ…¢çš„é˜¶æ®µ
            slowest_stage = max(stages_summary.items(), key=lambda x: x[1]['duration_seconds'])
            slowest_stage_cn = stage_names_cn.get(slowest_stage[0], slowest_stage[0])
            report += f"- **æœ€æ…¢é˜¶æ®µ**: {slowest_stage_cn} ({slowest_stage[1]['duration_seconds']:.1f}ç§’)\n"

            # è¯†åˆ«Tokenæ¶ˆè€—æœ€å¤§çš„é˜¶æ®µ
            highest_token_stage = max(stages_summary.items(), key=lambda x: x[1]['tokens_used'])
            highest_token_cn = stage_names_cn.get(highest_token_stage[0], highest_token_stage[0])
            report += f"- **æœ€é«˜Tokenæ¶ˆè€—**: {highest_token_cn} ({highest_token_stage[1]['tokens_used']:,} tokens)\n"

            # ç»“è®ºä¸å»ºè®®
            report += """
---

## ğŸ’¡ ç»“è®ºä¸å»ºè®®

### å…³é”®å‘ç°

1. **è‡ªåŠ¨åŒ–æµç¨‹**: Pipelineå·²å®ç°ç«¯åˆ°ç«¯è‡ªåŠ¨åŒ–ï¼Œä»æ•°æ®æŠ“å–åˆ°æœºä¼šè¯„åˆ†æ— éœ€äººå·¥å¹²é¢„
2. **æˆæœ¬å¯æ§**: æ•´ä¸ªæµç¨‹æˆæœ¬åœ¨å¯æ¥å—èŒƒå›´å†…ï¼Œé€‚åˆå®šæœŸæ‰§è¡Œ
3. **è¾“å‡ºå®Œæ•´**: åŒ…å«å¤šç»´åº¦åˆ†æå’Œæ€§èƒ½è¿½è¸ª

### ä¼˜åŒ–å»ºè®®

#### çŸ­æœŸä¼˜åŒ– (1-2å‘¨)

1. **æ‰¹é‡å¤„ç†**: å¯¹æ›´å¤§çš„æ•°æ®é›†è¿›è¡Œæ‰¹é‡å¤„ç†ï¼Œé™ä½å•ä½æˆæœ¬
2. **ç¼“å­˜ä¼˜åŒ–**: å¯¹ç›¸ä¼¼å†…å®¹è¿›è¡Œç¼“å­˜ï¼Œå‡å°‘é‡å¤LLMè°ƒç”¨
3. **å¹¶è¡Œå¤„ç†**: åœ¨ç‹¬ç«‹é˜¶æ®µå¹¶è¡Œå¤„ç†ï¼Œç¼©çŸ­æ€»æ‰§è¡Œæ—¶é—´

#### ä¸­æœŸä¼˜åŒ– (1ä¸ªæœˆ)

1. **å®šæœŸæ‰§è¡Œ**: è®¾ç½®å®šæ—¶ä»»åŠ¡ï¼Œæ¯å‘¨/æ¯æœˆè‡ªåŠ¨è¿è¡Œ
2. **æ•°æ®ç§¯ç´¯**: æŒç»­ç§¯ç´¯æ•°æ®ï¼Œå½¢æˆè¶‹åŠ¿åˆ†æ
3. **åé¦ˆé—­ç¯**: å»ºç«‹åé¦ˆæœºåˆ¶ï¼ŒæŒç»­æ”¹è¿›åˆ†æè´¨é‡

#### é•¿æœŸä¼˜åŒ– (æŒç»­)

1. **æ™ºèƒ½è°ƒåº¦**: æ ¹æ®æ•°æ®å˜åŒ–è‡ªåŠ¨è§¦å‘åˆ†æ
2. **A/Bæµ‹è¯•**: å¯¹ä¸åŒpromptå’Œå‚æ•°è¿›è¡ŒA/Bæµ‹è¯•
3. **æˆæœ¬ä¼˜åŒ–**: æ¢ç´¢æ›´ç»æµçš„æ¨¡å‹ç»„åˆ

---

*æœ¬æŠ¥å‘Šç”± Pipeline è‡ªåŠ¨ç”Ÿæˆ*
"""

            # ä¿å­˜æŠ¥å‘Š
            if output_file is None:
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                output_file = f"docs/reports/pipeline_cost_performance_{timestamp}.md"

            os.makedirs(os.path.dirname(output_file), exist_ok=True)

            with open(output_file, 'w', encoding='utf-8') as f:
                f.write(report)

            logger.info(f"ğŸ“ Markdown report saved to: {output_file}")
            return output_file

        except Exception as e:
            logger.error(f"Failed to generate markdown report: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return None

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="Wise Collection Multi-Source Pipeline")

    # è¿è¡Œæ¨¡å¼
    parser.add_argument("--stage", choices=["fetch", "filter", "extract", "embed", "cluster", "alignment", "map", "score", "shortlist", "all"],
                       default="all", help="Which stage to run (default: all)")

    # æ•°æ®æºé€‰æ‹©
    parser.add_argument("--sources", nargs="+", choices=["reddit", "hackernews"],
                       default=["reddit", "hackernews"], help="Data sources to fetch (default: reddit hackernews)")

    # é™åˆ¶å‚æ•°
    parser.add_argument("--limit-sources", type=int, help="Limit number of sources to fetch")
    parser.add_argument("--limit-posts", type=int, help="Limit number of posts to process")
    parser.add_argument("--limit-events", type=int, help="Limit number of pain events to process")
    parser.add_argument("--limit-clusters", type=int, help="Limit number of clusters to process")
    parser.add_argument("--limit-opportunities", type=int, help="Limit number of opportunities to score")

    # å…¨é‡å¤„ç†é€‰é¡¹
    parser.add_argument("--process-all", action="store_true",
                       help="Process ALL unprocessed data (ignore default limits)")

    # æ€§èƒ½ç›‘æ§é€‰é¡¹
    parser.add_argument("--no-monitoring", action="store_true", help="Disable performance monitoring")
    parser.add_argument("--save-metrics", action="store_true", help="Save performance metrics to file")
    parser.add_argument("--metrics-file", help="Custom metrics file path")
    parser.add_argument("--generate-report", action="store_true", help="Generate detailed markdown cost/performance report")
    parser.add_argument("--report-file", help="Custom markdown report file path")

    # å…¶ä»–é€‰é¡¹
    parser.add_argument("--stop-on-error", action="store_true", help="Stop pipeline on first error")
    parser.add_argument("--save-results", action="store_true", help="Save results to file")
    parser.add_argument("--results-file", help="Custom results filename")

    args = parser.parse_args()

    try:
        # åˆå§‹åŒ–pipeline
        pipeline = WiseCollectionPipeline(enable_monitoring=not args.no_monitoring)

        if args.stage == "all":
            # è¿è¡Œå®Œæ•´pipeline
            result = pipeline.run_full_pipeline(
                limit_sources=args.limit_sources,
                limit_posts=args.limit_posts,
                limit_events=args.limit_events,
                limit_clusters=args.limit_clusters,
                limit_opportunities=args.limit_opportunities,
                sources=args.sources,
                process_all=args.process_all,
                stop_on_error=args.stop_on_error,
                save_metrics=args.save_metrics,
                metrics_file=args.metrics_file,
                generate_report=args.generate_report,
                report_file=args.report_file
            )
        else:
            # è¿è¡Œå•ä¸ªé˜¶æ®µ
            stage_kwargs = {
                "limit_sources": args.limit_sources,
                "limit_posts": args.limit_posts,
                "limit_events": args.limit_events,
                "limit_clusters": args.limit_clusters,
                "limit_opportunities": args.limit_opportunities,
                "sources": args.sources,
                "process_all": args.process_all
            }

            # åªä¼ é€’ç›¸å…³çš„å‚æ•°
            relevant_kwargs = {k: v for k, v in stage_kwargs.items() if v is not None}
            result = pipeline.run_single_stage(args.stage, **relevant_kwargs)

        # ä¿å­˜ç»“æœ
        if args.save_results:
            pipeline.save_results(args.results_file)

        # è¾“å‡ºç»“æœ
        print("\n" + "=" * 60)
        print("PIPELINE RESULTS")
        print("=" * 60)
        print(json.dumps(result, indent=2, default=str))

    except KeyboardInterrupt:
        logger.info("Pipeline interrupted by user")
        sys.exit(1)
    except Exception as e:
        logger.error(f"Pipeline failed: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()