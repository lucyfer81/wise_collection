# Reddit Pain Finder - ä»£ç æ±‡æ€»

ç”Ÿæˆæ—¶é—´: 2025-12-15 20:43:12

æœ¬æ–‡æ¡£åŒ…å« reddit_pain_finder é¡¹ç›®çš„æ ¸å¿ƒä»£ç æ–‡ä»¶ï¼š
- Pipelineå¤„ç†æ¨¡å— (pipeline/)
- å·¥å…·æ¨¡å— (utils/)
- ä¸»è¦æ‰§è¡Œè„šæœ¬



================================================================================
æ–‡ä»¶: run_pipeline.py
================================================================================

```python
#!/usr/bin/env python3
"""
Reddit Pain Point Finder - Main Pipeline Runner
ä¸»è¦çš„pipelineæ‰§è¡Œè„šæœ¬ - ä¸€é”®è¿è¡Œæ•´ä¸ªç—›ç‚¹å‘ç°æµç¨‹
"""
import os
import sys
import argparse
import logging
import json
import time
from datetime import datetime
from typing import Dict, Any, Optional

# è®¾ç½®é¡¹ç›®æ ¹ç›®å½•
project_root = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, project_root)

# å¯¼å…¥pipelineæ¨¡å—
from pipeline.fetch import RedditPainFetcher
from pipeline.filter_signal import PainSignalFilter
from pipeline.extract_pain import PainPointExtractor
from pipeline.embed import PainEventEmbedder
from pipeline.cluster import PainEventClusterer
from pipeline.score_viability import ViabilityScorer
from pipeline.map_opportunity import OpportunityMapper

# å¯¼å…¥å·¥å…·æ¨¡å—
from utils.db import db

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/pipeline.log'),
        logging.StreamHandler(sys.stdout)
    ]
)

logger = logging.getLogger(__name__)

class RedditPainPipeline:
    """Redditç—›ç‚¹å‘ç°Pipeline"""

    def __init__(self):
        """åˆå§‹åŒ–pipeline"""
        self.pipeline_start_time = datetime.now()
        self.stats = {
            "start_time": self.pipeline_start_time.isoformat(),
            "stages_completed": [],
            "stages_failed": [],
            "stage_results": {},
            "total_runtime_seconds": 0,
            "final_summary": {}
        }

        # ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨
        os.makedirs("logs", exist_ok=True)

    def run_stage_fetch(self, limit_subreddits: Optional[int] = None) -> Dict[str, Any]:
        """é˜¶æ®µ1: æ•°æ®æŠ“å–"""
        logger.info("=" * 50)
        logger.info("STAGE 1: Fetching Reddit posts")
        logger.info("=" * 50)

        try:
            fetcher = RedditPainFetcher()
            result = fetcher.fetch_all(limit_subreddits=limit_subreddits)

            self.stats["stages_completed"].append("fetch")
            self.stats["stage_results"]["fetch"] = result

            logger.info(f"âœ… Stage 1 completed: Found {result['total_saved']} posts")
            return result

        except Exception as e:
            logger.error(f"âŒ Stage 1 failed: {e}")
            self.stats["stages_failed"].append("fetch")
            raise

    def run_stage_filter(self, limit_posts: Optional[int] = None) -> Dict[str, Any]:
        """é˜¶æ®µ2: ä¿¡å·è¿‡æ»¤"""
        logger.info("=" * 50)
        logger.info("STAGE 2: Filtering pain signals")
        logger.info("=" * 50)

        try:
            filter = PainSignalFilter()

            # è·å–æœªè¿‡æ»¤çš„å¸–å­
            unfiltered_posts = db.get_unprocessed_posts(limit=limit_posts or 1000)

            if not unfiltered_posts:
                logger.info("No posts to filter")
                result = {"processed": 0, "filtered": 0}
            else:
                logger.info(f"Filtering {len(unfiltered_posts)} posts")
                filtered_posts = filter.filter_posts_batch(unfiltered_posts)

                # ä¿å­˜è¿‡æ»¤ç»“æœ
                saved_count = 0
                for post in filtered_posts:
                    if db.insert_filtered_post(post):
                        saved_count += 1

                result = {
                    "processed": len(unfiltered_posts),
                    "filtered": len(filtered_posts),
                    "saved": saved_count,
                    "filter_stats": filter.get_statistics()
                }

            self.stats["stages_completed"].append("filter")
            self.stats["stage_results"]["filter"] = result

            logger.info(f"âœ… Stage 2 completed: Filtered {result['saved']} posts")
            return result

        except Exception as e:
            logger.error(f"âŒ Stage 2 failed: {e}")
            self.stats["stages_failed"].append("filter")
            raise

    def run_stage_extract(self, limit_posts: Optional[int] = None) -> Dict[str, Any]:
        """é˜¶æ®µ3: ç—›ç‚¹æŠ½å–"""
        logger.info("=" * 50)
        logger.info("STAGE 3: Extracting pain points")
        logger.info("=" * 50)

        try:
            extractor = PainPointExtractor()
            result = extractor.process_unextracted_posts(limit=limit_posts or 100)

            self.stats["stages_completed"].append("extract")
            self.stats["stage_results"]["extract"] = result

            logger.info(f"âœ… Stage 3 completed: Extracted {result['pain_events_saved']} pain events")
            return result

        except Exception as e:
            logger.error(f"âŒ Stage 3 failed: {e}")
            self.stats["stages_failed"].append("extract")
            raise

    def run_stage_embed(self, limit_events: Optional[int] = None) -> Dict[str, Any]:
        """é˜¶æ®µ4: å‘é‡åŒ–"""
        logger.info("=" * 50)
        logger.info("STAGE 4: Creating embeddings")
        logger.info("=" * 50)

        try:
            embedder = PainEventEmbedder()
            result = embedder.process_missing_embeddings(limit=limit_events or 200)

            self.stats["stages_completed"].append("embed")
            self.stats["stage_results"]["embed"] = result

            logger.info(f"âœ… Stage 4 completed: Created {result['embeddings_created']} embeddings")
            return result

        except Exception as e:
            logger.error(f"âŒ Stage 4 failed: {e}")
            self.stats["stages_failed"].append("embed")
            raise

    def run_stage_cluster(self, limit_events: Optional[int] = None) -> Dict[str, Any]:
        """é˜¶æ®µ5: èšç±»"""
        logger.info("=" * 50)
        logger.info("STAGE 5: Clustering pain events")
        logger.info("=" * 50)

        try:
            clusterer = PainEventClusterer()
            result = clusterer.cluster_pain_events(limit=limit_events or 200)

            self.stats["stages_completed"].append("cluster")
            self.stats["stage_results"]["cluster"] = result

            logger.info(f"âœ… Stage 5 completed: Created {result['clusters_created']} clusters")
            return result

        except Exception as e:
            logger.error(f"âŒ Stage 5 failed: {e}")
            self.stats["stages_failed"].append("cluster")
            raise

    def run_stage_map_opportunities(self, limit_clusters: Optional[int] = None) -> Dict[str, Any]:
        """é˜¶æ®µ6: æœºä¼šæ˜ å°„"""
        logger.info("=" * 50)
        logger.info("STAGE 6: Mapping opportunities")
        logger.info("=" * 50)

        try:
            mapper = OpportunityMapper()
            result = mapper.map_opportunities_for_clusters(limit=limit_clusters or 50)

            self.stats["stages_completed"].append("map_opportunities")
            self.stats["stage_results"]["map_opportunities"] = result

            logger.info(f"âœ… Stage 6 completed: Mapped {result['opportunities_created']} opportunities")
            return result

        except Exception as e:
            logger.error(f"âŒ Stage 6 failed: {e}")
            self.stats["stages_failed"].append("map_opportunities")
            raise

    def run_stage_score(self, limit_opportunities: Optional[int] = None) -> Dict[str, Any]:
        """é˜¶æ®µ7: å¯è¡Œæ€§è¯„åˆ†"""
        logger.info("=" * 50)
        logger.info("STAGE 7: Scoring viability")
        logger.info("=" * 50)

        try:
            scorer = ViabilityScorer()
            result = scorer.score_opportunities(limit=limit_opportunities or 100)

            self.stats["stages_completed"].append("score")
            self.stats["stage_results"]["score"] = result

            logger.info(f"âœ… Stage 7 completed: Scored {result['opportunities_scored']} opportunities")
            return result

        except Exception as e:
            logger.error(f"âŒ Stage 7 failed: {e}")
            self.stats["stages_failed"].append("score")
            raise

    def generate_final_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š"""
        logger.info("=" * 50)
        logger.info("GENERATING FINAL REPORT")
        logger.info("=" * 50)

        try:
            # è·å–æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯
            db_stats = db.get_statistics()

            # è·å–æœ€é«˜åˆ†çš„æœºä¼š
            top_opportunities = []
            try:
                with db.get_connection("clusters") as conn:
                    cursor = conn.execute("""
                        SELECT o.opportunity_name, o.total_score, o.recommendation, c.cluster_name
                        FROM opportunities o
                        JOIN clusters c ON o.cluster_id = c.id
                        WHERE o.total_score > 0
                        ORDER BY o.total_score DESC
                        LIMIT 10
                    """)
                    top_opportunities = [dict(row) for row in cursor.fetchall()]
            except Exception as e:
                logger.warning(f"Failed to get top opportunities: {e}")

            # è®¡ç®—è¿è¡Œæ—¶é—´
            end_time = datetime.now()
            total_runtime = (end_time - self.pipeline_start_time).total_seconds()
            self.stats["total_runtime_seconds"] = total_runtime

            # ç”Ÿæˆæœ€ç»ˆæ‘˜è¦
            final_summary = {
                "pipeline_completed": len(self.stats["stages_failed"]) == 0,
                "stages_completed": len(self.stats["stages_completed"]),
                "stages_failed": len(self.stats["stages_failed"]),
                "total_runtime_minutes": round(total_runtime / 60, 2),
                "database_statistics": db_stats,
                "top_opportunities": top_opportunities,
                "pipeline_efficiency": {
                    "posts_per_minute": self.stats["stage_results"].get("fetch", {}).get("total_saved", 0) / max(total_runtime / 60, 1),
                    "pain_events_per_hour": self.stats["stage_results"].get("extract", {}).get("pain_events_saved", 0) / max(total_runtime / 3600, 1),
                    "opportunities_per_hour": self.stats["stage_results"].get("map_opportunities", {}).get("opportunities_created", 0) / max(total_runtime / 3600, 1)
                }
            }

            self.stats["final_summary"] = final_summary

            logger.info("ğŸ‰ PIPELINE COMPLETED SUCCESSFULLY!")
            logger.info(f"ğŸ“Š Final Summary:")
            logger.info(f"   â€¢ Runtime: {final_summary['total_runtime_minutes']} minutes")
            logger.info(f"   â€¢ Stages completed: {final_summary['stages_completed']}/7")
            logger.info(f"   â€¢ Raw posts collected: {db_stats.get('raw_posts_count', 0)}")
            logger.info(f"   â€¢ Pain events extracted: {db_stats.get('pain_events_count', 0)}")
            logger.info(f"   â€¢ Clusters created: {db_stats.get('clusters_count', 0)}")
            logger.info(f"   â€¢ Opportunities identified: {db_stats.get('opportunities_count', 0)}")

            if top_opportunities:
                logger.info(f"   â€¢ Top opportunity: {top_opportunities[0]['opportunity_name']} (Score: {top_opportunities[0]['total_score']:.1f})")

            return final_summary

        except Exception as e:
            logger.error(f"Failed to generate final report: {e}")
            return {"error": str(e)}

    def run_full_pipeline(
        self,
        limit_subreddits: Optional[int] = None,
        limit_posts: Optional[int] = None,
        limit_events: Optional[int] = None,
        limit_clusters: Optional[int] = None,
        limit_opportunities: Optional[int] = None,
        stop_on_error: bool = False
    ) -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´pipeline"""
        logger.info("ğŸš€ Starting Reddit Pain Point Finder Pipeline")
        logger.info(f"â° Started at: {self.pipeline_start_time}")

        stages = [
            ("fetch", lambda: self.run_stage_fetch(limit_subreddits)),
            ("filter", lambda: self.run_stage_filter(limit_posts)),
            ("extract", lambda: self.run_stage_extract(limit_posts)),
            ("embed", lambda: self.run_stage_embed(limit_events)),
            ("cluster", lambda: self.run_stage_cluster(limit_events)),
            ("map_opportunities", lambda: self.run_stage_map_opportunities(limit_clusters)),
            ("score", lambda: self.run_stage_score(limit_opportunities))
        ]

        for stage_name, stage_func in stages:
            try:
                stage_func()
            except Exception as e:
                logger.error(f"Stage '{stage_name}' failed: {e}")
                if stop_on_error:
                    logger.error("Stopping pipeline due to error")
                    break
                else:
                    logger.warning(f"Continuing pipeline despite '{stage_name}' failure")

        # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
        final_report = self.generate_final_report()

        return final_report

    def run_single_stage(self, stage_name: str, **kwargs) -> Dict[str, Any]:
        """è¿è¡Œå•ä¸ªé˜¶æ®µ"""
        stage_map = {
            "fetch": lambda: self.run_stage_fetch(kwargs.get("limit_subreddits")),
            "filter": lambda: self.run_stage_filter(kwargs.get("limit_posts")),
            "extract": lambda: self.run_stage_extract(kwargs.get("limit_posts")),
            "embed": lambda: self.run_stage_embed(kwargs.get("limit_events")),
            "cluster": lambda: self.run_stage_cluster(kwargs.get("limit_events")),
            "map": lambda: self.run_stage_map_opportunities(kwargs.get("limit_clusters")),
            "score": lambda: self.run_stage_score(kwargs.get("limit_opportunities"))
        }

        if stage_name not in stage_map:
            raise ValueError(f"Unknown stage: {stage_name}")

        return stage_map[stage_name]()

    def save_results(self, filename: Optional[str] = None):
        """ä¿å­˜pipelineç»“æœ"""
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"pipeline_results_{timestamp}.json"

        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(self.stats, f, indent=2, default=str)

            logger.info(f"ğŸ“ Results saved to: {filename}")
            return filename

        except Exception as e:
            logger.error(f"Failed to save results: {e}")
            return None

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="Reddit Pain Point Finder Pipeline")

    # è¿è¡Œæ¨¡å¼
    parser.add_argument("--stage", choices=["fetch", "filter", "extract", "embed", "cluster", "map", "score", "all"],
                       default="all", help="Which stage to run (default: all)")

    # é™åˆ¶å‚æ•°
    parser.add_argument("--limit-subreddits", type=int, help="Limit number of subreddits to fetch")
    parser.add_argument("--limit-posts", type=int, help="Limit number of posts to process")
    parser.add_argument("--limit-events", type=int, help="Limit number of pain events to process")
    parser.add_argument("--limit-clusters", type=int, help="Limit number of clusters to process")
    parser.add_argument("--limit-opportunities", type=int, help="Limit number of opportunities to score")

    # å…¶ä»–é€‰é¡¹
    parser.add_argument("--stop-on-error", action="store_true", help="Stop pipeline on first error")
    parser.add_argument("--save-results", action="store_true", help="Save results to file")
    parser.add_argument("--results-file", help="Custom results filename")

    args = parser.parse_args()

    try:
        # åˆå§‹åŒ–pipeline
        pipeline = RedditPainPipeline()

        if args.stage == "all":
            # è¿è¡Œå®Œæ•´pipeline
            result = pipeline.run_full_pipeline(
                limit_subreddits=args.limit_subreddits,
                limit_posts=args.limit_posts,
                limit_events=args.limit_events,
                limit_clusters=args.limit_clusters,
                limit_opportunities=args.limit_opportunities,
                stop_on_error=args.stop_on_error
            )
        else:
            # è¿è¡Œå•ä¸ªé˜¶æ®µ
            stage_kwargs = {
                "limit_subreddits": args.limit_subreddits,
                "limit_posts": args.limit_posts,
                "limit_events": args.limit_events,
                "limit_clusters": args.limit_clusters,
                "limit_opportunities": args.limit_opportunities
            }

            # åªä¼ é€’ç›¸å…³çš„å‚æ•°
            relevant_kwargs = {k: v for k, v in stage_kwargs.items() if v is not None}
            result = pipeline.run_single_stage(args.stage, **relevant_kwargs)

        # ä¿å­˜ç»“æœ
        if args.save_results:
            pipeline.save_results(args.results_file)

        # è¾“å‡ºç»“æœ
        print("\n" + "=" * 60)
        print("PIPELINE RESULTS")
        print("=" * 60)
        print(json.dumps(result, indent=2, default=str))

    except KeyboardInterrupt:
        logger.info("Pipeline interrupted by user")
        sys.exit(1)
    except Exception as e:
        logger.error(f"Pipeline failed: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()
```


================================================================================
æ–‡ä»¶: pain_point_analyzer.py
================================================================================

```python
#!/usr/bin/env python3
"""
ç—›ç‚¹åº”ç”¨åˆ†æå™¨

é’ˆå¯¹æ¯ä¸ªç—›ç‚¹èšç±»ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Šï¼ŒåŒ…å«ï¼š
1. ç—›ç‚¹åˆ†æ
2. åº”ç”¨è®¾è®¡æ–¹æ¡ˆ
3. å¯æ‰§è¡Œæœºä¼šæ¸…å•

æ¯ä¸ªèšç±»ç”Ÿæˆä¸€ä¸ªç‹¬ç«‹çš„markdownæ–‡ä»¶
"""

import os
import sqlite3
import json
import requests
from datetime import datetime
from typing import Dict, List, Any, Optional
import re
from pathlib import Path
import logging
import sys

# åŠ è½½.envæ–‡ä»¶
def load_env():
    """åŠ è½½.envæ–‡ä»¶"""
    env_path = Path(__file__).parent / '.env'
    if env_path.exists():
        with open(env_path, 'r') as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith('#'):
                    key, value = line.split('=', 1)
                    os.environ[key.strip()] = value.strip()

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),  # è¾“å‡ºåˆ°æ§åˆ¶å°
        logging.FileHandler('pain_point_analyzer.log', encoding='utf-8')  # åŒæ—¶è¾“å‡ºåˆ°æ–‡ä»¶
    ]
)

logger = logging.getLogger(__name__)

# åŠ è½½ç¯å¢ƒå˜é‡
logger.info("å¼€å§‹åŠ è½½ç¯å¢ƒå˜é‡...")
load_env()
logger.info("ç¯å¢ƒå˜é‡åŠ è½½å®Œæˆ")


class PainPointAnalyzer:
    def __init__(self):
        """åˆå§‹åŒ–åˆ†æå™¨"""
        logger.info("åˆå§‹åŒ– PainPointAnalyzer...")

        self.base_url = os.getenv('Siliconflow_Base_URL', 'https://api.siliconflow.cn/v1')
        self.api_key = os.getenv('Siliconflow_KEY')
        self.model = os.getenv('Siliconflow_AI_Model_Default', 'deepseek-ai/DeepSeek-V3.2')

        logger.info(f"é…ç½®ä¿¡æ¯: base_url={self.base_url}, model={self.model}")
        logger.info(f"API key {'å·²è®¾ç½®' if self.api_key else 'æœªè®¾ç½®'}")

        if not self.api_key:
            logger.error("SiliconFlow API key not found in environment variables")
            raise ValueError("SiliconFlow API key not found in environment variables")

        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir = "pain_analysis_reports"
        os.makedirs(self.output_dir, exist_ok=True)
        logger.info(f"è¾“å‡ºç›®å½•å·²åˆ›å»º: {self.output_dir}")

        print(f"ğŸ”§ åˆå§‹åŒ–åˆ†æå™¨")
        print(f"   â€¢ APIæ¨¡å‹: {self.model}")
        print(f"   â€¢ è¾“å‡ºç›®å½•: {self.output_dir}")

    def get_db_connection(self, db_file: str) -> sqlite3.Connection:
        """è·å–æ•°æ®åº“è¿æ¥"""
        logger.debug(f"å°è¯•è¿æ¥æ•°æ®åº“: {db_file}")

        if not os.path.exists(db_file):
            logger.error(f"æ•°æ®åº“æ–‡ä»¶ä¸å­˜åœ¨: {db_file}")
            raise FileNotFoundError(f"æ•°æ®åº“æ–‡ä»¶ä¸å­˜åœ¨: {db_file}")

        try:
            conn = sqlite3.connect(db_file)
            conn.row_factory = sqlite3.Row
            logger.debug(f"æ•°æ®åº“è¿æ¥æˆåŠŸ: {db_file}")
            return conn
        except Exception as e:
            logger.error(f"è¿æ¥æ•°æ®åº“å¤±è´¥: {db_file}, é”™è¯¯: {e}")
            raise

    def call_llm(self, prompt: str, temperature: float = 0.3, max_retries: int = 3) -> str:
        """è°ƒç”¨LLM"""
        logger.info(f"å¼€å§‹è°ƒç”¨LLM: model={self.model}, temperature={temperature}, max_retries={max_retries}")
        logger.debug(f"prompté•¿åº¦: {len(prompt)} å­—ç¬¦")

        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }

        data = {
            "model": self.model,
            "messages": [
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä½èµ„æ·±çš„äº§å“åˆ†æå¸ˆå’ŒæŠ€æœ¯é¡¾é—®ï¼Œä¸“é—¨åˆ†æç”¨æˆ·ç—›ç‚¹å¹¶è®¾è®¡åˆ›æ–°çš„è§£å†³æ–¹æ¡ˆã€‚"},
                {"role": "user", "content": prompt}
            ],
            "temperature": temperature,
            "max_tokens": 4000
        }

        for attempt in range(max_retries):
            try:
                print(f"  ğŸ¤– è°ƒç”¨LLM (å°è¯• {attempt + 1}/{max_retries})...")
                logger.info(f"å°è¯•ç¬¬ {attempt + 1}/{max_retries} æ¬¡LLMè°ƒç”¨")

                url = f"{self.base_url}/chat/completions"
                logger.debug(f"è¯·æ±‚URL: {url}")

                response = requests.post(
                    url,
                    headers=headers,
                    json=data,
                    timeout=180  # å¢åŠ åˆ°3åˆ†é’Ÿ
                )

                logger.debug(f"å“åº”çŠ¶æ€ç : {response.status_code}")

                response.raise_for_status()
                result = response.json()

                if 'choices' not in result or len(result['choices']) == 0:
                    logger.error("LLMå“åº”ä¸­æ²¡æœ‰choiceså­—æ®µ")
                    return "LLMå“åº”æ ¼å¼é”™è¯¯: æ²¡æœ‰choices"

                content = result['choices'][0]['message']['content'].strip()
                logger.debug(f"LLMå“åº”é•¿åº¦: {len(content)} å­—ç¬¦")
                print(f"  âœ… LLMå“åº”æˆåŠŸ")
                logger.info("LLMè°ƒç”¨æˆåŠŸ")
                return content

            except requests.exceptions.Timeout:
                error_msg = f"LLMè°ƒç”¨è¶…æ—¶ (å°è¯• {attempt + 1}/{max_retries})"
                logger.warning(error_msg)
                print(f"  âš ï¸ {error_msg}")
                if attempt < max_retries - 1:
                    continue
                logger.error(f"LLMè°ƒç”¨è¶…æ—¶ï¼Œå·²é‡è¯•{max_retries}æ¬¡")
                return f"LLMè°ƒç”¨è¶…æ—¶: å·²é‡è¯•{max_retries}æ¬¡"

            except requests.exceptions.HTTPError as e:
                error_msg = f"HTTPé”™è¯¯: {e}"
                logger.error(error_msg)
                logger.error(f"å“åº”å†…å®¹: {response.text if 'response' in locals() else 'N/A'}")
                print(f"  âŒ {error_msg}")
                if attempt < max_retries - 1:
                    print(f"  ğŸ”„ æ­£åœ¨é‡è¯•...")
                    continue
                return f"LLM HTTPé”™è¯¯: {str(e)}"

            except Exception as e:
                error_msg = f"LLMè°ƒç”¨å¤±è´¥: {e}"
                logger.error(error_msg)
                import traceback
                logger.error(traceback.format_exc())
                print(f"  âŒ {error_msg}")
                if attempt < max_retries - 1:
                    print(f"  ğŸ”„ æ­£åœ¨é‡è¯•...")
                    continue
                return f"LLMè°ƒç”¨å¤±è´¥: {str(e)}"

    def get_top_clusters(self, min_score: float = 0.8, limit: int = 10) -> List[Dict]:
        """è·å–é«˜åˆ†èšç±»"""
        logger.info(f"è·å–é«˜åˆ†èšç±»: min_score={min_score}, limit={limit}")
        clusters = []

        try:
            conn = self.get_db_connection('data/clusters.db')
            cursor = conn.cursor()

            logger.debug("æ‰§è¡Œèšç±»æŸ¥è¯¢SQL...")

            cursor.execute("""
                SELECT c.id, c.cluster_name, c.cluster_description, c.avg_pain_score,
                       c.cluster_size, c.pain_event_ids,
                       COUNT(o.id) as opportunity_count,
                       MAX(o.total_score) as max_opportunity_score,
                       GROUP_CONCAT(o.opportunity_name, ' | ') as opportunity_names
                FROM clusters c
                LEFT JOIN opportunities o ON c.id = o.cluster_id
                GROUP BY c.id
                HAVING opportunity_count > 0 AND max_opportunity_score >= ?
                ORDER BY max_opportunity_score DESC, c.avg_pain_score DESC
                LIMIT ?
            """, (min_score, limit))

            logger.debug(f"æŸ¥è¯¢æ‰§è¡Œå®Œæˆï¼Œå¼€å§‹å¤„ç†ç»“æœ...")
            rows = cursor.fetchall()
            logger.info(f"æŸ¥è¯¢åˆ° {len(rows)} ä¸ªèšç±»")

            for i, row in enumerate(rows, 1):
                logger.debug(f"å¤„ç†ç¬¬ {i}/{len(rows)} ä¸ªèšç±»: {row['cluster_name'][:50]}...")
                # è·å–è¯¥èšç±»çš„æ‰€æœ‰æœºä¼š
                logger.debug(f"è·å–èšç±» {row['id']} çš„æœºä¼šæ•°æ®...")
                cursor.execute("""
                    SELECT opportunity_name, description, total_score, recommendation,
                           current_tools, missing_capability, why_existing_fail,
                           target_users, killer_risks
                    FROM opportunities
                    WHERE cluster_id = ?
                    ORDER BY total_score DESC
                """, (row['id'],))

                opportunities = []
                opp_rows = cursor.fetchall()
                logger.debug(f"èšç±» {row['id']} æœ‰ {len(opp_rows)} ä¸ªæœºä¼š")

                for opp_row in opp_rows:
                    opportunities.append({
                        'name': opp_row['opportunity_name'],
                        'description': opp_row['description'],
                        'score': opp_row['total_score'],
                        'recommendation': opp_row['recommendation'],
                        'current_tools': opp_row['current_tools'],
                        'missing_capability': opp_row['missing_capability'],
                        'why_existing_fail': opp_row['why_existing_fail'],
                        'target_users': opp_row['target_users'],
                        'killer_risks': json.loads(opp_row['killer_risks']) if opp_row['killer_risks'] else []
                    })

                # è·å–ç—›ç‚¹äº‹ä»¶æ ·æœ¬
                try:
                    pain_event_ids = json.loads(row['pain_event_ids'])
                    logger.debug(f"èšç±» {row['id']} ç—›ç‚¹äº‹ä»¶IDs: {len(pain_event_ids)} ä¸ª")
                    sample_pains = self.get_sample_pain_events(pain_event_ids[:5])
                except json.JSONDecodeError as e:
                    logger.warning(f"èšç±» {row['id']} pain_event_ids JSONè§£æå¤±è´¥: {e}")
                    sample_pains = []

                clusters.append({
                    'id': row['id'],
                    'name': row['cluster_name'],
                    'description': row['cluster_description'],
                    'avg_pain_score': row['avg_pain_score'],
                    'cluster_size': row['cluster_size'],
                    'opportunity_count': row['opportunity_count'],
                    'max_opportunity_score': row['max_opportunity_score'],
                    'opportunities': opportunities,
                    'sample_pains': sample_pains
                })

            conn.close()
            logger.info(f"æˆåŠŸè·å– {len(clusters)} ä¸ªèšç±»æ•°æ®")
            return clusters

        except Exception as e:
            logger.error(f"è·å–èšç±»æ•°æ®å¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return []

    def get_sample_pain_events(self, pain_event_ids: List[int]) -> List[Dict]:
        """è·å–ç—›ç‚¹äº‹ä»¶æ ·æœ¬"""
        logger.debug(f"è·å– {len(pain_event_ids)} ä¸ªç—›ç‚¹äº‹ä»¶æ ·æœ¬: {pain_event_ids}")
        pains = []

        if not pain_event_ids:
            logger.warning("pain_event_ids ä¸ºç©ºï¼Œè¿”å›ç©ºåˆ—è¡¨")
            return []

        try:
            conn = self.get_db_connection('data/pain_events.db')
            cursor = conn.cursor()

            placeholders = ','.join(['?' for _ in pain_event_ids])
            logger.debug(f"æ‰§è¡Œç—›ç‚¹äº‹ä»¶æŸ¥è¯¢ï¼ŒIDs: {pain_event_ids}")

            cursor.execute(f"""
                SELECT problem, current_workaround, frequency, emotional_signal, mentioned_tools
                FROM pain_events
                WHERE id IN ({placeholders})
            """, pain_event_ids)

            rows = cursor.fetchall()
            logger.debug(f"æŸ¥è¯¢åˆ° {len(rows)} ä¸ªç—›ç‚¹äº‹ä»¶")

            for row in rows:
                pains.append({
                    'problem': row['problem'],
                    'workaround': row['current_workaround'],
                    'frequency': row['frequency'],
                    'emotion': row['emotional_signal'],
                    'tools': row['mentioned_tools']
                })

            conn.close()
            logger.debug(f"æˆåŠŸè·å– {len(pains)} ä¸ªç—›ç‚¹äº‹ä»¶")
            return pains

        except Exception as e:
            logger.error(f"è·å–ç—›ç‚¹äº‹ä»¶å¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return []

    def generate_basic_analysis(self, cluster: Dict) -> str:
        """ç”ŸæˆåŸºç¡€åˆ†æï¼ˆå½“LLMè°ƒç”¨å¤±è´¥æ—¶ï¼‰"""
        pain_context = "\n".join([
            f"â€¢ {pain['problem']}" + chr(10) + f"  å½“å‰è§£å†³æ–¹æ¡ˆ: {pain['workaround']}" + chr(10) + f"  å‘ç”Ÿé¢‘ç‡: {pain['frequency']}" + chr(10) + f"  æƒ…ç»ªä¿¡å·: {pain['emotion']}"
            for pain in cluster['sample_pains']
        ])

        opp_analysis = ""
        for opp in cluster['opportunities'][:3]:
            opp_analysis += f"""
### {opp['name']} (è¯„åˆ†: {opp['score']:.2f})

**é—®é¢˜æè¿°**: {opp['description']}

**å…³é”®æœºä¼šåˆ†æ**:
- å¸‚åœºéœ€æ±‚: é€šè¿‡{cluster['cluster_size']}ä¸ªç›¸å…³å¸–å­éªŒè¯äº†å¼ºçƒˆéœ€æ±‚
- ç›®æ ‡ç”¨æˆ·: {opp['target_users'] or 'ä¸­å°ä¼ä¸šã€ä¸ªäººå¼€å‘è€…ã€è‡ªç”±èŒä¸šè€…'}
- ç«äº‰ä¼˜åŠ¿: {opp['missing_capability'] or 'å¡«è¡¥ç°æœ‰å·¥å…·çš„åŠŸèƒ½ç©ºç™½'}

**MVPåŠŸèƒ½å»ºè®®**:
1. æ ¸å¿ƒåŠŸèƒ½å®ç°{opp['current_tools'] and f"ï¼Œæ•´åˆ{opp['current_tools']}çš„å·¥ä½œæµ"}
2. ç®€åŒ–ç”¨æˆ·ç•Œé¢ï¼Œé™ä½å­¦ä¹ æˆæœ¬
3. å¿«é€Ÿéƒ¨ç½²å’Œé›†æˆèƒ½åŠ›

**å•†ä¸šåŒ–å»ºè®®**:
- å…è´¹åŸºç¡€ç‰ˆå¸å¼•åˆå§‹ç”¨æˆ·
- Proç‰ˆæœ¬æœˆè´¹$10-20
- ä¼ä¸šå®šåˆ¶ç‰ˆæ”¯æŒ
"""

        return f"""## ç—›ç‚¹æ·±åº¦åˆ†æ

### æ ¸å¿ƒé—®é¢˜
{cluster['description']}

### å½±å“èŒƒå›´
- å—å½±å“ç”¨æˆ·ç¾¤ä½“: {cluster['cluster_size']}ä¸ªçœŸå®ç”¨æˆ·åé¦ˆ
- ç—›ç‚¹å¼ºåº¦: {cluster['avg_pain_score']:.2f}/1.0

### å…¸å‹ç—›ç‚¹äº‹ä»¶
{pain_context}

## å¸‚åœºæœºä¼šè¯„ä¼°

### å¸‚åœºè§„æ¨¡
åŸºäºRedditè®¨è®ºçƒ­åº¦ï¼Œè¯¥é—®é¢˜å½±å“äº†å¤§é‡ç”¨æˆ·ï¼Œå…·æœ‰æ˜ç¡®çš„ä»˜è´¹æ„æ„¿ã€‚

### æœºä¼šæ•°é‡
å·²è¯†åˆ«{cluster['opportunity_count']}ä¸ªå…·ä½“æœºä¼šï¼Œæœ€é«˜è¯„åˆ†{cluster['max_opportunity_score']:.2f}

## äº§å“è®¾è®¡æ–¹æ¡ˆ

{opp_analysis}

## å¯æ‰§è¡Œè¡ŒåŠ¨è®¡åˆ’

### ç«‹å³è¡ŒåŠ¨ï¼ˆ1ä¸ªæœˆå†…ï¼‰
1. éªŒè¯ç›®æ ‡ç”¨æˆ·éœ€æ±‚ï¼Œè¿›è¡Œæ·±åº¦ç”¨æˆ·è®¿è°ˆ
2. å¼€å‘æœ€å°å¯è¡Œäº§å“(MVP)åŸå‹
3. å»ºç«‹ç”¨æˆ·åé¦ˆæ¸ é“

### çŸ­æœŸç›®æ ‡ï¼ˆ3ä¸ªæœˆå†…ï¼‰
1. å‘å¸ƒMVPç‰ˆæœ¬å¹¶è·å–é¦–æ‰¹100ä¸ªç”¨æˆ·
2. åŸºäºåé¦ˆè¿­ä»£äº§å“åŠŸèƒ½
3. æ¢ç´¢ç›ˆåˆ©æ¨¡å¼

### æˆåŠŸæŒ‡æ ‡
- ç”¨æˆ·ç•™å­˜ç‡ > 60%
- æœˆæ´»è·ƒç”¨æˆ·å¢é•¿ > 20%
- NPSå¾—åˆ† > 40
"""

    def analyze_cluster(self, cluster: Dict) -> str:
        """åˆ†æå•ä¸ªèšç±»å¹¶ç”Ÿæˆå®Œæ•´æŠ¥å‘Š"""

        # æ„å»ºåˆ†æprompt
        pain_context = "\n".join([
            f"â€¢ {pain['problem']} (å½“å‰è§£å†³æ–¹æ¡ˆ: {pain['workaround']}, é¢‘ç‡: {pain['frequency']}, æƒ…ç»ª: {pain['emotion']})"
            for pain in cluster['sample_pains']
        ])

        opportunities_context = "\n".join([
            f"â€¢ {opp['name']} (è¯„åˆ†: {opp['score']:.2f})"
            f"  æè¿°: {opp['description'][:100]}..."
            for opp in cluster['opportunities'][:3]
        ])

        prompt = f"""
è¯·åˆ†æä»¥ä¸‹ç—›ç‚¹èšç±»å¹¶ç”Ÿæˆç»¼åˆæŠ¥å‘Šï¼š

## èšç±»ä¿¡æ¯
- **èšç±»åç§°**: {cluster['name']}
- **èšç±»æè¿°**: {cluster['description']}
- **ç—›ç‚¹æ•°é‡**: {cluster['cluster_size']}
- **å¹³å‡ç—›ç‚¹å¼ºåº¦**: {cluster['avg_pain_score']:.2f}
- **æœºä¼šæ•°é‡**: {cluster['opportunity_count']}

## å…¸å‹ç—›ç‚¹æ ·æœ¬
{pain_context}

## å·²è¯†åˆ«çš„æœºä¼š
{opportunities_context}

## åˆ†æè¦æ±‚
è¯·æŒ‰ç…§ä»¥ä¸‹ç»“æ„ç”Ÿæˆè¯¦ç»†åˆ†ææŠ¥å‘Šï¼š

### 1. ç—›ç‚¹æ·±åº¦åˆ†æ
- æ ¸å¿ƒé—®é¢˜æœ¬è´¨
- å½±å“èŒƒå›´å’Œä¸¥é‡ç¨‹åº¦
- ç”¨æˆ·ç‰¹å¾å’Œä½¿ç”¨åœºæ™¯
- ç°æœ‰è§£å†³æ–¹æ¡ˆçš„ä¸è¶³

### 2. å¸‚åœºæœºä¼šè¯„ä¼°
- å¸‚åœºè§„æ¨¡ä¼°ç®—
- ç”¨æˆ·ä»˜è´¹æ„æ„¿
- ç«äº‰æ ¼å±€åˆ†æ
- è¿›å…¥å£å’è¯„ä¼°

### 3. äº§å“è®¾è®¡æ–¹æ¡ˆ
- MVPåŠŸèƒ½å®šä¹‰
- æŠ€æœ¯æ¶æ„å»ºè®®
- ç”¨æˆ·ä½“éªŒè®¾è®¡è¦ç‚¹
- å·®å¼‚åŒ–ç«äº‰ç­–ç•¥

### 4. å•†ä¸šåŒ–è·¯å¾„
- ç›ˆåˆ©æ¨¡å¼è®¾è®¡
- è·å®¢ç­–ç•¥
- å®šä»·ç­–ç•¥
- å‘å±•è·¯çº¿å›¾

### 5. å¯æ‰§è¡Œè¡ŒåŠ¨è®¡åˆ’
- è¿‘æœŸè¡ŒåŠ¨é¡¹ï¼ˆ1-3ä¸ªæœˆï¼‰
- ä¸­æœŸç›®æ ‡ï¼ˆ3-6ä¸ªæœˆï¼‰
- å…³é”®æˆåŠŸæŒ‡æ ‡
- é£é™©åº”å¯¹æªæ–½

è¯·ç¡®ä¿åˆ†ææ·±å…¥ã€å…·ä½“ä¸”å¯æ“ä½œã€‚ä½¿ç”¨markdownæ ¼å¼è¾“å‡ºã€‚
"""

        print(f"ğŸ¤– æ­£åœ¨åˆ†æèšç±»: {cluster['name'][:50]}...")

        # å°è¯•è°ƒç”¨LLM
        analysis = self.call_llm(prompt, temperature=0.4)

        # å¦‚æœLLMè°ƒç”¨å¤±è´¥ï¼Œä½¿ç”¨åŸºç¡€åˆ†æ
        if "LLMè°ƒç”¨" in analysis:
            print(f"  âš ï¸ ä½¿ç”¨åŸºç¡€åˆ†ææ›¿ä»£")
            analysis = self.generate_basic_analysis(cluster)

        return analysis

    def generate_cluster_report(self, cluster: Dict, analysis: str) -> str:
        """ç”Ÿæˆèšç±»æŠ¥å‘Šæ–‡ä»¶"""
        logger.info(f"ç”Ÿæˆèšç±»æŠ¥å‘Š: {cluster['name'][:50]}...")

        # æ¸…ç†æ–‡ä»¶å
        safe_name = re.sub(r'[^\w\s-]', '', cluster['name']).strip()
        safe_name = re.sub(r'[-\s]+', '_', safe_name)
        filename = f"{safe_name}_opportunity_analysis.md"
        filepath = os.path.join(self.output_dir, filename)

        logger.debug(f"æŠ¥å‘Šæ–‡ä»¶è·¯å¾„: {filepath}")

        # æ„å»ºå®Œæ•´æŠ¥å‘Š
        report_content = f"""# {cluster['name']} - æœºä¼šåˆ†ææŠ¥å‘Š

> **ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
> **èšç±»ID**: {cluster['id']}
> **ç—›ç‚¹æ•°é‡**: {cluster['cluster_size']}
> **å¹³å‡ç—›ç‚¹å¼ºåº¦**: {cluster['avg_pain_score']:.2f}
> **æœºä¼šæ•°é‡**: {cluster['opportunity_count']}

---

## ğŸ“Š èšç±»æ¦‚è§ˆ

**èšç±»æè¿°**: {cluster['description']}

### ğŸ¯ é¡¶çº§æœºä¼š
{chr(10).join([f"- **{opp['name']}** (è¯„åˆ†: {opp['score']:.2f})" for opp in cluster['opportunities'][:5]])}

---

## ğŸ” æ·±åº¦åˆ†æ

{analysis}

---

## ğŸ“‹ åŸå§‹æ•°æ®

### å…¸å‹ç—›ç‚¹äº‹ä»¶
{chr(10).join([f"**é—®é¢˜**: {pain['problem']}" + chr(10) + f"- å½“å‰æ–¹æ¡ˆ: {pain['workaround']}" + chr(10) + f"- å‘ç”Ÿé¢‘ç‡: {pain['frequency']}" + chr(10) + f"- æƒ…ç»ªä¿¡å·: {pain['emotion']}" + chr(10) for pain in cluster['sample_pains']])}

### å·²è¯†åˆ«æœºä¼šè¯¦æƒ…
{chr(10).join([f"**{opp['name']}** (è¯„åˆ†: {opp['score']:.2f})" + chr(10) + f"- æè¿°: {opp['description']}" + chr(10) + f"- æ¨èå»ºè®®: {opp['recommendation']}" + chr(10) + (f"- ç›®æ ‡ç”¨æˆ·: {opp['target_users']}" if opp['target_users'] else "") + chr(10) for opp in cluster['opportunities']])}

---

*æœ¬æŠ¥å‘Šç”± Reddit Pain Point Finder è‡ªåŠ¨ç”Ÿæˆ*
"""

        # å†™å…¥æ–‡ä»¶
        try:
            logger.debug(f"å¼€å§‹å†™å…¥æŠ¥å‘Šæ–‡ä»¶: {filepath}")
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(report_content)
            logger.info(f"æŠ¥å‘Šç”ŸæˆæˆåŠŸ: {filepath}")
            print(f"âœ… æŠ¥å‘Šå·²ç”Ÿæˆ: {filepath}")
            return filepath
        except Exception as e:
            logger.error(f"æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {filepath}, é”™è¯¯: {e}")
            import traceback
            logger.error(traceback.format_exc())
            print(f"âŒ æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")
            return None

    def generate_summary_index(self, report_files: List[str]) -> str:
        """ç”Ÿæˆæ€»ç»“ç´¢å¼•æ–‡ä»¶"""
        logger.info(f"ç”Ÿæˆæ€»ç»“ç´¢å¼•ï¼ŒåŒ…å« {len(report_files)} ä¸ªæŠ¥å‘Š")

        index_content = f"""# ç—›ç‚¹æœºä¼šåˆ†ææŠ¥å‘Šç´¢å¼•

> **ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
> **åˆ†ææ•°é‡**: {len(report_files)}

---

## ğŸ“ˆ åˆ†ææ¦‚è§ˆ

æœ¬æ¬¡å…±åˆ†æäº† {len(report_files)} ä¸ªé«˜ä»·å€¼ç—›ç‚¹èšç±»ï¼Œæ¯ä¸ªèšç±»éƒ½åŒ…å«è¯¦ç»†çš„ç—›ç‚¹åˆ†æã€åº”ç”¨è®¾è®¡æ–¹æ¡ˆå’Œå¯æ‰§è¡Œè¡ŒåŠ¨è®¡åˆ’ã€‚

---

## ğŸ“‹ æŠ¥å‘Šåˆ—è¡¨

{chr(10).join([f"- [{os.path.basename(f)}]({f})" for f in report_files])}

---

## ğŸ¯ ä¸‹ä¸€æ­¥è¡ŒåŠ¨å»ºè®®

1. **ä¼˜å…ˆçº§æ’åº**: æ ¹æ®æœºä¼šè¯„åˆ†å’Œå¸‚åœºè§„æ¨¡ç¡®å®šäº§å“å¼€å‘ä¼˜å…ˆçº§
2. **ç”¨æˆ·éªŒè¯**: é’ˆå¯¹Top 3æœºä¼šè¿›è¡Œç”¨æˆ·è®¿è°ˆå’Œéœ€æ±‚éªŒè¯
3. **MVPå¼€å‘**: é€‰æ‹©æœ€é«˜ä»·å€¼çš„æœºä¼šå¯åŠ¨MVPå¼€å‘
4. **æŒç»­ç›‘æ§**: å®šæœŸæ›´æ–°Redditæ•°æ®ï¼Œè·Ÿè¸ªæ–°çš„ç—›ç‚¹è¶‹åŠ¿

---

*ä½¿ç”¨æ–¹æ³•: ç‚¹å‡»ä¸Šæ–¹é“¾æ¥æŸ¥çœ‹å…·ä½“çš„æœºä¼šåˆ†ææŠ¥å‘Š*
"""

        index_path = os.path.join(self.output_dir, "README.md")
        try:
            logger.debug(f"å¼€å§‹å†™å…¥ç´¢å¼•æ–‡ä»¶: {index_path}")
            with open(index_path, 'w', encoding='utf-8') as f:
                f.write(index_content)
            logger.info(f"ç´¢å¼•æ–‡ä»¶ç”ŸæˆæˆåŠŸ: {index_path}")
            print(f"ğŸ“‘ ç´¢å¼•æ–‡ä»¶å·²ç”Ÿæˆ: {index_path}")
            return index_path
        except Exception as e:
            logger.error(f"ç´¢å¼•æ–‡ä»¶ç”Ÿæˆå¤±è´¥: {index_path}, é”™è¯¯: {e}")
            import traceback
            logger.error(traceback.format_exc())
            print(f"âŒ ç´¢å¼•æ–‡ä»¶ç”Ÿæˆå¤±è´¥: {e}")
            return None

    def run_analysis(self, min_score: float = 0.8, limit: int = 10):
        """è¿è¡Œå®Œæ•´åˆ†æ"""
        logger.info(f"å¼€å§‹è¿è¡Œå®Œæ•´åˆ†æ: min_score={min_score}, limit={limit}")
        print(f"\nğŸš€ å¼€å§‹ç—›ç‚¹æœºä¼šåˆ†æ...")
        print(f"   â€¢ æœ€ä½æœºä¼šè¯„åˆ†: {min_score}")
        print(f"   â€¢ æœ€å¤§åˆ†ææ•°é‡: {limit}")
        print("="*60)

        # è·å–èšç±»æ•°æ®
        logger.info("å¼€å§‹è·å–èšç±»æ•°æ®...")
        clusters = self.get_top_clusters(min_score, limit)
        if not clusters:
            logger.warning("æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„èšç±»æ•°æ®")
            print("âŒ æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„èšç±»æ•°æ®")
            return

        logger.info(f"æˆåŠŸè·å– {len(clusters)} ä¸ªèšç±»")
        print(f"ğŸ“Š æ‰¾åˆ° {len(clusters)} ä¸ªé«˜ä»·å€¼èšç±»")

        # åˆ†ææ¯ä¸ªèšç±»
        report_files = []
        for i, cluster in enumerate(clusters, 1):
            logger.info(f"å¼€å§‹åˆ†æç¬¬ {i}/{len(clusters)} ä¸ªèšç±»: {cluster['name'][:50]}...")
            print(f"\n[{i}/{len(clusters)}] åˆ†æèšç±»: {cluster['name'][:50]}...")

            # æ‰§è¡Œåˆ†æ
            logger.debug("æ‰§è¡Œèšç±»åˆ†æ...")
            analysis = self.analyze_cluster(cluster)

            # ç”ŸæˆæŠ¥å‘Š
            logger.debug("ç”Ÿæˆèšç±»æŠ¥å‘Š...")
            report_path = self.generate_cluster_report(cluster, analysis)
            if report_path:
                report_files.append(report_path)
                logger.info(f"æŠ¥å‘Šå·²æ·»åŠ åˆ°åˆ—è¡¨: {report_path}")

            logger.info(f"èšç±» {i} åˆ†æå®Œæˆ")
            print(f"âœ… å®Œæˆ: {cluster['name'][:50]}")

        # ç”Ÿæˆç´¢å¼•æ–‡ä»¶
        if report_files:
            logger.info("å¼€å§‹ç”Ÿæˆæ€»ç»“ç´¢å¼•...")
            self.generate_summary_index(report_files)

        logger.info(f"åˆ†æå®Œæˆï¼Œç”Ÿæˆäº† {len(report_files)} ä¸ªæŠ¥å‘Š")
        print(f"\nğŸ‰ åˆ†æå®Œæˆï¼")
        print(f"   â€¢ ç”ŸæˆæŠ¥å‘Š: {len(report_files)} ä»½")
        print(f"   â€¢ è¾“å‡ºç›®å½•: {self.output_dir}")
        print(f"   â€¢ æŸ¥çœ‹ç´¢å¼•: {self.output_dir}/README.md")


def main():
    """ä¸»å‡½æ•°"""
    logger.info("=" * 50)
    logger.info("ç—›ç‚¹åˆ†æå™¨å¼€å§‹è¿è¡Œ")
    logger.info("=" * 50)

    try:
        logger.info("åˆå§‹åŒ– PainPointAnalyzer...")
        analyzer = PainPointAnalyzer()
        logger.info("å¼€å§‹è¿è¡Œåˆ†æ...")
        analyzer.run_analysis(min_score=0.8, limit=15)
        logger.info("ç¨‹åºæ‰§è¡Œå®Œæˆ")
    except Exception as e:
        logger.error(f"ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        logger.error(traceback.format_exc())
        print(f"âŒ ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        traceback.print_exc()


if __name__ == "__main__":
    main()
```


================================================================================
æ–‡ä»¶: analyze_pain_points.py
================================================================================

```python
#!/usr/bin/env python3
"""
å¿«é€Ÿå¯åŠ¨ç—›ç‚¹åˆ†æè„šæœ¬
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from pain_point_analyzer import PainPointAnalyzer

if __name__ == "__main__":
    print("ğŸ¯ Reddit Pain Point Finder - ç—›ç‚¹æœºä¼šåˆ†æå™¨")
    print("=" * 60)

    # å¯ä»¥é€šè¿‡å‘½ä»¤è¡Œå‚æ•°è°ƒæ•´
    min_score = 0.8
    limit = 15

    if len(sys.argv) > 1:
        try:
            min_score = float(sys.argv[1])
        except:
            pass

    if len(sys.argv) > 2:
        try:
            limit = int(sys.argv[2])
        except:
            pass

    print(f"å‚æ•°è®¾ç½®:")
    print(f"  â€¢ æœ€ä½æœºä¼šè¯„åˆ†: {min_score}")
    print(f"  â€¢ æœ€å¤§åˆ†ææ•°é‡: {limit}")
    print()

    try:
        analyzer = PainPointAnalyzer()
        analyzer.run_analysis(min_score=min_score, limit=limit)
    except KeyboardInterrupt:
        print("\n\nâš ï¸  åˆ†æè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ åˆ†æå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
```


================================================================================
æ–‡ä»¶: pipeline/cluster.py
================================================================================

```python
"""
Cluster module for Reddit Pain Point Finder
å·¥ä½œæµçº§èšç±»æ¨¡å— - å‘ç°ç›¸ä¼¼çš„ç—›ç‚¹æ¨¡å¼
"""
import json
import logging
import time
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime
import numpy as np

from utils.embedding import pain_clustering
from utils.llm_client import llm_client
from utils.db import db

logger = logging.getLogger(__name__)

class PainEventClusterer:
    """ç—›ç‚¹äº‹ä»¶èšç±»å™¨"""

    def __init__(self):
        """åˆå§‹åŒ–èšç±»å™¨"""
        self.stats = {
            "total_events_processed": 0,
            "clusters_created": 0,
            "llm_validations": 0,
            "processing_time": 0.0,
            "avg_cluster_size": 0.0
        }

    def _find_similar_events(
        self,
        target_event: Dict[str, Any],
        candidate_events: List[Dict[str, Any]],
        threshold: float = 0.7
    ) -> List[Dict[str, Any]]:
        """æ‰¾åˆ°ä¸ç›®æ ‡äº‹ä»¶ç›¸ä¼¼çš„äº‹ä»¶"""
        try:
            # ä½¿ç”¨ç›¸ä¼¼åº¦æœç´¢æ‰¾åˆ°ç›¸ä¼¼äº‹ä»¶
            similar_events = pain_clustering.find_similar_events(
                target_event=target_event,
                candidate_events=candidate_events,
                threshold=threshold,
                top_k=20
            )

            return similar_events

        except Exception as e:
            logger.error(f"Failed to find similar events: {e}")
            return []

    def _validate_cluster_with_llm(
        self,
        pain_events: List[Dict[str, Any]],
        cluster_name: str = None
    ) -> Dict[str, Any]:
        """ä½¿ç”¨LLMéªŒè¯èšç±»æ˜¯å¦å±äºåŒä¸€å·¥ä½œæµ"""
        try:
            # è°ƒç”¨LLMè¿›è¡Œèšç±»éªŒè¯
            response = llm_client.cluster_pain_events(pain_events)

            validation_result = response["content"]

            # æ£€æŸ¥LLMæ˜¯å¦è®¤ä¸ºè¿™äº›äº‹ä»¶å±äºåŒä¸€å·¥ä½œæµ
            if validation_result.get("same_workflow", False):
                return {
                    "is_valid_cluster": True,
                    "cluster_name": validation_result.get("workflow_name", "Unnamed Cluster"),
                    "cluster_description": validation_result.get("workflow_description", ""),
                    "confidence": validation_result.get("confidence", 0.0),
                    "reasoning": validation_result.get("reasoning", "")
                }
            else:
                return {
                    "is_valid_cluster": False,
                    "reasoning": validation_result.get("reasoning", "Not same workflow")
                }

        except Exception as e:
            logger.error(f"Failed to validate cluster with LLM: {e}")
            return {
                "is_valid_cluster": False,
                "reasoning": f"Validation error: {e}"
            }

    def _create_cluster_summary(self, pain_events: List[Dict[str, Any]]) -> Dict[str, Any]:
        """åˆ›å»ºèšç±»æ‘˜è¦"""
        if not pain_events:
            return {}

        try:
            # ç»Ÿè®¡ä¿¡æ¯
            cluster_size = len(pain_events)
            subreddits = list(set(event.get("subreddit", "") for event in pain_events))

            # ç—›ç‚¹ç±»å‹ç»Ÿè®¡
            pain_types = []
            for event in pain_events:
                types = event.get("pain_types", [])
                if isinstance(types, list):
                    pain_types.extend(types)

            pain_type_counts = {}
            for pain_type in pain_types:
                pain_type_counts[pain_type] = pain_type_counts.get(pain_type, 0) + 1

            # ä¸»è¦ç—›ç‚¹ç±»å‹
            primary_pain_type = max(pain_type_counts.items(), key=lambda x: x[1])[0] if pain_type_counts else "general"

            # æƒ…ç»ªä¿¡å·ç»Ÿè®¡
            emotional_signals = [event.get("emotional_signal", "") for event in pain_events]
            emotion_counts = {}
            for signal in emotional_signals:
                if signal:
                    emotion_counts[signal] = emotion_counts.get(signal, 0) + 1

            # é¢‘ç‡åˆ†æ•°ç»Ÿè®¡
            frequency_scores = [event.get("frequency_score", 5) for event in pain_events]
            avg_frequency_score = np.mean(frequency_scores) if frequency_scores else 5.0

            # æåˆ°çš„å·¥å…·
            mentioned_tools = []
            for event in pain_events:
                tools = event.get("mentioned_tools", [])
                if isinstance(tools, list):
                    mentioned_tools.extend(tools)

            tool_counts = {}
            for tool in mentioned_tools:
                tool_counts[tool] = tool_counts.get(tool, 0) + 1

            # æå–ä»£è¡¨æ€§çš„é—®é¢˜
            problems = [event.get("problem", "") for event in pain_events if event.get("problem")]
            representative_problems = sorted(problems, key=len, reverse=True)[:5]

            # æå–ä»£è¡¨æ€§å·¥ä½œæ–¹å¼
            workarounds = [event.get("current_workaround", "") for event in pain_events if event.get("current_workaround")]
            representative_workarounds = [w for w in workarounds if w][:3]

            return {
                "cluster_size": cluster_size,
                "subreddits": subreddits,
                "primary_pain_type": primary_pain_type,
                "pain_type_distribution": pain_type_counts,
                "emotional_signals": emotion_counts,
                "avg_frequency_score": avg_frequency_score,
                "mentioned_tools": tool_counts,
                "representative_problems": representative_problems,
                "representative_workarounds": representative_workarounds,
                "total_pain_score": sum(event.get("post_pain_score", 0) for event in pain_events)
            }

        except Exception as e:
            logger.error(f"Failed to create cluster summary: {e}")
            return {}

    def _save_cluster_to_database(self, cluster_data: Dict[str, Any]) -> Optional[int]:
        """ä¿å­˜èšç±»åˆ°æ•°æ®åº“"""
        try:
            # å‡†å¤‡èšç±»æ•°æ®
            cluster_record = {
                "cluster_name": cluster_data["cluster_name"],
                "cluster_description": cluster_data["cluster_description"],
                "pain_event_ids": json.dumps(cluster_data["pain_event_ids"]),
                "cluster_size": cluster_data["cluster_size"],
                "avg_pain_score": cluster_data.get("avg_pain_score", 0.0),
                "workflow_confidence": cluster_data.get("workflow_confidence", 0.0)
            }

            cluster_id = db.insert_cluster(cluster_record)
            return cluster_id

        except Exception as e:
            logger.error(f"Failed to save cluster to database: {e}")
            return None

    def cluster_pain_events(self, limit: int = 200) -> Dict[str, Any]:
        """èšç±»ç—›ç‚¹äº‹ä»¶"""
        logger.info(f"Starting clustering of up to {limit} pain events")

        start_time = time.time()

        try:
            # è·å–æ‰€æœ‰æœ‰åµŒå…¥å‘é‡çš„ç—›ç‚¹äº‹ä»¶
            pain_events = db.get_all_pain_events_with_embeddings()

            if len(pain_events) < 2:
                logger.info("Not enough pain events for clustering")
                return {"clusters_created": 0, "events_processed": 0}

            # é™åˆ¶å¤„ç†æ•°é‡
            if len(pain_events) > limit:
                pain_events = pain_events[:limit]

            logger.info(f"Processing {len(pain_events)} pain events for clustering")

            # ä½¿ç”¨å‘é‡èšç±»
            vector_clusters = pain_clustering.cluster_pain_events(pain_events)

            if not vector_clusters:
                logger.info("No clusters found")
                return {"clusters_created": 0, "events_processed": len(pain_events)}

            logger.info(f"Found {len(vector_clusters)} vector clusters")

            # éªŒè¯å’Œä¼˜åŒ–èšç±»
            final_clusters = []
            validated_clusters = 0

            for i, cluster in enumerate(vector_clusters):
                logger.info(f"Validating cluster {i+1}/{len(vector_clusters)} (size: {cluster['cluster_size']})")

                # è·å–èšç±»ä¸­çš„äº‹ä»¶
                cluster_events = cluster["events"]

                # è·³è¿‡å¤ªå°çš„èšç±»
                if len(cluster_events) < 2:
                    logger.debug(f"Skipping cluster {i+1}: too small ({len(cluster_events)} events)")
                    continue

                # å¯¹äºå¤§èšç±»ï¼Œé‡‡æ ·å‰20ä¸ªäº‹ä»¶è¿›è¡ŒéªŒè¯
                events_for_validation = cluster_events
                if len(cluster_events) > 20:
                    events_for_validation = cluster_events[:20]
                    logger.info(f"Sampling first 20 events from large cluster of {len(cluster_events)} for validation")

                # ä½¿ç”¨LLMéªŒè¯èšç±»
                validation_result = self._validate_cluster_with_llm(events_for_validation)
                self.stats["llm_validations"] += 1

                if validation_result["is_valid_cluster"]:
                    # åˆ›å»ºèšç±»æ‘˜è¦
                    cluster_summary = self._create_cluster_summary(cluster_events)

                    # å‡†å¤‡æœ€ç»ˆèšç±»æ•°æ®
                    final_cluster = {
                        "cluster_name": validation_result["cluster_name"],
                        "cluster_description": validation_result["cluster_description"],
                        "pain_event_ids": [event["id"] for event in cluster_events],
                        "cluster_size": len(cluster_events),
                        "workflow_confidence": validation_result["confidence"],
                        "cluster_summary": cluster_summary,
                        "validation_reasoning": validation_result["reasoning"]
                    }

                    # ä¿å­˜åˆ°æ•°æ®åº“
                    cluster_id = self._save_cluster_to_database(final_cluster)
                    if cluster_id:
                        final_cluster["cluster_id"] = cluster_id
                        final_clusters.append(final_cluster)
                        validated_clusters += 1

                        logger.info(f"Saved cluster: {validation_result['cluster_name']} ({len(cluster_events)} events)")
                else:
                    logger.warning(f"Cluster {i+1} rejected by LLM: {validation_result['reasoning']}")

                # æ·»åŠ å»¶è¿Ÿé¿å…APIé™åˆ¶
                time.sleep(1)

            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            processing_time = time.time() - start_time
            self.stats["total_events_processed"] = len(pain_events)
            self.stats["clusters_created"] = validated_clusters
            self.stats["processing_time"] = processing_time

            if validated_clusters > 0:
                self.stats["avg_cluster_size"] = sum(len(cluster["pain_event_ids"]) for cluster in final_clusters) / validated_clusters

            logger.info(f"""
=== Clustering Summary ===
Pain events processed: {len(pain_events)}
Vector clusters found: {len(vector_clusters)}
Validated clusters created: {validated_clusters}
Average cluster size: {self.stats['avg_cluster_size']:.1f}
Processing time: {processing_time:.2f}s
""")

            return {
                "clusters_created": validated_clusters,
                "events_processed": len(pain_events),
                "vector_clusters": len(vector_clusters),
                "final_clusters": final_clusters,
                "clustering_stats": self.get_statistics()
            }

        except Exception as e:
            logger.error(f"Failed to cluster pain events: {e}")
            raise

    def get_cluster_analysis(self, cluster_id: int) -> Optional[Dict[str, Any]]:
        """è·å–èšç±»è¯¦ç»†åˆ†æ"""
        try:
            # ä»æ•°æ®åº“è·å–èšç±»ä¿¡æ¯
            with db.get_connection("clusters") as conn:
                cursor = conn.execute("""
                    SELECT * FROM clusters WHERE id = ?
                """, (cluster_id,))
                cluster_data = cursor.fetchone()

            if not cluster_data:
                return None

            cluster_info = dict(cluster_data)

            # è·å–èšç±»ä¸­çš„ç—›ç‚¹äº‹ä»¶
            pain_event_ids = json.loads(cluster_info["pain_event_ids"])
            pain_events = []

            with db.get_connection("pain") as conn:
                for event_id in pain_event_ids:
                    cursor = conn.execute("""
                        SELECT * FROM pain_events WHERE id = ?
                    """, (event_id,))
                    event_data = cursor.fetchone()
                    if event_data:
                        pain_events.append(dict(event_data))

            cluster_info["pain_events"] = pain_events

            # é‡æ–°è®¡ç®—èšç±»æ‘˜è¦
            cluster_summary = self._create_cluster_summary(pain_events)
            cluster_info["cluster_summary"] = cluster_summary

            return cluster_info

        except Exception as e:
            logger.error(f"Failed to get cluster analysis: {e}")
            return None

    def get_all_clusters_summary(self) -> List[Dict[str, Any]]:
        """è·å–æ‰€æœ‰èšç±»çš„æ‘˜è¦"""
        try:
            with db.get_connection("clusters") as conn:
                cursor = conn.execute("""
                    SELECT id, cluster_name, cluster_size, avg_pain_score, workflow_confidence, created_at
                    FROM clusters
                    ORDER BY cluster_size DESC, workflow_confidence DESC
                """)
                clusters = [dict(row) for row in cursor.fetchall()]

            return clusters

        except Exception as e:
            logger.error(f"Failed to get clusters summary: {e}")
            return []

    def get_statistics(self) -> Dict[str, Any]:
        """è·å–èšç±»ç»Ÿè®¡ä¿¡æ¯"""
        stats = self.stats.copy()

        if stats["total_events_processed"] > 0:
            stats["clustering_rate"] = stats["clusters_created"] / stats["total_events_processed"]
            stats["processing_rate"] = stats["total_events_processed"] / max(stats["processing_time"], 1)
        else:
            stats["clustering_rate"] = 0
            stats["processing_rate"] = 0

        # æ·»åŠ åµŒå…¥å®¢æˆ·ç«¯ç»Ÿè®¡
        embedding_stats = pain_clustering.embedding_client.get_embedding_statistics()
        stats["embedding_stats"] = embedding_stats

        return stats

    def reset_statistics(self):
        """é‡ç½®ç»Ÿè®¡ä¿¡æ¯"""
        self.stats = {
            "total_events_processed": 0,
            "clusters_created": 0,
            "llm_validations": 0,
            "processing_time": 0.0,
            "avg_cluster_size": 0.0
        }

def main():
    """ä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description="Cluster pain events into workflow groups")
    parser.add_argument("--limit", type=int, default=200, help="Limit number of pain events to process")
    parser.add_argument("--analyze", type=int, help="Analyze specific cluster ID")
    parser.add_argument("--list", action="store_true", help="List all clusters summary")
    args = parser.parse_args()

    try:
        logger.info("Starting pain event clustering...")

        clusterer = PainEventClusterer()

        if args.analyze:
            # åˆ†æç‰¹å®šèšç±»
            cluster_analysis = clusterer.get_cluster_analysis(args.analyze)
            if cluster_analysis:
                print(json.dumps(cluster_analysis, indent=2))
            else:
                logger.error(f"Cluster {args.analyze} not found")

        elif args.list:
            # åˆ—å‡ºæ‰€æœ‰èšç±»
            clusters_summary = clusterer.get_all_clusters_summary()
            print(json.dumps(clusters_summary, indent=2))

        else:
            # æ‰§è¡Œèšç±»
            result = clusterer.cluster_pain_events(limit=args.limit)

            logger.info(f"""
=== Clustering Complete ===
Clusters created: {result['clusters_created']}
Events processed: {result['events_processed']}
Clustering stats: {result['clustering_stats']}
""")

    except Exception as e:
        logger.error(f"Fatal error: {e}")
        raise

if __name__ == "__main__":
    main()
```


================================================================================
æ–‡ä»¶: pipeline/embed.py
================================================================================

```python
"""
Embed module for Reddit Pain Point Finder
ç—›ç‚¹äº‹ä»¶å‘é‡åŒ–æ¨¡å— - ä¸ºèšç±»åšå‡†å¤‡
"""
import logging
import time
from typing import List, Dict, Any, Optional
from datetime import datetime

from utils.embedding import embedding_client
from utils.db import db

logger = logging.getLogger(__name__)

class PainEventEmbedder:
    """ç—›ç‚¹äº‹ä»¶å‘é‡åŒ–å™¨"""

    def __init__(self):
        """åˆå§‹åŒ–å‘é‡åŒ–å™¨"""
        self.stats = {
            "total_processed": 0,
            "embeddings_created": 0,
            "errors": 0,
            "processing_time": 0.0,
            "cache_hits": 0
        }

    def _create_embedding_text(self, pain_event: Dict[str, Any]) -> str:
        """åˆ›å»ºç”¨äºåµŒå…¥çš„æ–‡æœ¬"""
        text_parts = []

        # æ ¸å¿ƒè¦ç´ 
        if pain_event.get("actor"):
            text_parts.append(pain_event["actor"])

        if pain_event.get("context"):
            text_parts.append(pain_event["context"])

        if pain_event.get("problem"):
            text_parts.append(pain_event["problem"])

        if pain_event.get("current_workaround"):
            text_parts.append(pain_event["current_workaround"])

        # ç”¨è¿æ¥ç¬¦ä¿æŒè¯­ä¹‰ç»“æ„
        embedding_text = " | ".join(text_parts)

        # æ£€æŸ¥æ–‡æœ¬é•¿åº¦
        if len(embedding_text) > 2000:
            logger.warning(f"Embedding text too long ({len(embedding_text)} chars), truncating")
            # ä¼˜å…ˆä¿ç•™problemå’Œcontext
            core_text = f"{pain_event.get('context', '')} | {pain_event.get('problem', '')}"
            if len(core_text) > 1000:
                # è¿›ä¸€æ­¥æˆªæ–­
                embedding_text = core_text[:1000]
            else:
                embedding_text = core_text

        return embedding_text

    def embed_single_event(self, pain_event: Dict[str, Any]) -> Optional[List[float]]:
        """ä¸ºå•ä¸ªç—›ç‚¹äº‹ä»¶åˆ›å»ºåµŒå…¥å‘é‡"""
        try:
            # åˆ›å»ºåµŒå…¥æ–‡æœ¬
            embedding_text = self._create_embedding_text(pain_event)

            if not embedding_text:
                logger.warning(f"Empty embedding text for pain event {pain_event.get('id')}")
                return None

            # åˆ›å»ºåµŒå…¥å‘é‡
            embedding = embedding_client.create_embedding(embedding_text)

            self.stats["embeddings_created"] += 1
            return embedding

        except Exception as e:
            logger.error(f"Failed to create embedding for pain event {pain_event.get('id')}: {e}")
            self.stats["errors"] += 1
            return None

    def save_embedding(self, pain_event_id: int, embedding: List[float]) -> bool:
        """ä¿å­˜åµŒå…¥å‘é‡åˆ°æ•°æ®åº“"""
        try:
            success = db.insert_pain_embedding(
                pain_event_id=pain_event_id,
                embedding_vector=embedding,
                model_name=embedding_client.model_name
            )
            return success

        except Exception as e:
            logger.error(f"Failed to save embedding for pain event {pain_event_id}: {e}")
            return False

    def process_pain_events_batch(self, pain_events: List[Dict[str, Any]], batch_size: int = 20) -> int:
        """æ‰¹é‡å¤„ç†ç—›ç‚¹äº‹ä»¶çš„å‘é‡åŒ–"""
        logger.info(f"Creating embeddings for {len(pain_events)} pain events")

        start_time = time.time()
        saved_count = 0

        for i, event in enumerate(pain_events):
            if i % 10 == 0:
                logger.info(f"Processed {i}/{len(pain_events)} pain events")

            # åˆ›å»ºåµŒå…¥å‘é‡
            embedding = self.embed_single_event(event)
            if embedding is None:
                continue

            # ä¿å­˜åˆ°æ•°æ®åº“
            if self.save_embedding(event["id"], embedding):
                saved_count += 1

            # æ‰¹é‡å¤„ç†å»¶è¿Ÿ
            if i % batch_size == 0 and i > 0:
                time.sleep(1)  # é¿å…APIé™åˆ¶

        # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        processing_time = time.time() - start_time
        self.stats["total_processed"] = len(pain_events)
        self.stats["processing_time"] = processing_time

        # æ·»åŠ åµŒå…¥å®¢æˆ·ç«¯ç»Ÿè®¡
        embedding_stats = embedding_client.get_embedding_statistics()
        self.stats["cache_hits"] = embedding_stats.get("cache_hits", 0)

        logger.info(f"Embedding complete: {saved_count}/{len(pain_events)} embeddings saved")
        logger.info(f"Processing time: {processing_time:.2f}s")

        return saved_count

    def process_missing_embeddings(self, limit: int = 100) -> Dict[str, Any]:
        """å¤„ç†ç¼ºå¤±åµŒå…¥å‘é‡çš„ç—›ç‚¹äº‹ä»¶"""
        logger.info(f"Processing up to {limit} pain events without embeddings")

        try:
            # è·å–æ²¡æœ‰åµŒå…¥å‘é‡çš„ç—›ç‚¹äº‹ä»¶
            pain_events = db.get_pain_events_without_embeddings(limit=limit)

            if not pain_events:
                logger.info("No pain events without embeddings found")
                return {"processed": 0, "embeddings_created": 0}

            logger.info(f"Found {len(pain_events)} pain events to embed")

            # æ‰¹é‡åˆ›å»ºåµŒå…¥å‘é‡
            saved_count = self.process_pain_events_batch(pain_events)

            return {
                "processed": len(pain_events),
                "embeddings_created": saved_count,
                "embedding_stats": self.get_statistics()
            }

        except Exception as e:
            logger.error(f"Failed to process missing embeddings: {e}")
            raise

    def get_embedding_statistics(self) -> Dict[str, Any]:
        """è·å–å‘é‡åŒ–ç»Ÿè®¡ä¿¡æ¯"""
        stats = self.stats.copy()

        if stats["total_processed"] > 0:
            stats["success_rate"] = stats["embeddings_created"] / stats["total_processed"]
            stats["processing_rate"] = stats["total_processed"] / max(stats["processing_time"], 1)
        else:
            stats["success_rate"] = 0
            stats["processing_rate"] = 0

        # æ·»åŠ åµŒå…¥å®¢æˆ·ç«¯ç»Ÿè®¡
        embedding_stats = embedding_client.get_embedding_statistics()
        stats["embedding_client_stats"] = embedding_stats

        return stats

    def reset_statistics(self):
        """é‡ç½®ç»Ÿè®¡ä¿¡æ¯"""
        self.stats = {
            "total_processed": 0,
            "embeddings_created": 0,
            "errors": 0,
            "processing_time": 0.0,
            "cache_hits": 0
        }

    def verify_embeddings(self, limit: int = 50) -> Dict[str, Any]:
        """éªŒè¯åµŒå…¥å‘é‡çš„è´¨é‡"""
        logger.info(f"Verifying {limit} embeddings")

        try:
            # è·å–æ‰€æœ‰æœ‰åµŒå…¥å‘é‡çš„ç—›ç‚¹äº‹ä»¶
            pain_events = db.get_all_pain_events_with_embeddings()

            if len(pain_events) > limit:
                pain_events = pain_events[:limit]

            if not pain_events:
                return {"verified": 0, "issues": []}

            issues = []
            verified_count = 0

            for event in pain_events:
                try:
                    embedding = event.get("embedding_vector")
                    if not embedding:
                        issues.append(f"Event {event['id']}: Missing embedding vector")
                        continue

                    # æ£€æŸ¥ç»´åº¦
                    if len(embedding) == 0:
                        issues.append(f"Event {event['id']}: Empty embedding vector")
                        continue

                    # æ£€æŸ¥æ˜¯å¦åŒ…å«æœ‰æ•ˆæ•°å€¼
                    if not all(isinstance(x, (int, float)) for x in embedding):
                        issues.append(f"Event {event['id']}: Invalid embedding data types")
                        continue

                    # æ£€æŸ¥æ˜¯å¦å…¨ä¸ºé›¶ï¼ˆå¼‚å¸¸ï¼‰
                    if all(abs(x) < 1e-6 for x in embedding):
                        issues.append(f"Event {event['id']}: All-zero embedding vector")
                        continue

                    verified_count += 1

                except Exception as e:
                    issues.append(f"Event {event.get('id', 'unknown')}: Verification error - {e}")

            logger.info(f"Embedding verification complete: {verified_count}/{len(pain_events)} passed")

            return {
                "verified": verified_count,
                "total": len(pain_events),
                "issues": issues
            }

        except Exception as e:
            logger.error(f"Failed to verify embeddings: {e}")
            return {"verified": 0, "issues": [f"Verification failed: {e}"]}

    def get_statistics(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        stats = self.stats.copy()
        if stats["total_processed"] > 0:
            stats["success_rate"] = stats["embeddings_created"] / stats["total_processed"]
        else:
            stats["success_rate"] = 0.0
        return stats

def main():
    """ä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description="Create embeddings for pain events")
    parser.add_argument("--limit", type=int, default=100, help="Limit number of pain events to process")
    parser.add_argument("--verify", action="store_true", help="Verify existing embeddings")
    parser.add_argument("--batch-size", type=int, default=20, help="Batch size for processing")
    args = parser.parse_args()

    try:
        logger.info("Starting pain event embedding...")

        embedder = PainEventEmbedder()

        if args.verify:
            # éªŒè¯ç°æœ‰åµŒå…¥
            result = embedder.verify_embeddings(limit=args.limit)
            logger.info(f"Verification result: {result}")
        else:
            # å¤„ç†ç¼ºå¤±çš„åµŒå…¥
            result = embedder.process_missing_embeddings(limit=args.limit)

            logger.info(f"""
=== Embedding Summary ===
Pain events processed: {result['processed']}
Embeddings created: {result['embeddings_created']}
Embedding stats: {result['embedding_stats']}
""")

    except Exception as e:
        logger.error(f"Fatal error: {e}")
        raise

if __name__ == "__main__":
    main()
```


================================================================================
æ–‡ä»¶: pipeline/extract_pain.py
================================================================================

```python
"""
Extract Pain module for Reddit Pain Point Finder
ç—›ç‚¹äº‹ä»¶æŠ½å–æ¨¡å— - ä½¿ç”¨LLMè¿›è¡Œç»“æ„åŒ–æŠ½å–
"""
import json
import logging
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime
import time

from utils.llm_client import llm_client
from utils.db import db

logger = logging.getLogger(__name__)

class PainPointExtractor:
    """ç—›ç‚¹äº‹ä»¶æŠ½å–å™¨"""

    def __init__(self):
        """åˆå§‹åŒ–æŠ½å–å™¨"""
        self.stats = {
            "total_processed": 0,
            "total_pain_events": 0,
            "extraction_errors": 0,
            "avg_confidence": 0.0,
            "processing_time": 0.0
        }

    def _extract_from_single_post(self, post_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """ä»å•ä¸ªå¸–å­æŠ½å–ç—›ç‚¹äº‹ä»¶"""
        try:
            title = post_data.get("title", "")
            body = post_data.get("body", "")
            subreddit = post_data.get("subreddit", "")
            upvotes = post_data.get("score", 0)
            comments_count = post_data.get("num_comments", 0)

            # è°ƒç”¨LLMè¿›è¡ŒæŠ½å–
            response = llm_client.extract_pain_points(
                title=title,
                body=body,
                subreddit=subreddit,
                upvotes=upvotes,
                comments_count=comments_count
            )

            extraction_result = response["content"]
            pain_events = extraction_result.get("pain_events", [])

            # ä¸ºæ¯ä¸ªç—›ç‚¹äº‹ä»¶æ·»åŠ å…ƒæ•°æ®
            for event in pain_events:
                event.update({
                    "post_id": post_data["id"],
                    "subreddit": subreddit,
                    "original_score": upvotes,
                    "extraction_model": response["model"],
                    "extraction_timestamp": datetime.now().isoformat(),
                    "confidence": event.get("confidence", 0.0)
                })

            self.stats["total_pain_events"] += len(pain_events)
            logger.debug(f"Extracted {len(pain_events)} pain events from post {post_data['id']}")

            return pain_events

        except Exception as e:
            logger.error(f"Failed to extract pain from post {post_data.get('id')}: {e}")
            self.stats["extraction_errors"] += 1
            return []

    def _validate_pain_event(self, pain_event: Dict[str, Any]) -> bool:
        """éªŒè¯ç—›ç‚¹äº‹ä»¶çš„è´¨é‡"""
        try:
            # æ£€æŸ¥å¿…éœ€å­—æ®µ
            required_fields = ["problem", "post_id"]
            for field in required_fields:
                if not pain_event.get(field):
                    logger.warning(f"Missing required field '{field}' in pain event")
                    return False

            # æ£€æŸ¥é—®é¢˜æè¿°é•¿åº¦
            problem = pain_event.get("problem", "")
            if len(problem) < 10:
                logger.warning(f"Problem description too short: {problem}")
                return False

            if len(problem) > 1000:
                logger.warning(f"Problem description too long: {len(problem)} characters")
                return False

            # æ£€æŸ¥ç½®ä¿¡åº¦
            confidence = pain_event.get("confidence", 0.0)
            if confidence < 0.3:
                logger.warning(f"Low confidence pain event: {confidence}")
                return False

            # æ£€æŸ¥æ˜¯å¦è¿‡äºæ³›æ³›
            generic_problems = [
                "it's slow", "it's bad", "it doesn't work", "it's broken",
                "i don't like it", "it's annoying", "it's frustrating"
            ]
            problem_lower = problem.lower()
            for generic in generic_problems:
                if problem_lower == generic:
                    logger.warning(f"Too generic problem: {problem}")
                    return False

            return True

        except Exception as e:
            logger.error(f"Error validating pain event: {e}")
            return False

    def _enhance_pain_event(self, pain_event: Dict[str, Any], post_data: Dict[str, Any]) -> Dict[str, Any]:
        """å¢å¼ºç—›ç‚¹äº‹ä»¶ä¿¡æ¯"""
        try:
            enhanced = pain_event.copy()

            # æ·»åŠ å¸–å­ä¸Šä¸‹æ–‡
            enhanced.update({
                "post_title": post_data.get("title", ""),
                "post_category": post_data.get("category", ""),
                "post_pain_score": post_data.get("pain_score", 0.0),
                "post_comments": post_data.get("num_comments", 0)
            })

            # åˆ†æç—›ç‚¹ç±»å‹
            problem_text = enhanced.get("problem", "").lower()
            context_text = enhanced.get("context", "").lower()
            full_text = f"{problem_text} {context_text}"

            # ç—›ç‚¹ç±»å‹åˆ†ç±»
            pain_types = {
                "workflow": ["workflow", "process", "flow", "pipeline", "automation"],
                "technical": ["code", "programming", "development", "technical", "bug"],
                "efficiency": ["slow", "time", "inefficient", "productivity", "performance"],
                "complexity": ["complex", "complicated", "difficult", "hard", "confusing"],
                "integration": ["integration", "connect", "api", "compatibility", "sync"],
                "cost": ["expensive", "cost", "price", "pricing", "budget"],
                "user_experience": ["ui", "ux", "interface", "usability", "experience"],
                "data": ["data", "database", "storage", "backup", "analysis"]
            }

            detected_types = []
            for pain_type, keywords in pain_types.items():
                if any(keyword in full_text for keyword in keywords):
                    detected_types.append(pain_type)

            enhanced["pain_types"] = detected_types
            enhanced["primary_pain_type"] = detected_types[0] if detected_types else "general"

            # æå–æåˆ°çš„å·¥å…·
            mentioned_tools = enhanced.get("mentioned_tools", [])
            if not isinstance(mentioned_tools, list):
                mentioned_tools = []

            # ä»æ–‡æœ¬ä¸­æå–æ›´å¤šå·¥å…·åï¼ˆç®€å•è§„åˆ™ï¼‰
            common_tools = [
                "excel", "google sheets", "slack", "discord", "jira", "trello", "asana",
                "github", "gitlab", "vscode", "intellij", "docker", "kubernetes", "aws",
                "azure", "gcp", "mysql", "postgresql", "mongodb", "redis", "figma",
                "sketch", "photoshop", "wordpress", "shopify", "salesforce"
            ]

            for tool in common_tools:
                if tool in full_text and tool not in mentioned_tools:
                    mentioned_tools.append(tool)

            enhanced["mentioned_tools"] = mentioned_tools

            # åˆ†æé¢‘ç‡
            frequency = enhanced.get("frequency", "").lower()
            if "daily" in frequency or "every day" in frequency:
                enhanced["frequency_score"] = 10
            elif "weekly" in frequency or "every week" in frequency:
                enhanced["frequency_score"] = 8
            elif "monthly" in frequency or "every month" in frequency:
                enhanced["frequency_score"] = 6
            elif "often" in frequency or "frequent" in frequency:
                enhanced["frequency_score"] = 7
            elif "sometimes" in frequency or "occasional" in frequency:
                enhanced["frequency_score"] = 4
            elif "rarely" in frequency:
                enhanced["frequency_score"] = 2
            else:
                enhanced["frequency_score"] = 5  # é»˜è®¤ä¸­ç­‰é¢‘ç‡

            return enhanced

        except Exception as e:
            logger.error(f"Error enhancing pain event: {e}")
            return pain_event

    def extract_from_posts_batch(self, posts: List[Dict[str, Any]], batch_size: int = 10) -> List[Dict[str, Any]]:
        """æ‰¹é‡ä»å¸–å­ä¸­æŠ½å–ç—›ç‚¹äº‹ä»¶"""
        logger.info(f"Extracting pain points from {len(posts)} posts")

        all_pain_events = []
        start_time = time.time()

        for i, post in enumerate(posts):
            if i % 10 == 0:
                logger.info(f"Processed {i}/{len(posts)} posts")

            # æŠ½å–ç—›ç‚¹äº‹ä»¶
            pain_events = self._extract_from_single_post(post)

            # éªŒè¯å’Œå¢å¼ºæ¯ä¸ªç—›ç‚¹äº‹ä»¶
            for event in pain_events:
                if self._validate_pain_event(event):
                    enhanced_event = self._enhance_pain_event(event, post)
                    all_pain_events.append(enhanced_event)

            # æ·»åŠ å»¶è¿Ÿé¿å…APIé™åˆ¶
            time.sleep(0.5)

        # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        processing_time = time.time() - start_time
        self.stats["total_processed"] = len(posts)
        self.stats["processing_time"] = processing_time

        if all_pain_events:
            avg_confidence = sum(event.get("confidence", 0) for event in all_pain_events) / len(all_pain_events)
            self.stats["avg_confidence"] = avg_confidence

        logger.info(f"Extraction complete: {len(all_pain_events)} pain events from {len(posts)} posts")
        logger.info(f"Processing time: {processing_time:.2f}s, Avg per post: {processing_time/len(posts):.2f}s")

        return all_pain_events

    def save_pain_events(self, pain_events: List[Dict[str, Any]]) -> int:
        """ä¿å­˜ç—›ç‚¹äº‹ä»¶åˆ°æ•°æ®åº“"""
        saved_count = 0

        for event in pain_events:
            try:
                # å‡†å¤‡æ•°æ®åº“è®°å½•
                event_data = {
                    "post_id": event["post_id"],
                    "actor": event.get("actor", ""),
                    "context": event.get("context", ""),
                    "problem": event["problem"],
                    "current_workaround": event.get("current_workaround", ""),
                    "frequency": event.get("frequency", ""),
                    "emotional_signal": event.get("emotional_signal", ""),
                    "mentioned_tools": event.get("mentioned_tools", []),
                    "extraction_confidence": event.get("confidence", 0.0)
                }

                # ä¿å­˜åˆ°æ•°æ®åº“
                pain_event_id = db.insert_pain_event(event_data)
                if pain_event_id:
                    saved_count += 1
                    logger.debug(f"Saved pain event {pain_event_id}: {event['problem'][:50]}...")

            except Exception as e:
                logger.error(f"Failed to save pain event: {e}")

        logger.info(f"Saved {saved_count}/{len(pain_events)} pain events to database")
        return saved_count

    def process_unextracted_posts(self, limit: int = 100) -> Dict[str, Any]:
        """å¤„ç†æœªæŠ½å–çš„å¸–å­"""
        logger.info(f"Processing up to {limit} unextracted posts")

        try:
            # è·å–æœªå¤„ç†çš„è¿‡æ»¤å¸–å­
            unextracted_posts = db.get_filtered_posts(limit=limit, min_pain_score=0.3)

            if not unextracted_posts:
                logger.info("No unextracted posts found")
                return {"processed": 0, "pain_events": 0}

            logger.info(f"Found {len(unextracted_posts)} posts to extract from")

            # æŠ½å–ç—›ç‚¹äº‹ä»¶
            pain_events = self.extract_from_posts_batch(unextracted_posts)

            # ä¿å­˜åˆ°æ•°æ®åº“
            saved_count = self.save_pain_events(pain_events)

            return {
                "processed": len(unextracted_posts),
                "pain_events_extracted": len(pain_events),
                "pain_events_saved": saved_count,
                "extraction_stats": self.get_statistics()
            }

        except Exception as e:
            logger.error(f"Failed to process unextracted posts: {e}")
            raise

    def get_statistics(self) -> Dict[str, Any]:
        """è·å–æŠ½å–ç»Ÿè®¡ä¿¡æ¯"""
        stats = self.stats.copy()
        if stats["total_processed"] > 0:
            stats["avg_events_per_post"] = stats["total_pain_events"] / stats["total_processed"]
            stats["processing_rate"] = stats["total_processed"] / max(stats["processing_time"], 1)
        else:
            stats["avg_events_per_post"] = 0
            stats["processing_rate"] = 0

        # æ·»åŠ LLMå®¢æˆ·ç«¯ç»Ÿè®¡
        llm_stats = llm_client.get_statistics()
        stats["llm_stats"] = llm_stats

        return stats

    def reset_statistics(self):
        """é‡ç½®ç»Ÿè®¡ä¿¡æ¯"""
        self.stats = {
            "total_processed": 0,
            "total_pain_events": 0,
            "extraction_errors": 0,
            "avg_confidence": 0.0,
            "processing_time": 0.0
        }

def main():
    """ä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description="Extract pain points from filtered Reddit posts")
    parser.add_argument("--limit", type=int, default=100, help="Limit number of posts to process")
    parser.add_argument("--min-score", type=float, default=0.3, help="Minimum pain score threshold")
    parser.add_argument("--batch-size", type=int, default=10, help="Batch size for processing")
    args = parser.parse_args()

    try:
        logger.info("Starting pain point extraction...")

        extractor = PainPointExtractor()

        # å¤„ç†æœªæŠ½å–çš„å¸–å­
        result = extractor.process_unextracted_posts(limit=args.limit)

        logger.info(f"""
=== Extraction Summary ===
Posts processed: {result['processed']}
Pain events extracted: {result['pain_events_extracted']}
Pain events saved: {result['pain_events_saved']}
Extraction stats: {result['extraction_stats']}
""")

    except Exception as e:
        logger.error(f"Fatal error: {e}")
        raise

if __name__ == "__main__":
    main()
```


================================================================================
æ–‡ä»¶: pipeline/fetch.py
================================================================================

```python
"""
Fetch module for Reddit Pain Point Finder
åŸºäºåŸæœ‰reddit_collection.pyä¼˜åŒ–çš„Redditæ•°æ®æŠ“å–æ¨¡å—
"""
import os
import json
import sys
import time
import logging
import praw
import yaml
from typing import List, Dict, Any, Optional
from datetime import datetime, timedelta

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# å¯¼å…¥å·¥å…·æ¨¡å—
try:
    from utils.db import db
except ImportError:
    logger.warning("Could not import db utility, will use local file storage")

class RedditPainFetcher:
    """Redditç—›ç‚¹æ•°æ®æŠ“å–å™¨"""

    def __init__(self, config_path: str = "config/subreddits.yaml"):
        """åˆå§‹åŒ–æŠ“å–å™¨"""
        self.config = self._load_config(config_path)
        self.reddit_client = self._init_reddit_client()
        self.processed_posts = set()
        self.stats = {
            "total_fetched": 0,
            "total_saved": 0,
            "filtered_out": 0,
            "errors": 0,
            "start_time": None
        }

    def _load_config(self, config_path: str) -> Dict[str, Any]:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                return yaml.safe_load(f)
        except Exception as e:
            logger.error(f"Failed to load config from {config_path}: {e}")
            raise

    def _init_reddit_client(self) -> praw.Reddit:
        """åˆå§‹åŒ–Redditå®¢æˆ·ç«¯"""
        try:
            # ä»ç¯å¢ƒå˜é‡è·å–APIå‡­è¯
            client_id = os.getenv('REDDIT_CLIENT_ID')
            client_secret = os.getenv('REDDIT_CLIENT_SECRET')

            if not client_id or not client_secret:
                raise ValueError("Reddit API credentials not found in environment variables")

            reddit = praw.Reddit(
                client_id=client_id,
                client_secret=client_secret,
                user_agent="python:PainPointFinder:v1.0",
                read_only=True
            )

            # æµ‹è¯•è®¤è¯
            test_subreddit = reddit.subreddit('test')
            test_subreddit.display_name
            logger.info("Reddit authentication successful")

            return reddit

        except Exception as e:
            logger.error(f"Failed to initialize Reddit client: {e}")
            raise

    def _load_processed_posts(self):
        """åŠ è½½å·²å¤„ç†çš„å¸–å­ID"""
        try:
            # å°è¯•ä»æ•°æ®åº“åŠ è½½
            if 'db' in globals():
                # ä»æ•°æ®åº“è·å–å·²å¤„ç†çš„å¸–å­ID
                with db.get_connection("raw") as conn:
                    cursor = conn.execute("SELECT id FROM posts")
                    self.processed_posts = {row[0] for row in cursor.fetchall()}
            else:
                # ä»æ–‡ä»¶åŠ è½½ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰
                processed_file = "data/processed_posts.json"
                if os.path.exists(processed_file):
                    with open(processed_file, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                        self.processed_posts = set(data.get("processed_ids", []))

            logger.info(f"Loaded {len(self.processed_posts)} previously processed post IDs")

        except Exception as e:
            logger.error(f"Failed to load processed posts: {e}")
            self.processed_posts = set()

    def _save_processed_posts(self):
        """ä¿å­˜å·²å¤„ç†çš„å¸–å­ID"""
        try:
            if 'db' in globals():
                # æ•°æ®åº“æ¨¡å¼ä¸éœ€è¦é¢å¤–ä¿å­˜ï¼Œå› ä¸ºæ¯æ¡è®°å½•éƒ½å•ç‹¬å­˜å‚¨
                pass
            else:
                # ä¿å­˜åˆ°æ–‡ä»¶ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰
                processed_file = "data/processed_posts.json"
                os.makedirs(os.path.dirname(processed_file), exist_ok=True)
                with open(processed_file, 'w', encoding='utf-8') as f:
                    json.dump({
                        "processed_ids": list(self.processed_posts),
                        "last_updated": datetime.now().isoformat()
                    }, f, indent=2)

        except Exception as e:
            logger.error(f"Failed to save processed posts: {e}")

    def _build_search_query(self, subreddit_config: Dict[str, Any]) -> str:
        """æ„å»ºæœç´¢æŸ¥è¯¢"""
        search_focus = subreddit_config.get("search_focus", [])
        pain_keywords = self.config.get("pain_keywords", {})

        query_parts = []
        for category in search_focus:
            if category in pain_keywords:
                # ä½¿ç”¨å¼•å·ç¡®ä¿ç²¾ç¡®åŒ¹é…
                category_keywords = pain_keywords[category][:5]  # é™åˆ¶å…³é”®è¯æ•°é‡
                quoted_keywords = [f'"{kw}"' for kw in category_keywords]
                query_parts.append(f"({' OR '.join(quoted_keywords)})")

        return " OR ".join(query_parts) if query_parts else ""

    def _calculate_pain_score(self, submission, subreddit_config: Dict[str, Any]) -> float:
        """è®¡ç®—å¸–å­ç—›ç‚¹åˆ†æ•°"""
        score = 0.0

        # 1. è´¨é‡åŸºç¡€åˆ†
        thresholds = subreddit_config.get("thresholds", {})
        min_upvotes = thresholds.get("min_upvotes", 5)
        min_comments = thresholds.get("min_comments", 3)

        if submission.score >= min_upvotes:
            score += 0.3
        if submission.num_comments >= min_comments:
            score += 0.2

        # 2. ç—›ç‚¹å…³é”®è¯åŒ¹é…
        title = (submission.title or "").lower()
        body = (submission.selftext or "").lower()
        full_text = f"{title} {body}"

        pain_keywords = self.config.get("pain_keywords", {})
        keyword_matches = 0

        for category_keywords in pain_keywords.values():
            for keyword in category_keywords:
                if keyword.lower() in full_text:
                    keyword_matches += 1
                    score += 0.1

        # 3. é•¿åº¦åˆ†æï¼ˆæ›´é•¿çš„å¸–å­å¯èƒ½åŒ…å«æ›´å¤šç—›ç‚¹ç»†èŠ‚ï¼‰
        if len(full_text) > 200:
            score += 0.1
        if len(full_text) > 500:
            score += 0.1

        # 4. æƒ…ç»ªä¿¡å·æ£€æµ‹ï¼ˆç®€å•è§„åˆ™ï¼‰
        emotion_indicators = ["frustrated", "annoying", "struggling", "can't", "doesn't work", "broken"]
        emotion_count = sum(1 for indicator in emotion_indicators if indicator in full_text)
        score += emotion_count * 0.05

        return min(score, 1.0)  # é™åˆ¶åœ¨0-1èŒƒå›´å†…

    def _is_pain_post(self, submission, subreddit_config: Dict[str, Any]) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºç—›ç‚¹å¸–å­"""
        # 1. åŸºç¡€è´¨é‡æ£€æŸ¥
        if submission.score < subreddit_config.get("min_upvotes", 5):
            return False
        if submission.num_comments < subreddit_config.get("min_comments", 3):
            return False

        # 2. ç—›ç‚¹å…³é”®è¯æ£€æŸ¥
        title = (submission.title or "").lower()
        body = (submission.selftext or "").lower()
        full_text = f"{title} {body}"

        pain_keywords = self.config.get("pain_keywords", {})
        keyword_matches = 0

        for category_keywords in pain_keywords.values():
            for keyword in category_keywords:
                if keyword.lower() in full_text:
                    keyword_matches += 1
                    if keyword_matches >= 1:  # è‡³å°‘åŒ¹é…ä¸€ä¸ªå…³é”®è¯
                        return True

        # 3. æ’é™¤æ¨¡å¼æ£€æŸ¥
        exclude_patterns = self.config.get("exclude_patterns", {})
        for pattern_category, patterns in exclude_patterns.items():
            for pattern in patterns:
                if pattern.lower() in full_text:
                    logger.debug(f"Excluded post due to {pattern_category}: {pattern}")
                    return False

        return False

    def _extract_post_data(self, submission, subreddit_config: Dict[str, Any]) -> Dict[str, Any]:
        """æå–å¸–å­æ•°æ®"""
        try:
            # è·å–è¯„è®º
            comments = []
            try:
                submission.comment_sort = "top"
                submission.comments.replace_more(limit=0)
                for comment in submission.comments.list()[:20]:  # è·å–å‰20æ¡è¯„è®º
                    if hasattr(comment, 'author') and comment.author:
                        comments.append({
                            "author": comment.author.name,
                            "body": comment.body,
                            "score": comment.score
                        })
            except Exception as e:
                logger.warning(f"Failed to fetch comments for {submission.id}: {e}")

            # è®¡ç®—ç—›ç‚¹åˆ†æ•°
            pain_score = self._calculate_pain_score(submission, subreddit_config)

            return {
                "id": submission.id,
                "title": submission.title,
                "body": submission.selftext,
                "subreddit": subreddit_config["name"],
                "category": subreddit_config["category"],
                "url": submission.url,
                "score": submission.score,
                "num_comments": submission.num_comments,
                "upvote_ratio": getattr(submission, 'upvote_ratio', 0.0),
                "is_self": getattr(submission, 'is_self', False),
                "created_utc": submission.created_utc,
                "author": submission.author.name if submission.author else "[deleted]",
                "comments": comments,
                "pain_score": pain_score,
                "collected_at": datetime.now().isoformat()
            }

        except Exception as e:
            logger.error(f"Failed to extract data for submission {submission.id}: {e}")
            return None

    def _process_submission(self, submission, subreddit_config: Dict[str, Any]) -> bool:
        """å¤„ç†å•ä¸ªå¸–å­"""
        try:
            # æ£€æŸ¥æ˜¯å¦å·²å¤„ç†
            if submission.id in self.processed_posts:
                return False

            # æ£€æŸ¥æ˜¯å¦ä¸ºç—›ç‚¹å¸–å­
            if not self._is_pain_post(submission, subreddit_config):
                self.stats["filtered_out"] += 1
                return False

            # æå–å¸–å­æ•°æ®
            post_data = self._extract_post_data(submission, subreddit_config)
            if not post_data:
                self.stats["errors"] += 1
                return False

            # ä¿å­˜åˆ°æ•°æ®åº“
            if 'db' in globals():
                success = db.insert_raw_post(post_data)
            else:
                # å¤‡ç”¨æ–‡ä»¶å­˜å‚¨æ–¹æ¡ˆ
                success = self._save_post_to_file(post_data)

            if success:
                self.processed_posts.add(submission.id)
                self.stats["total_saved"] += 1
                logger.info(f"Saved post: {submission.title[:60]}... (Score: {submission.score}, Pain: {post_data['pain_score']:.2f})")
                return True
            else:
                self.stats["errors"] += 1
                return False

        except Exception as e:
            logger.error(f"Failed to process submission: {e}")
            self.stats["errors"] += 1
            return False

    def _save_post_to_file(self, post_data: Dict[str, Any]) -> bool:
        """ä¿å­˜å¸–å­åˆ°æ–‡ä»¶ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰"""
        try:
            output_dir = "data/raw_posts"
            os.makedirs(output_dir, exist_ok=True)
            file_path = os.path.join(output_dir, f"{post_data['id']}.json")

            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(post_data, f, indent=2, ensure_ascii=False)

            return True
        except Exception as e:
            logger.error(f"Failed to save post to file: {e}")
            return False

    def fetch_subreddit(self, subreddit_config: Dict[str, Any]) -> int:
        """æŠ“å–å•ä¸ªå­ç‰ˆå—"""
        subreddit_name = subreddit_config["name"]
        category = subreddit_config["category"]
        methods = subreddit_config.get("methods", ["hot"])

        logger.info(f"Fetching from r/{subreddit_name} (Category: {category})")

        try:
            subreddit = self.reddit_client.subreddit(subreddit_name)
            total_found = 0

            # æ„å»ºæœç´¢æŸ¥è¯¢
            search_query = self._build_search_query(subreddit_config)
            if search_query:
                logger.debug(f"Search query for r/{subreddit_name}: {search_query}")

            # è·å–å¸–å­é™åˆ¶
            max_results = self.config.get("search_strategy", {}).get("max_results_per_method", 100)

            for method in methods:
                logger.debug(f"Using method: {method}")

                try:
                    submissions = []

                    if method == "hot":
                        submissions = subreddit.hot(limit=max_results)
                    elif method == "new":
                        submissions = subreddit.new(limit=max_results)
                    elif method == "rising":
                        submissions = subreddit.rising(limit=max_results)
                    elif method == "controversial":
                        submissions = subreddit.controversial('week', limit=max_results)
                    elif method.startswith("top_"):
                        time_filter = method.split("_", 1)[1] if "_" in method else "week"
                        submissions = subreddit.top(time_filter=time_filter, limit=max_results)
                    elif method == "search" and search_query:
                        submissions = subreddit.search(search_query, sort='new', limit=max_results)
                    else:
                        logger.warning(f"Unknown method: {method}")
                        continue

                    # å¤„ç†å¸–å­
                    method_count = 0
                    for submission in submissions:
                        if self._process_submission(submission, subreddit_config):
                            method_count += 1

                    total_found += method_count
                    logger.info(f"Method {method}: found {method_count} posts in r/{subreddit_name}")

                    # æ·»åŠ å»¶è¿Ÿé¿å…APIé™åˆ¶
                    time.sleep(1)

                except Exception as e:
                    logger.error(f"Error with method {method} in r/{subreddit_name}: {e}")
                    continue

            return total_found

        except Exception as e:
            logger.error(f"Failed to fetch subreddit r/{subreddit_name}: {e}")
            return 0

    def fetch_all(self, limit_subreddits: Optional[int] = None) -> Dict[str, Any]:
        """æŠ“å–æ‰€æœ‰é…ç½®çš„å­ç‰ˆå—"""
        self.stats["start_time"] = datetime.now()

        logger.info("Starting Reddit pain point fetching...")

        # åŠ è½½å·²å¤„ç†çš„å¸–å­
        self._load_processed_posts()

        # ä»é…ç½®ä¸­æ„å»º subreddit åˆ—è¡¨
        subreddits = []

        # å¤„ç†æ‰€æœ‰åˆ†ç»„ä¸­çš„ subreddits
        for group_name, group_data in self.config.items():
            if group_name in ["ignore", "search_strategy"]:
                continue  # è·³è¿‡å¿½ç•¥åˆ—è¡¨å’Œæœç´¢ç­–ç•¥é…ç½®

            if isinstance(group_data, dict):
                for subreddit_name, subreddit_config in group_data.items():
                    if isinstance(subreddit_config, dict):
                        # æ„å»ºæœŸæœ›çš„é…ç½®æ ¼å¼
                        subreddit_data = {
                            "name": subreddit_name,
                            "category": group_name,
                            "min_upvotes": subreddit_config.get("min_upvotes", 0),
                            "min_comments": subreddit_config.get("min_comments", 0),
                            "methods": ["hot", "new", "top_week"]  # é»˜è®¤ä½¿ç”¨è¿™äº›æ–¹æ³•
                        }
                        subreddits.append(subreddit_data)

        # å¦‚æœæŒ‡å®šäº†é™åˆ¶ï¼Œåˆ™æˆªå–åˆ—è¡¨
        if limit_subreddits:
            subreddits = subreddits[:limit_subreddits]

        logger.info(f"Will fetch from {len(subreddits)} subreddits")

        total_found = 0
        for i, subreddit_config in enumerate(subreddits, 1):
            logger.info(f"Processing subreddit {i}/{len(subreddits)}")
            found = self.fetch_subreddit(subreddit_config)
            total_found += found

        # ä¿å­˜å·²å¤„ç†çš„å¸–å­
        self._save_processed_posts()

        # è®¡ç®—è¿è¡Œæ—¶é—´
        runtime = datetime.now() - self.stats["start_time"]

        # æ›´æ–°ç»Ÿè®¡
        self.stats["total_fetched"] = total_found
        self.stats["runtime_seconds"] = runtime.total_seconds()

        # è¾“å‡ºæ€»ç»“
        logger.info(f"""
=== Fetch Summary ===
Total subreddits processed: {len(subreddits)}
Total posts found: {total_found}
Total posts saved: {self.stats["total_saved"]}
Posts filtered out: {self.stats["filtered_out"]}
Errors encountered: {self.stats["errors"]}
Runtime: {runtime}
Posts per minute: {self.stats["total_saved"] / max(runtime.total_seconds() / 60, 1):.1f}
""")

        return self.stats.copy()

def main():
    """ä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description="Fetch Reddit posts for pain point discovery")
    parser.add_argument("--limit", type=int, help="Limit number of subreddits to process")
    parser.add_argument("--config", default="config/subreddits.yaml", help="Config file path")
    args = parser.parse_args()

    try:
        fetcher = RedditPainFetcher(args.config)
        stats = fetcher.fetch_all(limit_subreddits=args.limit)

        # è¾“å‡ºJSONæ ¼å¼çš„ç»Ÿè®¡ä¿¡æ¯ï¼ˆç”¨äºè„šæœ¬é›†æˆï¼‰
        print(json.dumps(stats, indent=2))

    except Exception as e:
        logger.error(f"Fatal error: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()
```


================================================================================
æ–‡ä»¶: pipeline/filter_signal.py
================================================================================

```python
"""
Filter Signal module for Reddit Pain Point Finder
ç—›ç‚¹ä¿¡å·è¿‡æ»¤æ¨¡å— - å†·è¡€å®ˆé—¨å‘˜
"""
import os
import json
import logging
import re
import yaml
from typing import List, Dict, Any, Set, Tuple
from datetime import datetime

logger = logging.getLogger(__name__)

class PainSignalFilter:
    """ç—›ç‚¹ä¿¡å·è¿‡æ»¤å™¨"""

    def __init__(self, config_path: str = "config/thresholds.yaml"):
        """åˆå§‹åŒ–è¿‡æ»¤å™¨"""
        self.thresholds = self._load_thresholds(config_path)
        self.subreddits_config = self._load_subreddits_config("config/subreddits.yaml")
        self.stats = {
            "total_processed": 0,
            "passed_filter": 0,
            "filtered_out": 0,
            "filter_reasons": {}
        }

    def _load_thresholds(self, config_path: str) -> Dict[str, Any]:
        """åŠ è½½é˜ˆå€¼é…ç½®"""
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                return yaml.safe_load(f)
        except Exception as e:
            logger.error(f"Failed to load thresholds from {config_path}: {e}")
            return {}

    def _load_subreddits_config(self, config_path: str) -> Dict[str, Any]:
        """åŠ è½½å­ç‰ˆå—é…ç½®"""
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                return yaml.safe_load(f)
        except Exception as e:
            logger.error(f"Failed to load subreddits config from {config_path}: {e}")
            return {}

    def _check_quality_thresholds(self, post_data: Dict[str, Any]) -> Tuple[bool, str]:
        """æ£€æŸ¥è´¨é‡é˜ˆå€¼"""
        quality_config = self.thresholds.get("reddit_quality", {})
        base_thresholds = quality_config.get("base", {})

        score = post_data.get("score", 0)
        comments = post_data.get("num_comments", 0)
        upvote_ratio = post_data.get("upvote_ratio", 0.0)
        text_length = len(post_data.get("title", "") + " " + post_data.get("body", ""))

        # æ£€æŸ¥åŸºç¡€é˜ˆå€¼
        if score < base_thresholds.get("min_upvotes", 5):
            return False, f"Too few upvotes: {score} < {base_thresholds.get('min_upvotes')}"

        if comments < base_thresholds.get("min_comments", 3):
            return False, f"Too few comments: {comments} < {base_thresholds.get('min_comments')}"

        if upvote_ratio < base_thresholds.get("min_upvote_ratio", 0.1):
            return False, f"Too low upvote ratio: {upvote_ratio:.2f} < {base_thresholds.get('min_upvote_ratio')}"

        if text_length < base_thresholds.get("min_post_length", 50):
            return False, f"Post too short: {text_length} < {base_thresholds.get('min_post_length')}"

        if text_length > base_thresholds.get("max_post_length", 5000):
            return False, f"Post too long: {text_length} > {base_thresholds.get('max_post_length')}"

        return True, "Passed quality thresholds"

    def _check_pain_keywords(self, post_data: Dict[str, Any]) -> Tuple[bool, List[str], float]:
        """æ£€æŸ¥ç—›ç‚¹å…³é”®è¯"""
        title = (post_data.get("title", "")).lower()
        body = (post_data.get("body", "")).lower()
        full_text = f"{title} {body}"

        pain_keywords = self.subreddits_config.get("pain_keywords", {})
        matched_keywords = []
        keyword_scores = {}

        # ç»Ÿè®¡å„ç±»åˆ«å…³é”®è¯åŒ¹é…
        for category, keywords in pain_keywords.items():
            category_matches = 0
            category_weight = {"frustration": 1.0, "inefficiency": 0.8, "complexity": 0.7, "workflow": 0.9, "cost": 0.6}.get(category, 0.5)

            for keyword in keywords:
                if keyword.lower() in full_text:
                    matched_keywords.append(f"{category}:{keyword}")
                    category_matches += 1
                    keyword_scores[keyword] = category_weight

            # è®¡ç®—è¯¥ç±»åˆ«çš„å¾—åˆ†
            if category_matches > 0:
                keyword_scores[f"category_{category}"] = category_matches * category_weight

        # è®¡ç®—æ€»ç—›ç‚¹åˆ†æ•°
        total_score = sum(score for score in keyword_scores.values() if isinstance(score, (int, float)))

        # æ ‡å‡†åŒ–åˆ†æ•°ï¼ˆ0-1èŒƒå›´ï¼‰
        normalized_score = min(total_score / 5.0, 1.0)  # å‡è®¾5åˆ†ä¸ºæ»¡åˆ†

        return len(matched_keywords) > 0, matched_keywords, normalized_score

    def _check_pain_patterns(self, post_data: Dict[str, Any]) -> Tuple[bool, List[str]]:
        """æ£€æŸ¥ç—›ç‚¹å¥å¼æ¨¡å¼"""
        title = (post_data.get("title", "")).lower()
        body = (post_data.get("body", "")).lower()
        full_text = f"{title} {body}"

        pain_config = self.thresholds.get("pain_signal", {})
        required_patterns = pain_config.get("pain_patterns", {}).get("required_patterns", [])
        strong_signals = pain_config.get("pain_patterns", {}).get("strong_signals", [])

        matched_patterns = []
        matched_strong = []

        # æ£€æŸ¥å¿…é¡»åŒ¹é…çš„å¥å¼
        for pattern in required_patterns:
            if pattern.lower() in full_text:
                matched_patterns.append(pattern)

        # æ£€æŸ¥å¼ºåŒ–ä¿¡å·å¥å¼
        for pattern in strong_signals:
            if pattern.lower() in full_text:
                matched_strong.append(pattern)

        # åˆ¤æ–­æ˜¯å¦é€šè¿‡æ¨¡å¼æ£€æŸ¥
        min_pattern_matches = pain_config.get("pain_patterns", {}).get("min_pattern_matches", 1)
        min_strong_signals = pain_config.get("pain_patterns", {}).get("min_strong_signals", 0)

        has_required = len(matched_patterns) >= min_pattern_matches
        has_strong = len(matched_strong) >= min_strong_signals

        all_matches = matched_patterns + matched_strong

        return (has_required or has_strong), all_matches

    def _check_exclusion_patterns(self, post_data: Dict[str, Any]) -> Tuple[bool, str]:
        """æ£€æŸ¥æ’é™¤æ¨¡å¼"""
        title = (post_data.get("title", "")).lower()
        body = (post_data.get("body", "")).lower()
        full_text = f"{title} {body}"

        exclude_patterns = self.subreddits_config.get("exclude_patterns", {})

        for category, patterns in exclude_patterns.items():
            for pattern in patterns:
                if pattern.lower() in full_text:
                    return False, f"Excluded due to {category}: {pattern}"

        return True, "No exclusion patterns matched"

    def _calculate_emotional_intensity(self, post_data: Dict[str, Any]) -> float:
        """è®¡ç®—æƒ…ç»ªå¼ºåº¦"""
        title = (post_data.get("title", "")).lower()
        body = (post_data.get("body", "")).lower()
        full_text = f"{title} {body}"

        # é«˜å¼ºåº¦æƒ…ç»ªè¯æ±‡
        high_intensity_words = [
            "frustrated", "frustrating", "annoying", "annoyed", "hate", "terrible",
            "awful", "horrible", "disaster", "catastrophe", "nightmare", "hell",
            "impossible", "useless", "worthless", "broken", "crashed", "failed"
        ]

        # ä¸­å¼ºåº¦æƒ…ç»ªè¯æ±‡
        medium_intensity_words = [
            "difficult", "hard", "struggling", "trouble", "problem", "issue",
            "challenge", "confusing", "complicated", "complex", "slow", "tedious"
        ]

        # ä½å¼ºåº¦æƒ…ç»ªè¯æ±‡
        low_intensity_words = [
            "annoyance", "minor", "slight", "inconvenient", "suboptimal", "could be better"
        ]

        high_count = sum(1 for word in high_intensity_words if word in full_text)
        medium_count = sum(1 for word in medium_intensity_words if word in full_text)
        low_count = sum(1 for word in low_intensity_words if word in full_text)

        # è®¡ç®—åŠ æƒæƒ…ç»ªå¼ºåº¦
        intensity = (high_count * 1.0 + medium_count * 0.6 + low_count * 0.3) / max(len(full_text.split()) / 100, 1)
        return min(intensity, 1.0)

    def _check_post_type_specific(self, post_data: Dict[str, Any]) -> Tuple[bool, str]:
        """æ£€æŸ¥ç‰¹å®šç±»å‹å¸–å­çš„é˜ˆå€¼"""
        subreddit = post_data.get("subreddit", "").lower()
        score = post_data.get("score", 0)
        comments = post_data.get("num_comments", 0)

        quality_config = self.thresholds.get("reddit_quality", {})
        type_specific = quality_config.get("type_specific", {})

        # æ ¹æ®å­ç‰ˆå—ç±»åˆ«åˆ¤æ–­ç±»å‹
        post_type = "general"  # é»˜è®¤ç±»å‹
        if any(keyword in subreddit for keyword in ["programming", "sysadmin", "webdev", "technical"]):
            post_type = "technical"
        elif any(keyword in subreddit for keyword in ["entrepreneur", "startups", "business"]):
            post_type = "business"
        elif "discussion" in subreddit or comments > score * 2:
            post_type = "discussion"

        if post_type in type_specific:
            type_config = type_specific[post_type]
            if score < type_config.get("min_upvotes", 0):
                return False, f"Type {post_type}: too few upvotes: {score} < {type_config.get('min_upvotes')}"
            if comments < type_config.get("min_comments", 0):
                return False, f"Type {post_type}: too few comments: {comments} < {type_config.get('min_comments')}"

        return True, f"Type {post_type} check passed"

    def filter_post(self, post_data: Dict[str, Any]) -> Tuple[bool, Dict[str, Any]]:
        """è¿‡æ»¤å•ä¸ªå¸–å­"""
        self.stats["total_processed"] += 1

        filter_result = {
            "post_id": post_data.get("id"),
            "passed": False,
            "pain_score": 0.0,
            "reasons": [],
            "matched_keywords": [],
            "matched_patterns": [],
            "emotional_intensity": 0.0,
            "filter_summary": {}
        }

        # 1. è´¨é‡é˜ˆå€¼æ£€æŸ¥
        quality_passed, quality_reason = self._check_quality_thresholds(post_data)
        if not quality_passed:
            self.stats["filtered_out"] += 1
            self.stats["filter_reasons"][quality_reason] = self.stats["filter_reasons"].get(quality_reason, 0) + 1
            filter_result["reasons"].append(quality_reason)
            filter_result["filter_summary"] = {"reason": "quality_threshold", "details": quality_reason}
            return False, filter_result

        # 2. æ’é™¤æ¨¡å¼æ£€æŸ¥
        exclusion_passed, exclusion_reason = self._check_exclusion_patterns(post_data)
        if not exclusion_passed:
            self.stats["filtered_out"] += 1
            self.stats["filter_reasons"][exclusion_reason] = self.stats["filter_reasons"].get(exclusion_reason, 0) + 1
            filter_result["reasons"].append(exclusion_reason)
            filter_result["filter_summary"] = {"reason": "exclusion_pattern", "details": exclusion_reason}
            return False, filter_result

        # 3. ç—›ç‚¹å…³é”®è¯æ£€æŸ¥
        has_keywords, matched_keywords, keyword_score = self._check_pain_keywords(post_data)
        filter_result["matched_keywords"] = matched_keywords

        # 4. ç—›ç‚¹å¥å¼æ£€æŸ¥
        has_patterns, matched_patterns = self._check_pain_patterns(post_data)
        filter_result["matched_patterns"] = matched_patterns

        # 5. æƒ…ç»ªå¼ºåº¦è®¡ç®—
        emotional_intensity = self._calculate_emotional_intensity(post_data)
        filter_result["emotional_intensity"] = emotional_intensity

        # 6. ç±»å‹ç‰¹å®šæ£€æŸ¥
        type_passed, type_reason = self._check_post_type_specific(post_data)
        if not type_passed:
            self.stats["filtered_out"] += 1
            self.stats["filter_reasons"][type_reason] = self.stats["filter_reasons"].get(type_reason, 0) + 1
            filter_result["reasons"].append(type_reason)
            filter_result["filter_summary"] = {"reason": "type_specific", "details": type_reason}
            return False, filter_result

        # è®¡ç®—ç»¼åˆç—›ç‚¹åˆ†æ•°
        pain_score = 0.0

        # å…³é”®è¯åˆ†æ•° (40%)
        pain_score += keyword_score * 0.4

        # å¥å¼åˆ†æ•° (30%)
        pattern_score = min(len(matched_patterns) / 3.0, 1.0) * 0.3
        pain_score += pattern_score

        # æƒ…ç»ªå¼ºåº¦åˆ†æ•° (20%)
        pain_score += emotional_intensity * 0.2

        # åŸºç¡€è´¨é‡åˆ†æ•° (10%)
        score_normalized = min(post_data.get("score", 0) / 100.0, 1.0)
        comments_normalized = min(post_data.get("num_comments", 0) / 50.0, 1.0)
        quality_score = (score_normalized + comments_normalized) / 2.0 * 0.1
        pain_score += quality_score

        # ç¡®ä¿åˆ†æ•°åœ¨0-1èŒƒå›´å†…
        pain_score = min(max(pain_score, 0.0), 1.0)

        filter_result["pain_score"] = pain_score

        # åˆ¤æ–­æ˜¯å¦é€šè¿‡ç—›ç‚¹ä¿¡å·æ£€æŸ¥
        pain_config = self.thresholds.get("pain_signal", {})
        min_keyword_matches = pain_config.get("keyword_match", {}).get("min_matches", 1)
        min_emotional_intensity = pain_config.get("emotional_intensity", {}).get("min_score", 0.3)

        # æœ€ç»ˆåˆ¤æ–­
        passed = (
            has_keywords and
            len(matched_keywords) >= min_keyword_matches and
            emotional_intensity >= min_emotional_intensity and
            pain_score >= 0.3  # ç»¼åˆåˆ†æ•°é˜ˆå€¼
        )

        if passed:
            self.stats["passed_filter"] += 1
            filter_result["passed"] = True
            filter_result["filter_summary"] = {
                "reason": "passed",
                "pain_score": pain_score,
                "components": {
                    "keywords": keyword_score,
                    "patterns": pattern_score,
                    "emotion": emotional_intensity,
                    "quality": quality_score
                }
            }
        else:
            self.stats["filtered_out"] += 1
            failure_reasons = []
            if not has_keywords or len(matched_keywords) < min_keyword_matches:
                failure_reasons.append("insufficient_keywords")
            if emotional_intensity < min_emotional_intensity:
                failure_reasons.append("low_emotional_intensity")
            if pain_score < 0.3:
                failure_reasons.append("low_overall_score")

            reason_str = "; ".join(failure_reasons)
            self.stats["filter_reasons"][reason_str] = self.stats["filter_reasons"].get(reason_str, 0) + 1
            filter_result["reasons"].append(f"Failed: {reason_str}")
            filter_result["filter_summary"] = {"reason": "failed", "details": failure_reasons}

        return passed, filter_result

    def filter_posts_batch(self, posts: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """æ‰¹é‡è¿‡æ»¤å¸–å­"""
        logger.info(f"Filtering {len(posts)} posts through pain signal detector")

        filtered_posts = []
        for i, post in enumerate(posts):
            if i % 100 == 0:
                logger.info(f"Processed {i}/{len(posts)} posts")

            passed, result = self.filter_post(post)

            if passed:
                # ä¸ºå¸–å­æ·»åŠ è¿‡æ»¤ç»“æœ
                filtered_post = post.copy()
                filtered_post.update({
                    "pain_score": result["pain_score"],
                    "pain_keywords": result["matched_keywords"],
                    "pain_patterns": result["matched_patterns"],
                    "emotional_intensity": result["emotional_intensity"],
                    "filter_reason": "pain_signal_passed"
                })
                filtered_posts.append(filtered_post)

        logger.info(f"Filter complete: {len(filtered_posts)}/{len(posts)} posts passed")
        return filtered_posts

    def get_statistics(self) -> Dict[str, Any]:
        """è·å–è¿‡æ»¤ç»Ÿè®¡ä¿¡æ¯"""
        stats = self.stats.copy()
        if stats["total_processed"] > 0:
            stats["pass_rate"] = stats["passed_filter"] / stats["total_processed"]
        else:
            stats["pass_rate"] = 0.0
        return stats

    def reset_statistics(self):
        """é‡ç½®ç»Ÿè®¡ä¿¡æ¯"""
        self.stats = {
            "total_processed": 0,
            "passed_filter": 0,
            "filtered_out": 0,
            "filter_reasons": {}
        }

def main():
    """ä¸»å‡½æ•° - è¿‡æ»¤åŸå§‹å¸–å­"""
    import argparse
    from utils.db import db

    parser = argparse.ArgumentParser(description="Filter Reddit posts for pain signals")
    parser.add_argument("--limit", type=int, default=1000, help="Limit number of posts to process")
    parser.add_argument("--min-score", type=float, default=0.0, help="Minimum pain score threshold")
    args = parser.parse_args()

    try:
        logger.info("Starting pain signal filtering...")

        # åˆå§‹åŒ–è¿‡æ»¤å™¨
        filter = PainSignalFilter()

        # è·å–æœªè¿‡æ»¤çš„å¸–å­
        logger.info(f"Fetching up to {args.limit} unprocessed posts...")
        unfiltered_posts = db.get_unprocessed_posts(limit=args.limit)

        if not unfiltered_posts:
            logger.info("No unprocessed posts found")
            return

        logger.info(f"Found {len(unfiltered_posts)} posts to filter")

        # æ‰¹é‡è¿‡æ»¤
        filtered_posts = filter.filter_posts_batch(unfiltered_posts)

        # åº”ç”¨æœ€å°åˆ†æ•°é˜ˆå€¼
        if args.min_score > 0:
            filtered_posts = [p for p in filtered_posts if p.get("pain_score", 0) >= args.min_score]
            logger.info(f"After applying min_score threshold: {len(filtered_posts)} posts")

        # ä¿å­˜è¿‡æ»¤ç»“æœ
        saved_count = 0
        for post in filtered_posts:
            if db.insert_filtered_post(post):
                saved_count += 1

        logger.info(f"Saved {saved_count}/{len(filtered_posts)} filtered posts to database")

        # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
        stats = filter.get_statistics()
        logger.info(f"""
=== Filter Summary ===
Total processed: {stats['total_processed']}
Passed filter: {stats['passed_filter']}
Filtered out: {stats['filtered_out']}
Pass rate: {stats['pass_rate']:.2%}
Filter reasons: {stats['filter_reasons']}
""")

    except Exception as e:
        logger.error(f"Fatal error: {e}")
        raise

if __name__ == "__main__":
    main()
```


================================================================================
æ–‡ä»¶: pipeline/map_opportunity.py
================================================================================

```python
"""
Map Opportunity module for Reddit Pain Point Finder
æœºä¼šæ˜ å°„æ¨¡å— - ä»ç—›ç‚¹èšç±»ä¸­å‘ç°å·¥å…·æœºä¼š
"""
import json
import logging
import time
from typing import List, Dict, Any, Optional
from datetime import datetime

from utils.llm_client import llm_client
from utils.db import db

logger = logging.getLogger(__name__)

class OpportunityMapper:
    """æœºä¼šæ˜ å°„å™¨"""

    def __init__(self):
        """åˆå§‹åŒ–æœºä¼šæ˜ å°„å™¨"""
        self.stats = {
            "total_clusters_processed": 0,
            "opportunities_identified": 0,
            "viable_opportunities": 0,
            "processing_time": 0.0,
            "avg_opportunity_score": 0.0
        }

    def _enrich_cluster_data(self, cluster_data: Dict[str, Any]) -> Dict[str, Any]:
        """ä¸°å¯Œèšç±»æ•°æ®"""
        try:
            # è·å–èšç±»ä¸­çš„ç—›ç‚¹äº‹ä»¶è¯¦æƒ…
            pain_event_ids = json.loads(cluster_data.get("pain_event_ids", "[]"))

            pain_events = []
            with db.get_connection("pain") as conn:
                for event_id in pain_event_ids:
                    cursor = conn.execute("""
                        SELECT * FROM pain_events WHERE id = ?
                    """, (event_id,))
                    event_data = cursor.fetchone()
                    if event_data:
                        pain_events.append(dict(event_data))

            # æ·»åŠ åŸå§‹å¸–å­ä¿¡æ¯
            for event in pain_events:
                with db.get_connection("filtered") as conn:
                    cursor = conn.execute("""
                        SELECT title, subreddit, score, num_comments, pain_score
                        FROM filtered_posts WHERE id = ?
                    """, (event["post_id"],))
                    post_data = cursor.fetchone()
                    if post_data:
                        event.update(dict(post_data))

            # æ„å»ºä¸°å¯Œçš„èšç±»æ‘˜è¦
            enriched_cluster = {
                "cluster_id": cluster_data["id"],
                "cluster_name": cluster_data["cluster_name"],
                "cluster_description": cluster_data["cluster_description"],
                "cluster_size": cluster_data["cluster_size"],
                "workflow_confidence": cluster_data.get("workflow_confidence", 0.0),
                "pain_events": pain_events,
                "created_at": cluster_data["created_at"]
            }

            # åˆ†æèšç±»ç‰¹å¾
            self._analyze_cluster_characteristics(enriched_cluster)

            return enriched_cluster

        except Exception as e:
            logger.error(f"Failed to enrich cluster data: {e}")
            return cluster_data

    def _analyze_cluster_characteristics(self, cluster_data: Dict[str, Any]):
        """åˆ†æèšç±»ç‰¹å¾"""
        try:
            pain_events = cluster_data.get("pain_events", [])

            if not pain_events:
                return

            # ç»Ÿè®¡å­ç‰ˆå—åˆ†å¸ƒ
            subreddits = {}
            for event in pain_events:
                subreddit = event.get("subreddit", "unknown")
                subreddits[subreddit] = subreddits.get(subreddit, 0) + 1

            # ç»Ÿè®¡æåˆ°çš„å·¥å…·
            mentioned_tools = []
            for event in pain_events:
                tools = event.get("mentioned_tools", [])
                if isinstance(tools, list):
                    mentioned_tools.extend(tools)
                elif isinstance(tools, str):
                    mentioned_tools.append(tools)

            tool_counts = {}
            for tool in mentioned_tools:
                if tool:
                    tool_counts[tool] = tool_counts.get(tool, 0) + 1

            # ç»Ÿè®¡æƒ…ç»ªä¿¡å·
            emotional_signals = {}
            for event in pain_events:
                signal = event.get("emotional_signal", "")
                if signal:
                    emotional_signals[signal] = emotional_signals.get(signal, 0) + 1

            # ç»Ÿè®¡é¢‘ç‡åˆ†æ•°
            frequency_scores = [event.get("frequency_score", 5) for event in pain_events if event.get("frequency_score")]
            avg_frequency_score = sum(frequency_scores) / len(frequency_scores) if frequency_scores else 5.0

            # æå–ä»£è¡¨æ€§é—®é¢˜
            problems = [event.get("problem", "") for event in pain_events if event.get("problem")]
            unique_problems = list(set(problems))

            # æå–å·¥ä½œæ–¹å¼
            workarounds = [event.get("current_workaround", "") for event in pain_events if event.get("current_workaround")]
            unique_workarounds = [w for w in set(workarounds) if w]

            # æ›´æ–°èšç±»æ•°æ®
            cluster_data.update({
                "subreddit_distribution": subreddits,
                "mentioned_tools": tool_counts,
                "emotional_signals": emotional_signals,
                "avg_frequency_score": avg_frequency_score,
                "representative_problems": unique_problems[:10],  # æœ€å¤š10ä¸ª
                "representative_workarounds": unique_workarounds[:5],  # æœ€å¤š5ä¸ª
                "total_pain_score": sum(event.get("post_pain_score", 0) for event in pain_events)
            })

        except Exception as e:
            logger.error(f"Failed to analyze cluster characteristics: {e}")

    def _map_opportunity_with_llm(self, cluster_data: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """ä½¿ç”¨LLMæ˜ å°„æœºä¼š"""
        try:
            # è°ƒç”¨LLMè¿›è¡Œæœºä¼šæ˜ å°„
            response = llm_client.map_opportunity(cluster_data)

            opportunity_data = response["content"]

            # æ£€æŸ¥æ˜¯å¦æ‰¾åˆ°æœºä¼š
            if "opportunity" in opportunity_data and opportunity_data["opportunity"]:
                return opportunity_data
            else:
                logger.info(f"No viable opportunity found for cluster {cluster_data['cluster_name']}")
                return None

        except Exception as e:
            logger.error(f"Failed to map opportunity with LLM: {e}")
            return None

    def _evaluate_opportunity_quality(self, opportunity_data: Dict[str, Any], cluster_data: Dict[str, Any]) -> Dict[str, Any]:
        """è¯„ä¼°æœºä¼šè´¨é‡"""
        try:
            opportunity = opportunity_data.get("opportunity", {})

            if not opportunity:
                return {"is_viable": False, "reason": "No opportunity data"}

            # åŸºç¡€è´¨é‡æ£€æŸ¥
            required_fields = ["name", "description", "target_users"]
            for field in required_fields:
                if not opportunity.get(field):
                    return {"is_viable": False, "reason": f"Missing required field: {field}"}

            # è´¨é‡è¯„åˆ†
            quality_score = 0.0
            reasons = []

            # ç—›ç‚¹é¢‘ç‡ (20%)
            pain_frequency = opportunity.get("pain_frequency", 0)
            if pain_frequency >= 7:
                quality_score += 0.2
                reasons.append("High pain frequency")
            elif pain_frequency >= 5:
                quality_score += 0.1
                reasons.append("Medium pain frequency")

            # å¸‚åœºè§„æ¨¡ (20%)
            market_size = opportunity.get("market_size", 0)
            if market_size >= 7:
                quality_score += 0.2
                reasons.append("Large market size")
            elif market_size >= 5:
                quality_score += 0.1
                reasons.append("Medium market size")

            # MVPå¤æ‚åº¦ (25%) - è¶Šä½è¶Šå¥½
            mvp_complexity = opportunity.get("mvp_complexity", 10)
            if mvp_complexity <= 4:
                quality_score += 0.25
                reasons.append("Simple MVP")
            elif mvp_complexity <= 6:
                quality_score += 0.15
                reasons.append("Moderate MVP complexity")

            # ç«äº‰é£é™© (20%) - è¶Šä½è¶Šå¥½
            competition_risk = opportunity.get("competition_risk", 10)
            if competition_risk <= 4:
                quality_score += 0.2
                reasons.append("Low competition")
            elif competition_risk <= 6:
                quality_score += 0.1
                reasons.append("Moderate competition")

            # é›†æˆéš¾åº¦ (15%) - è¶Šä½è¶Šå¥½
            integration_complexity = opportunity.get("integration_complexity", 10)
            if integration_complexity <= 5:
                quality_score += 0.15
                reasons.append("Easy integration")
            elif integration_complexity <= 7:
                quality_score += 0.08
                reasons.append("Moderate integration")

            # èšç±»å¤§å°åŠ åˆ†
            cluster_size = cluster_data.get("cluster_size", 0)
            if cluster_size >= 10:
                quality_score += 0.1
                reasons.append("Large cluster size")

            # æ€»åˆ†èŒƒå›´ï¼š0-1
            total_score = min(quality_score, 1.0)

            # åˆ¤æ–­æ˜¯å¦å¯è¡Œ
            is_viable = total_score >= 0.4  # 40%ä»¥ä¸Šè®¤ä¸ºå¯è¡Œ

            return {
                "is_viable": is_viable,
                "quality_score": total_score,
                "quality_reasons": reasons,
                "detailed_scores": {
                    "pain_frequency": pain_frequency,
                    "market_size": market_size,
                    "mvp_complexity": mvp_complexity,
                    "competition_risk": competition_risk,
                    "integration_complexity": integration_complexity
                }
            }

        except Exception as e:
            logger.error(f"Failed to evaluate opportunity quality: {e}")
            return {"is_viable": False, "reason": f"Evaluation error: {e}"}

    def _save_opportunity_to_database(self, cluster_id: int, opportunity_data: Dict[str, Any], quality_result: Dict[str, Any]) -> Optional[int]:
        """ä¿å­˜æœºä¼šåˆ°æ•°æ®åº“"""
        try:
            opportunity = opportunity_data.get("opportunity", {})

            # å‡†å¤‡æœºä¼šæ•°æ®
            opportunity_record = {
                "cluster_id": cluster_id,
                "opportunity_name": opportunity.get("name", ""),
                "description": opportunity.get("description", ""),
                "current_tools": json.dumps(opportunity_data.get("current_tools", [])),
                "missing_capability": opportunity_data.get("missing_capability", ""),
                "why_existing_fail": opportunity_data.get("why_existing_fail", ""),
                "target_users": opportunity.get("target_users", ""),
                "pain_frequency_score": opportunity.get("pain_frequency", 0),
                "market_size_score": opportunity.get("market_size", 0),
                "mvp_complexity_score": opportunity.get("mvp_complexity", 0),
                "competition_risk_score": opportunity.get("competition_risk", 0),
                "integration_complexity_score": opportunity.get("integration_complexity", 0),
                "total_score": quality_result["quality_score"],
                "killer_risks": json.dumps([]),  # ç¨ååœ¨viability scoringä¸­å¡«å……
                "recommendation": ""  # ç¨ååœ¨viability scoringä¸­å¡«å……
            }

            opportunity_id = db.insert_opportunity(opportunity_record)
            return opportunity_id

        except Exception as e:
            logger.error(f"Failed to save opportunity to database: {e}")
            return None

    def map_opportunities_for_clusters(self, limit: int = 50) -> Dict[str, Any]:
        """ä¸ºèšç±»æ˜ å°„æœºä¼š"""
        logger.info(f"Mapping opportunities for up to {limit} clusters")

        start_time = time.time()

        try:
            # è·å–èšç±»
            with db.get_connection("clusters") as conn:
                cursor = conn.execute("""
                    SELECT * FROM clusters
                    ORDER BY cluster_size DESC, workflow_confidence DESC
                    LIMIT ?
                """, (limit,))
                clusters = [dict(row) for row in cursor.fetchall()]

            if not clusters:
                logger.info("No clusters found for opportunity mapping")
                return {"opportunities_identified": 0, "clusters_processed": 0}

            logger.info(f"Processing {len(clusters)} clusters for opportunity mapping")

            opportunities_created = []
            viable_opportunities = 0

            for i, cluster in enumerate(clusters):
                logger.info(f"Processing cluster {i+1}/{len(clusters)}: {cluster['cluster_name']}")

                try:
                    # ä¸°å¯Œèšç±»æ•°æ®
                    enriched_cluster = self._enrich_cluster_data(cluster)

                    # ä½¿ç”¨LLMæ˜ å°„æœºä¼š
                    opportunity_data = self._map_opportunity_with_llm(enriched_cluster)

                    if opportunity_data:
                        # è¯„ä¼°æœºä¼šè´¨é‡
                        quality_result = self._evaluate_opportunity_quality(opportunity_data, enriched_cluster)

                        if quality_result["is_viable"]:
                            # ä¿å­˜åˆ°æ•°æ®åº“
                            opportunity_id = self._save_opportunity_to_database(
                                cluster["id"], opportunity_data, quality_result
                            )

                            if opportunity_id:
                                opportunity_summary = {
                                    "opportunity_id": opportunity_id,
                                    "cluster_id": cluster["id"],
                                    "cluster_name": cluster["cluster_name"],
                                    "opportunity_name": opportunity_data["opportunity"]["name"],
                                    "opportunity_description": opportunity_data["opportunity"]["description"],
                                    "quality_score": quality_result["quality_score"],
                                    "quality_reasons": quality_result["quality_reasons"]
                                }

                                opportunities_created.append(opportunity_summary)
                                viable_opportunities += 1

                                logger.info(f"Created opportunity: {opportunity_data['opportunity']['name']} (Score: {quality_result['quality_score']:.2f})")
                        else:
                            logger.debug(f"Opportunity not viable: {quality_result.get('reason', 'Unknown')}")
                    else:
                        logger.debug(f"No opportunity found for cluster {cluster['cluster_name']}")

                except Exception as e:
                    logger.error(f"Failed to process cluster {cluster['cluster_name']}: {e}")
                    continue

                # æ·»åŠ å»¶è¿Ÿé¿å…APIé™åˆ¶
                time.sleep(2)

            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            processing_time = time.time() - start_time
            self.stats["total_clusters_processed"] = len(clusters)
            self.stats["opportunities_identified"] = len(opportunities_created)
            self.stats["viable_opportunities"] = viable_opportunities
            self.stats["processing_time"] = processing_time

            if opportunities_created:
                self.stats["avg_opportunity_score"] = sum(opp["quality_score"] for opp in opportunities_created) / len(opportunities_created)

            logger.info(f"""
=== Opportunity Mapping Summary ===
Clusters processed: {len(clusters)}
Opportunities identified: {len(opportunities_created)}
Viable opportunities: {viable_opportunities}
Average opportunity score: {self.stats['avg_opportunity_score']:.2f}
Processing time: {processing_time:.2f}s
""")

            return {
                "opportunities_created": len(opportunities_created),
                "viable_opportunities": viable_opportunities,
                "clusters_processed": len(clusters),
                "opportunity_details": opportunities_created,
                "mapping_stats": self.get_statistics()
            }

        except Exception as e:
            logger.error(f"Failed to map opportunities: {e}")
            raise

    def get_opportunities_summary(self, min_score: float = 0.0, limit: int = 50) -> List[Dict[str, Any]]:
        """è·å–æœºä¼šæ‘˜è¦"""
        try:
            with db.get_connection("clusters") as conn:
                cursor = conn.execute("""
                    SELECT o.*, c.cluster_name, c.cluster_description, c.cluster_size
                    FROM opportunities o
                    JOIN clusters c ON o.cluster_id = c.id
                    WHERE o.total_score >= ?
                    ORDER BY o.total_score DESC
                    LIMIT ?
                """, (min_score, limit))
                opportunities = [dict(row) for row in cursor.fetchall()]

            return opportunities

        except Exception as e:
            logger.error(f"Failed to get opportunities summary: {e}")
            return []

    def get_statistics(self) -> Dict[str, Any]:
        """è·å–æ˜ å°„ç»Ÿè®¡ä¿¡æ¯"""
        stats = self.stats.copy()

        if stats["total_clusters_processed"] > 0:
            stats["opportunity_rate"] = stats["opportunities_identified"] / stats["total_clusters_processed"]
            stats["viable_rate"] = stats["viable_opportunities"] / stats["total_clusters_processed"]
            stats["processing_rate"] = stats["total_clusters_processed"] / max(stats["processing_time"], 1)
        else:
            stats["opportunity_rate"] = 0
            stats["viable_rate"] = 0
            stats["processing_rate"] = 0

        return stats

    def reset_statistics(self):
        """é‡ç½®ç»Ÿè®¡ä¿¡æ¯"""
        self.stats = {
            "total_clusters_processed": 0,
            "opportunities_identified": 0,
            "viable_opportunities": 0,
            "processing_time": 0.0,
            "avg_opportunity_score": 0.0
        }

def main():
    """ä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description="Map opportunities from pain point clusters")
    parser.add_argument("--limit", type=int, default=50, help="Limit number of clusters to process")
    parser.add_argument("--min-score", type=float, default=0.0, help="Minimum opportunity score")
    parser.add_argument("--list", action="store_true", help="List existing opportunities")
    args = parser.parse_args()

    try:
        logger.info("Starting opportunity mapping...")

        mapper = OpportunityMapper()

        if args.list:
            # åˆ—å‡ºç°æœ‰æœºä¼š
            opportunities = mapper.get_opportunities_summary(min_score=args.min_score)
            print(json.dumps(opportunities, indent=2, default=str))

        else:
            # æ˜ å°„æ–°æœºä¼š
            result = mapper.map_opportunities_for_clusters(limit=args.limit)

            logger.info(f"""
=== Opportunity Mapping Complete ===
Opportunities created: {result['opportunities_created']}
Viable opportunities: {result['viable_opportunities']}
Clusters processed: {result['clusters_processed']}
Mapping stats: {result['mapping_stats']}
""")

    except Exception as e:
        logger.error(f"Fatal error: {e}")
        raise

if __name__ == "__main__":
    main()
```


================================================================================
æ–‡ä»¶: pipeline/score_viability.py
================================================================================

```python
"""
Score Viability module for Reddit Pain Point Finder
å¯è¡Œæ€§è¯„åˆ†æ¨¡å— - é’ˆå¯¹ä¸€äººå…¬å¸çš„å¯è¡Œæ€§è¯„ä¼°
"""
import json
import logging
import time
from typing import List, Dict, Any, Optional
from datetime import datetime

from utils.llm_client import llm_client
from utils.db import db

logger = logging.getLogger(__name__)

class ViabilityScorer:
    """å¯è¡Œæ€§è¯„åˆ†å™¨"""

    def __init__(self):
        """åˆå§‹åŒ–è¯„åˆ†å™¨"""
        self.stats = {
            "total_opportunities_scored": 0,
            "viable_opportunities": 0,
            "good_opportunities": 0,
            "excellent_opportunities": 0,
            "processing_time": 0.0,
            "avg_total_score": 0.0
        }

    def _enhance_opportunity_data(self, opportunity_data: Dict[str, Any]) -> Dict[str, Any]:
        """å¢å¼ºæœºä¼šæ•°æ®"""
        try:
            # è·å–èšç±»ä¿¡æ¯
            cluster_id = opportunity_data["cluster_id"]
            with db.get_connection("clusters") as conn:
                cursor = conn.execute("""
                    SELECT * FROM clusters WHERE id = ?
                """, (cluster_id,))
                cluster_data = cursor.fetchone()

            if not cluster_data:
                return opportunity_data

            cluster_info = dict(cluster_data)

            # è·å–èšç±»ä¸­çš„ç—›ç‚¹äº‹ä»¶
            pain_event_ids = json.loads(cluster_info.get("pain_event_ids", "[]"))
            pain_events = []

            with db.get_connection("pain") as conn:
                for event_id in pain_event_ids:
                    cursor = conn.execute("""
                        SELECT * FROM pain_events WHERE id = ?
                    """, (event_id,))
                    event_data = cursor.fetchone()
                    if event_data:
                        pain_events.append(dict(event_data))

            # å¢å¼ºæœºä¼šæ•°æ®
            enhanced_opportunity = opportunity_data.copy()
            enhanced_opportunity.update({
                "cluster_info": {
                    "cluster_name": cluster_info["cluster_name"],
                    "cluster_description": cluster_info["cluster_description"],
                    "cluster_size": cluster_info["cluster_size"],
                    "workflow_confidence": cluster_info.get("workflow_confidence", 0.0),
                    "pain_events": pain_events
                }
            })

            # æ·»åŠ å¸‚åœºè§„æ¨¡ä¼°ç®—
            self._estimate_market_size(enhanced_opportunity)

            # æ·»åŠ ç«äº‰åˆ†æ
            self._analyze_competition(enhanced_opportunity)

            return enhanced_opportunity

        except Exception as e:
            logger.error(f"Failed to enhance opportunity data: {e}")
            return opportunity_data

    def _estimate_market_size(self, opportunity_data: Dict[str, Any]):
        """ä¼°ç®—å¸‚åœºè§„æ¨¡"""
        try:
            cluster_info = opportunity_data.get("cluster_info", {})
            pain_events = cluster_info.get("pain_events", [])

            # åŸºäºå­ç‰ˆå—åˆ†å¸ƒä¼°ç®—ç”¨æˆ·ç¾¤ä½“
            subreddit_distribution = {}
            for event in pain_events:
                with db.get_connection("filtered") as conn:
                    cursor = conn.execute("""
                        SELECT subreddit FROM filtered_posts WHERE id = ?
                    """, (event["post_id"],))
                    post_data = cursor.fetchone()
                    if post_data:
                        subreddit = post_data[0]
                        subreddit_distribution[subreddit] = subreddit_distribution.get(subreddit, 0) + 1

            # ä¼°ç®—ç”¨æˆ·åŸºæ•°
            subreddit_estimates = {
                "programming": 5000000,  # 500ä¸‡å¼€å‘è€…
                "MachineLearning": 2000000,  # 200ä¸‡MLä»ä¸šè€…
                "Entrepreneur": 1000000,  # 100ä¸‡åˆ›ä¸šè€…
                "startups": 2000000,  # 200ä¸‡åˆåˆ›å…¬å¸äººå‘˜
                "dataisbeautiful": 500000,  # 50ä¸‡æ•°æ®çˆ±å¥½è€…
                "webdev": 3000000,  # 300ä¸‡Webå¼€å‘è€…
                "sysadmin": 1500000,  # 150ä¸‡ç³»ç»Ÿç®¡ç†å‘˜
                "ChatGPT": 10000000,  # 1000ä¸‡ChatGPTç”¨æˆ·
                "LocalLLaMA": 500000,  # 50ä¸‡æœ¬åœ°LLMç”¨æˆ·
            }

            # è®¡ç®—æ€»å¸‚åœºè§„æ¨¡
            total_estimated_users = 0
            for subreddit, count in subreddit_distribution.items():
                estimated_users = subreddit_estimates.get(subreddit, 100000)  # é»˜è®¤10ä¸‡
                weight = count / len(pain_events)  # åŸºäºå‡ºç°é¢‘ç‡çš„æƒé‡
                total_estimated_users += estimated_users * weight

            # å¸‚åœºæ¸—é€ç‡ä¼°ç®—ï¼ˆä¿å®ˆä¼°è®¡ï¼‰
            penetration_rate = 0.001  # 0.1%çš„å¸‚åœºæ¸—é€ç‡
            addressable_market = total_estimated_users * penetration_rate

            opportunity_data["market_analysis"] = {
                "subreddit_distribution": subreddit_distribution,
                "estimated_total_users": int(total_estimated_users),
                "conservative_penetration_rate": penetration_rate,
                "addressable_market_size": int(addressable_market),
                "market_tier": self._get_market_tier(addressable_market)
            }

        except Exception as e:
            logger.error(f"Failed to estimate market size: {e}")

    def _get_market_tier(self, market_size: int) -> str:
        """è·å–å¸‚åœºå±‚çº§"""
        if market_size > 100000:  # 10ä¸‡+
            return "large"
        elif market_size > 50000:  # 5ä¸‡-10ä¸‡
            return "medium"
        elif market_size > 10000:  # 1ä¸‡-5ä¸‡
            return "small"
        else:  # 1ä¸‡ä»¥ä¸‹
            return "niche"

    def _analyze_competition(self, opportunity_data: Dict[str, Any]):
        """åˆ†æç«äº‰æƒ…å†µ"""
        try:
            opportunity_name = opportunity_data.get("opportunity_name", "").lower()
            description = opportunity_data.get("description", "").lower()
            target_users = opportunity_data.get("target_users", "").lower()

            # ç«äº‰å¯¹æ‰‹å…³é”®è¯ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
            competitor_keywords = {
                "automation": ["zapier", "ifttt", "integromat", "make.com"],
                "data_analysis": ["tableau", "power bi", "looker", "metabase"],
                "project_management": ["jira", "trello", "asana", "monday.com"],
                "documentation": ["notion", "confluence", "obsidian", "roam research"],
                "api_tools": ["postman", "insomnia", "swagger", "openapi"],
                "monitoring": ["datadog", "new relic", "grafana", "prometheus"],
                "testing": ["jest", "cypress", "selenium", "playwright"],
                "development": ["vs code", "github", "gitlab", "intellij"],
                "communication": ["slack", "discord", "teams", "zoom"]
            }

            # æ£€æµ‹ç«äº‰å¯¹æ‰‹
            detected_competitors = []
            for category, competitors in competitor_keywords.items():
                for competitor in competitors:
                    if (competitor in opportunity_name or
                        competitor in description or
                        competitor in target_users):
                        detected_competitors.append({
                            "name": competitor,
                            "category": category
                        })

            # ç«äº‰å¼ºåº¦è¯„ä¼°
            if len(detected_competitors) == 0:
                competition_level = "low"
                competition_score = 2  # 1-10åˆ†ï¼Œè¶Šä½è¶Šå¥½
            elif len(detected_competitors) <= 2:
                competition_level = "medium"
                competition_score = 5
            else:
                competition_level = "high"
                competition_score = 8

            opportunity_data["competition_analysis"] = {
                "detected_competitors": detected_competitors,
                "competition_level": competition_level,
                "competition_score": competition_score,
                "differentiation_opportunity": self._identify_differentiation_opportunity(opportunity_data, detected_competitors)
            }

        except Exception as e:
            logger.error(f"Failed to analyze competition: {e}")

    def _identify_differentiation_opportunity(self, opportunity_data: Dict[str, Any], competitors: List[Dict[str, Any]]) -> str:
        """è¯†åˆ«å·®å¼‚åŒ–æœºä¼š"""
        try:
            # ç®€å•çš„å·®å¼‚åŒ–åˆ†æ
            if not competitors:
                return "No direct competitors detected"

            opportunity_name = opportunity_data.get("opportunity_name", "").lower()

            # æ£€æŸ¥æ˜¯å¦æœ‰ç»†åˆ†å¸‚åœºæœºä¼š
            niche_indicators = ["for startups", "for indie", "for solo", "for small", "simple", "lightweight", "minimal"]
            for indicator in niche_indicators:
                if indicator in opportunity_name:
                    return f"Niche focus on {indicator}"

            return "Generic space, needs clear differentiation"

        except Exception as e:
            logger.error(f"Failed to identify differentiation opportunity: {e}")
            return "Unable to determine"

    def _score_with_llm(self, opportunity_data: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """ä½¿ç”¨LLMè¿›è¡Œå¯è¡Œæ€§è¯„åˆ†"""
        try:
            # æ„å»ºæœºä¼šæè¿°æ–‡æœ¬
            description = f"""
Opportunity: {opportunity_data.get('opportunity_name', '')}

Description: {opportunity_data.get('description', '')}

Target Users: {opportunity_data.get('target_users', '')}

Current Tools: {opportunity_data.get('current_tools', '')}

Missing Capability: {opportunity_data.get('missing_capability', '')}

Why Existing Tools Fail: {opportunity_data.get('why_existing_fail', '')}

Market Analysis: {opportunity_data.get('market_analysis', {})}

Competition Analysis: {opportunity_data.get('competition_analysis', {})}
"""

            # è°ƒç”¨LLMè¿›è¡Œè¯„åˆ†
            response = llm_client.score_viability(description)

            scoring_result = response["content"]

            return scoring_result

        except Exception as e:
            logger.error(f"Failed to score with LLM: {e}")
            return None

    def _combine_scores(self, llm_scores: Dict[str, Any], opportunity_data: Dict[str, Any]) -> Dict[str, Any]:
        """ç»“åˆLLMè¯„åˆ†å’Œè§„åˆ™è¯„åˆ†"""
        try:
            # LLMè¯„åˆ†
            llm_component_scores = llm_scores.get("scores", {})
            llm_total_score = llm_scores.get("total_score", 0.0)

            # è§„åˆ™è¯„åˆ†
            market_analysis = opportunity_data.get("market_analysis", {})
            competition_analysis = opportunity_data.get("competition_analysis", {})

            # å¸‚åœºè§„æ¨¡è¯„åˆ† (0-10)
            market_tier = market_analysis.get("market_tier", "niche")
            market_score_by_tier = {
                "large": 9,
                "medium": 7,
                "small": 5,
                "niche": 3
            }
            market_score = market_score_by_tier.get(market_tier, 3)

            # ç«äº‰è¯„åˆ† (0-10, è¶Šä½è¶Šå¥½)
            competition_score = competition_analysis.get("competition_score", 8)
            competition_normalized = max(10 - competition_score, 1)  # è½¬æ¢ä¸ºè¶Šé«˜è¶Šå¥½

            # èšç±»å¤§å°è¯„åˆ† (0-10)
            cluster_info = opportunity_data.get("cluster_info", {})
            cluster_size = cluster_info.get("cluster_size", 0)
            cluster_score = min(cluster_size, 10)  # æ¯ä¸ªäº‹ä»¶1åˆ†ï¼Œæœ€å¤š10åˆ†

            # å·¥ä½œæµç½®ä¿¡åº¦è¯„åˆ† (0-10)
            workflow_confidence = cluster_info.get("workflow_confidence", 0.0)
            workflow_score = workflow_confidence * 10

            # ç»¼åˆè¯„åˆ†è®¡ç®—
            final_component_scores = {
                "pain_frequency": llm_component_scores.get("pain_frequency", 5),
                "clear_buyer": llm_component_scores.get("clear_buyer", 5),
                "mvp_buildable": llm_component_scores.get("mvp_buildable", 5),
                "crowded_market": competition_normalized,
                "integration": llm_component_scores.get("integration", 5),
                "market_size": market_score,
                "cluster_strength": cluster_score,
                "workflow_confidence": workflow_score
            }

            # è®¡ç®—åŠ æƒæ€»åˆ†
            weights = {
                "pain_frequency": 0.15,
                "clear_buyer": 0.15,
                "mvp_buildable": 0.20,
                "crowded_market": 0.15,
                "integration": 0.10,
                "market_size": 0.10,
                "cluster_strength": 0.10,
                "workflow_confidence": 0.05
            }

            weighted_total = sum(
                final_component_scores[component] * weight
                for component, weight in weights.items()
            )

            # ç¡®ä¿åˆ†æ•°åœ¨0-10èŒƒå›´å†…
            final_total_score = min(max(weighted_total, 0), 10)

            # ç”Ÿæˆæ€æ‰‹é£é™©
            killer_risks = self._generate_killer_risks(final_component_scores, opportunity_data)

            return {
                "component_scores": final_component_scores,
                "total_score": final_total_score,
                "llm_total_score": llm_total_score,
                "killer_risks": killer_risks,
                "recommendation": self._generate_recommendation(final_total_score, killer_risks)
            }

        except Exception as e:
            logger.error(f"Failed to combine scores: {e}")
            return {"total_score": 0.0, "component_scores": {}, "killer_risks": [], "recommendation": "Error in scoring"}

    def _generate_killer_risks(self, component_scores: Dict[str, Any], opportunity_data: Dict[str, Any]) -> List[str]:
        """ç”Ÿæˆæ€æ‰‹é£é™©"""
        risks = []

        # åŸºäºåˆ†é¡¹è¯„åˆ†ç”Ÿæˆé£é™©
        if component_scores.get("market_size", 0) < 4:
            risks.append("Small market size may not sustain business")

        if component_scores.get("crowded_market", 0) < 4:
            risks.append("Highly competitive market with established players")

        if component_scores.get("mvp_buildable", 0) < 4:
            risks.append("Technical complexity too high for solo founder")

        if component_scores.get("clear_buyer", 0) < 4:
            risks.append("Unclear who will pay for this solution")

        if component_scores.get("pain_frequency", 0) < 4:
            risks.append("Problem may not be frequent enough to drive adoption")

        if component_scores.get("integration", 0) < 4:
            risks.append("Difficult integration with existing workflows")

        # åŸºäºç«äº‰åˆ†æç”Ÿæˆé£é™©
        competition_analysis = opportunity_data.get("competition_analysis", {})
        if competition_analysis.get("competition_level") == "high":
            risks.append("Direct competition with well-funded incumbents")

        # åŸºäºå¸‚åœºåˆ†æç”Ÿæˆé£é™©
        market_analysis = opportunity_data.get("market_analysis", {})
        if market_analysis.get("market_tier") == "niche":
            risks.append("Very niche market may limit growth potential")

        return risks[:3]  # æœ€å¤šè¿”å›3ä¸ªé£é™©

    def _generate_recommendation(self, total_score: float, killer_risks: List[str]) -> str:
        """ç”Ÿæˆå»ºè®®"""
        if total_score >= 8.0:
            return "pursue - Strong opportunity with high potential"
        elif total_score >= 6.5:
            return "pursue - Good opportunity with manageable risks"
        elif total_score >= 5.0:
            return "modify - Viable with some adjustments needed"
        elif total_score >= 3.5:
            return "research - Needs more validation before pursuing"
        else:
            return "abandon - Too many risks or unclear value proposition"

    def _update_opportunity_in_database(self, opportunity_id: int, scoring_result: Dict[str, Any]) -> bool:
        """æ›´æ–°æ•°æ®åº“ä¸­çš„æœºä¼šè¯„åˆ†"""
        try:
            with db.get_connection("clusters") as conn:
                conn.execute("""
                    UPDATE opportunities
                    SET pain_frequency_score = ?,
                        market_size_score = ?,
                        mvp_complexity_score = ?,
                        competition_risk_score = ?,
                        integration_complexity_score = ?,
                        total_score = ?,
                        killer_risks = ?,
                        recommendation = ?
                    WHERE id = ?
                """, (
                    scoring_result["component_scores"].get("pain_frequency", 0),
                    scoring_result["component_scores"].get("market_size", 0),
                    scoring_result["component_scores"].get("mvp_buildable", 0),
                    scoring_result["component_scores"].get("crowded_market", 0),
                    scoring_result["component_scores"].get("integration", 0),
                    scoring_result["total_score"],
                    json.dumps(scoring_result["killer_risks"]),
                    scoring_result.get("recommendation", ""),
                    opportunity_id
                ))
                conn.commit()
                return True

        except Exception as e:
            logger.error(f"Failed to update opportunity in database: {e}")
            return False

    def score_opportunities(self, limit: int = 100) -> Dict[str, Any]:
        """ä¸ºæœºä¼šè¯„åˆ†"""
        logger.info(f"Scoring up to {limit} opportunities")

        start_time = time.time()

        try:
            # è·å–æœªè¯„åˆ†çš„æœºä¼š
            with db.get_connection("clusters") as conn:
                cursor = conn.execute("""
                    SELECT * FROM opportunities
                    WHERE total_score = 0 OR total_score IS NULL
                    ORDER BY cluster_id DESC
                    LIMIT ?
                """, (limit,))
                opportunities = [dict(row) for row in cursor.fetchall()]

            if not opportunities:
                logger.info("No unscored opportunities found")
                return {"opportunities_scored": 0, "viable_opportunities": 0}

            logger.info(f"Found {len(opportunities)} opportunities to score")

            scored_opportunities = []
            viable_count = 0
            good_count = 0
            excellent_count = 0

            for i, opportunity in enumerate(opportunities):
                logger.info(f"Scoring opportunity {i+1}/{len(opportunities)}: {opportunity['opportunity_name']}")

                try:
                    # å¢å¼ºæœºä¼šæ•°æ®
                    enhanced_opportunity = self._enhance_opportunity_data(opportunity)

                    # LLMè¯„åˆ†
                    llm_result = self._score_with_llm(enhanced_opportunity)

                    if llm_result:
                        # ç»“åˆè¯„åˆ†
                        final_scores = self._combine_scores(llm_result, enhanced_opportunity)

                        # æ›´æ–°æ•°æ®åº“
                        if self._update_opportunity_in_database(opportunity["id"], final_scores):
                            # ç»Ÿè®¡
                            total_score = final_scores["total_score"]
                            if total_score >= 8.5:
                                excellent_count += 1
                            elif total_score >= 7.0:
                                good_count += 1
                            elif total_score >= 5.0:
                                viable_count += 1

                            opportunity_summary = {
                                "opportunity_id": opportunity["id"],
                                "opportunity_name": opportunity["opportunity_name"],
                                "total_score": total_score,
                                "recommendation": final_scores["recommendation"],
                                "killer_risks": final_scores["killer_risks"]
                            }

                            scored_opportunities.append(opportunity_summary)

                            logger.info(f"Scored: {opportunity['opportunity_name']} - {total_score:.1f}/10 ({final_scores['recommendation']})")
                        else:
                            logger.error(f"Failed to update opportunity {opportunity['id']} in database")

                except Exception as e:
                    logger.error(f"Failed to score opportunity {opportunity['opportunity_name']}: {e}")
                    continue

                # æ·»åŠ å»¶è¿Ÿé¿å…APIé™åˆ¶
                time.sleep(2)

            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            processing_time = time.time() - start_time
            self.stats["total_opportunities_scored"] = len(scored_opportunities)
            self.stats["viable_opportunities"] = viable_count
            self.stats["good_opportunities"] = good_count
            self.stats["excellent_opportunities"] = excellent_count
            self.stats["processing_time"] = processing_time

            if scored_opportunities:
                self.stats["avg_total_score"] = sum(opp["total_score"] for opp in scored_opportunities) / len(scored_opportunities)

            logger.info(f"""
=== Viability Scoring Summary ===
Opportunities scored: {len(scored_opportunities)}
Viable opportunities (5.0+): {viable_count}
Good opportunities (7.0+): {good_count}
Excellent opportunities (8.5+): {excellent_count}
Average score: {self.stats['avg_total_score']:.2f}
Processing time: {processing_time:.2f}s
""")

            return {
                "opportunities_scored": len(scored_opportunities),
                "viable_opportunities": viable_count,
                "good_opportunities": good_count,
                "excellent_opportunities": excellent_count,
                "scored_opportunities": scored_opportunities,
                "scoring_stats": self.get_statistics()
            }

        except Exception as e:
            logger.error(f"Failed to score opportunities: {e}")
            raise

    def get_top_opportunities(self, min_score: float = 5.0, limit: int = 20) -> List[Dict[str, Any]]:
        """è·å–æœ€é«˜åˆ†çš„æœºä¼š"""
        try:
            with db.get_connection("clusters") as conn:
                cursor = conn.execute("""
                    SELECT o.*, c.cluster_name, c.cluster_size
                    FROM opportunities o
                    JOIN clusters c ON o.cluster_id = c.id
                    WHERE o.total_score >= ?
                    ORDER BY o.total_score DESC
                    LIMIT ?
                """, (min_score, limit))
                opportunities = [dict(row) for row in cursor.fetchall()]

            return opportunities

        except Exception as e:
            logger.error(f"Failed to get top opportunities: {e}")
            return []

    def get_statistics(self) -> Dict[str, Any]:
        """è·å–è¯„åˆ†ç»Ÿè®¡ä¿¡æ¯"""
        stats = self.stats.copy()

        if stats["total_opportunities_scored"] > 0:
            stats["viable_rate"] = stats["viable_opportunities"] / stats["total_opportunities_scored"]
            stats["good_rate"] = stats["good_opportunities"] / stats["total_opportunities_scored"]
            stats["excellent_rate"] = stats["excellent_opportunities"] / stats["total_opportunities_scored"]
            stats["processing_rate"] = stats["total_opportunities_scored"] / max(stats["processing_time"], 1)
        else:
            stats["viable_rate"] = 0
            stats["good_rate"] = 0
            stats["excellent_rate"] = 0
            stats["processing_rate"] = 0

        return stats

    def reset_statistics(self):
        """é‡ç½®ç»Ÿè®¡ä¿¡æ¯"""
        self.stats = {
            "total_opportunities_scored": 0,
            "viable_opportunities": 0,
            "good_opportunities": 0,
            "excellent_opportunities": 0,
            "processing_time": 0.0,
            "avg_total_score": 0.0
        }

def main():
    """ä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description="Score opportunity viability for solo founders")
    parser.add_argument("--limit", type=int, default=100, help="Limit number of opportunities to score")
    parser.add_argument("--min-score", type=float, default=5.0, help="Minimum score for top opportunities")
    parser.add_argument("--list", action="store_true", help="List top scored opportunities")
    args = parser.parse_args()

    try:
        logger.info("Starting viability scoring...")

        scorer = ViabilityScorer()

        if args.list:
            # åˆ—å‡ºæœ€é«˜åˆ†çš„æœºä¼š
            top_opportunities = scorer.get_top_opportunities(min_score=args.min_score)
            print(json.dumps(top_opportunities, indent=2, default=str))

        else:
            # ä¸ºæœºä¼šè¯„åˆ†
            result = scorer.score_opportunities(limit=args.limit)

            logger.info(f"""
=== Viability Scoring Complete ===
Opportunities scored: {result['opportunities_scored']}
Viable opportunities: {result['viable_opportunities']}
Good opportunities: {result['good_opportunities']}
Excellent opportunities: {result['excellent_opportunities']}
Scoring stats: {result['scoring_stats']}
""")

    except Exception as e:
        logger.error(f"Fatal error: {e}")
        raise

if __name__ == "__main__":
    main()
```


================================================================================
æ–‡ä»¶: utils/db.py
================================================================================

```python
"""
Database utilities for Reddit Pain Point Finder
SQLiteæ•°æ®åº“æ“ä½œå·¥å…·
"""
import sqlite3
import json
import logging
from datetime import datetime
from typing import List, Dict, Any, Optional
from contextlib import contextmanager
import os

logger = logging.getLogger(__name__)

class PainPointDB:
    """Redditç—›ç‚¹å‘ç°ç³»ç»Ÿæ•°æ®åº“ç®¡ç†å™¨"""

    def __init__(self, db_dir: str = "data"):
        """åˆå§‹åŒ–æ•°æ®åº“è¿æ¥"""
        self.db_dir = db_dir
        os.makedirs(db_dir, exist_ok=True)

        # æ•°æ®åº“æ–‡ä»¶è·¯å¾„
        self.raw_db_path = os.path.join(db_dir, "raw_posts.db")
        self.filtered_db_path = os.path.join(db_dir, "filtered_posts.db")
        self.pain_db_path = os.path.join(db_dir, "pain_events.db")
        self.clusters_db_path = os.path.join(db_dir, "clusters.db")

        # åˆå§‹åŒ–æ‰€æœ‰æ•°æ®åº“
        self._init_databases()

    @contextmanager
    def get_connection(self, db_type: str = "raw"):
        """è·å–æ•°æ®åº“è¿æ¥çš„ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
        db_paths = {
            "raw": self.raw_db_path,
            "filtered": self.filtered_db_path,
            "pain": self.pain_db_path,
            "clusters": self.clusters_db_path
        }

        if db_type not in db_paths:
            raise ValueError(f"Invalid db_type: {db_type}")

        conn = None
        try:
            conn = sqlite3.connect(db_paths[db_type])
            conn.row_factory = sqlite3.Row
            yield conn
        except Exception as e:
            if conn:
                conn.rollback()
            logger.error(f"Database error: {e}")
            raise
        finally:
            if conn:
                conn.close()

    def _init_databases(self):
        """åˆå§‹åŒ–æ‰€æœ‰æ•°æ®åº“è¡¨ç»“æ„"""
        self._init_raw_posts_db()
        self._init_filtered_posts_db()
        self._init_pain_events_db()
        self._init_clusters_db()

    def _init_raw_posts_db(self):
        """åˆå§‹åŒ–åŸå§‹å¸–å­æ•°æ®åº“"""
        with self.get_connection("raw") as conn:
            conn.execute("""
                CREATE TABLE IF NOT EXISTS posts (
                    id TEXT PRIMARY KEY,
                    title TEXT NOT NULL,
                    body TEXT,
                    subreddit TEXT NOT NULL,
                    url TEXT NOT NULL,
                    score INTEGER NOT NULL,
                    num_comments INTEGER NOT NULL,
                    upvote_ratio REAL NOT NULL,
                    is_self INTEGER NOT NULL,
                    created_utc REAL NOT NULL,
                    author TEXT,
                    category TEXT,
                    collected_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    raw_data TEXT  -- åŸå§‹JSONæ•°æ®
                )
            """)

            # åˆ›å»ºç´¢å¼•
            conn.execute("CREATE INDEX IF NOT EXISTS idx_posts_subreddit ON posts(subreddit)")
            conn.execute("CREATE INDEX IF NOT EXISTS idx_posts_score ON posts(score)")
            conn.execute("CREATE INDEX IF NOT EXISTS idx_posts_collected_at ON posts(collected_at)")
            conn.commit()

    def _init_filtered_posts_db(self):
        """åˆå§‹åŒ–è¿‡æ»¤åçš„å¸–å­æ•°æ®åº“"""
        with self.get_connection("filtered") as conn:
            conn.execute("""
                CREATE TABLE IF NOT EXISTS filtered_posts (
                    id TEXT PRIMARY KEY,
                    title TEXT NOT NULL,
                    body TEXT,
                    subreddit TEXT NOT NULL,
                    url TEXT NOT NULL,
                    score INTEGER NOT NULL,
                    num_comments INTEGER NOT NULL,
                    upvote_ratio REAL NOT NULL,
                    pain_score REAL NOT NULL,
                    pain_keywords TEXT,
                    filtered_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    filter_reason TEXT
                )
            """)

            # åˆ›å»ºç´¢å¼•
            conn.execute("CREATE INDEX IF NOT EXISTS idx_filtered_pain_score ON filtered_posts(pain_score)")
            conn.execute("CREATE INDEX IF NOT EXISTS idx_filtered_subreddit ON filtered_posts(subreddit)")
            conn.execute("CREATE INDEX IF NOT EXISTS idx_filtered_at ON filtered_posts(filtered_at)")
            conn.commit()

    def _init_pain_events_db(self):
        """åˆå§‹åŒ–ç—›ç‚¹äº‹ä»¶æ•°æ®åº“"""
        with self.get_connection("pain") as conn:
            conn.execute("""
                CREATE TABLE IF NOT EXISTS pain_events (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    post_id TEXT NOT NULL,
                    actor TEXT,
                    context TEXT,
                    problem TEXT NOT NULL,
                    current_workaround TEXT,
                    frequency TEXT,
                    emotional_signal TEXT,
                    mentioned_tools TEXT,
                    extraction_confidence REAL,
                    extracted_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    FOREIGN KEY (post_id) REFERENCES filtered_posts(id)
                )
            """)

            # åˆ›å»ºåµŒå…¥å‘é‡è¡¨
            conn.execute("""
                CREATE TABLE IF NOT EXISTS pain_embeddings (
                    pain_event_id INTEGER PRIMARY KEY,
                    embedding_vector BLOB NOT NULL,
                    embedding_model TEXT NOT NULL,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    FOREIGN KEY (pain_event_id) REFERENCES pain_events(id)
                )
            """)

            # åˆ›å»ºç´¢å¼•
            conn.execute("CREATE INDEX IF NOT EXISTS idx_pain_post_id ON pain_events(post_id)")
            conn.execute("CREATE INDEX IF NOT EXISTS idx_pain_problem ON pain_events(problem)")
            conn.execute("CREATE INDEX IF NOT EXISTS idx_pain_extracted_at ON pain_events(extracted_at)")
            conn.commit()

    def _init_clusters_db(self):
        """åˆå§‹åŒ–èšç±»æ•°æ®åº“"""
        with self.get_connection("clusters") as conn:
            conn.execute("""
                CREATE TABLE IF NOT EXISTS clusters (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    cluster_name TEXT NOT NULL,
                    cluster_description TEXT,
                    pain_event_ids TEXT NOT NULL,  -- JSONæ•°ç»„
                    cluster_size INTEGER NOT NULL,
                    avg_pain_score REAL,
                    workflow_confidence REAL,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            """)

            # åˆ›å»ºæœºä¼šè¡¨
            conn.execute("""
                CREATE TABLE IF NOT EXISTS opportunities (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    cluster_id INTEGER NOT NULL,
                    opportunity_name TEXT NOT NULL,
                    description TEXT NOT NULL,
                    current_tools TEXT,
                    missing_capability TEXT,
                    why_existing_fail TEXT,
                    target_users TEXT,
                    pain_frequency_score REAL,
                    market_size_score REAL,
                    mvp_complexity_score REAL,
                    competition_risk_score REAL,
                    integration_complexity_score REAL,
                    total_score REAL,
                    killer_risks TEXT,  -- JSONæ•°ç»„
                    recommendation TEXT,  -- AIå»ºè®®ï¼špursue/modify/abandon with reason
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    FOREIGN KEY (cluster_id) REFERENCES clusters(id)
                )
            """)

            # åˆ›å»ºç´¢å¼•
            conn.execute("CREATE INDEX IF NOT EXISTS idx_clusters_size ON clusters(cluster_size)")
            conn.execute("CREATE INDEX IF NOT EXISTS idx_opportunities_score ON opportunities(total_score)")
            conn.execute("CREATE INDEX IF NOT EXISTS idx_opportunities_cluster_id ON opportunities(cluster_id)")
            conn.commit()

    # Raw posts operations
    def insert_raw_post(self, post_data: Dict[str, Any]) -> bool:
        """æ’å…¥åŸå§‹å¸–å­æ•°æ®"""
        try:
            with self.get_connection("raw") as conn:
                conn.execute("""
                    INSERT OR REPLACE INTO posts
                    (id, title, body, subreddit, url, score, num_comments,
                     upvote_ratio, is_self, created_utc, author, category, raw_data)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                """, (
                    post_data["id"],
                    post_data["title"],
                    post_data.get("body", ""),
                    post_data["subreddit"],
                    post_data["url"],
                    post_data["score"],
                    post_data["num_comments"],
                    post_data.get("upvote_ratio", 0.0),
                    int(post_data.get("is_self", False)),
                    post_data.get("created_utc", 0),
                    post_data.get("author", ""),
                    post_data.get("category", ""),
                    json.dumps(post_data)
                ))
                conn.commit()
                return True
        except Exception as e:
            logger.error(f"Failed to insert raw post {post_data.get('id')}: {e}")
            return False

    def get_unprocessed_posts(self, limit: int = 100) -> List[Dict]:
        """è·å–æœªå¤„ç†çš„å¸–å­"""
        try:
            # é¦–å…ˆè·å–æ‰€æœ‰å·²å¤„ç†çš„å¸–å­ID
            with self.get_connection("filtered") as conn:
                cursor = conn.execute("SELECT id FROM filtered_posts")
                processed_ids = {row['id'] for row in cursor.fetchall()}

            # ç„¶åè·å–æœªå¤„ç†çš„å¸–å­
            with self.get_connection("raw") as conn:
                if processed_ids:
                    # å¦‚æœæœ‰å·²å¤„ç†çš„å¸–å­ï¼Œæ’é™¤å®ƒä»¬
                    placeholders = ','.join('?' * len(processed_ids))
                    cursor = conn.execute(f"""
                        SELECT * FROM posts
                        WHERE id NOT IN ({placeholders})
                        ORDER BY collected_at DESC
                        LIMIT ?
                    """, list(processed_ids) + [limit])
                else:
                    # å¦‚æœæ²¡æœ‰å·²å¤„ç†çš„å¸–å­ï¼Œç›´æ¥è·å–
                    cursor = conn.execute("""
                        SELECT * FROM posts
                        ORDER BY collected_at DESC
                        LIMIT ?
                    """, (limit,))
                return [dict(row) for row in cursor.fetchall()]
        except Exception as e:
            logger.error(f"Failed to get unprocessed posts: {e}")
            return []

    # Filtered posts operations
    def insert_filtered_post(self, post_data: Dict[str, Any]) -> bool:
        """æ’å…¥è¿‡æ»¤åçš„å¸–å­"""
        try:
            with self.get_connection("filtered") as conn:
                conn.execute("""
                    INSERT OR REPLACE INTO filtered_posts
                    (id, title, body, subreddit, url, score, num_comments,
                     upvote_ratio, pain_score, pain_keywords, filter_reason)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                """, (
                    post_data["id"],
                    post_data["title"],
                    post_data.get("body", ""),
                    post_data["subreddit"],
                    post_data["url"],
                    post_data["score"],
                    post_data["num_comments"],
                    post_data.get("upvote_ratio", 0.0),
                    post_data.get("pain_score", 0.0),
                    json.dumps(post_data.get("pain_keywords", [])),
                    post_data.get("filter_reason", "")
                ))
                conn.commit()
                return True
        except Exception as e:
            logger.error(f"Failed to insert filtered post {post_data.get('id')}: {e}")
            return False

    def get_filtered_posts(self, limit: int = 100, min_pain_score: float = 0.0) -> List[Dict]:
        """è·å–è¿‡æ»¤åçš„å¸–å­"""
        try:
            # é¦–å…ˆè·å–æ‰€æœ‰å·²æå–çš„å¸–å­ID
            with self.get_connection("pain") as conn:
                cursor = conn.execute("SELECT DISTINCT post_id FROM pain_events")
                extracted_ids = {row['post_id'] for row in cursor.fetchall()}

            # ç„¶åè·å–è¿‡æ»¤åçš„å¸–å­
            with self.get_connection("filtered") as conn:
                if extracted_ids:
                    # å¦‚æœæœ‰å·²æå–çš„å¸–å­ï¼Œæ’é™¤å®ƒä»¬
                    placeholders = ','.join('?' * len(extracted_ids))
                    cursor = conn.execute(f"""
                        SELECT * FROM filtered_posts
                        WHERE pain_score >= ?
                        AND id NOT IN ({placeholders})
                        ORDER BY pain_score DESC
                        LIMIT ?
                    """, [min_pain_score] + list(extracted_ids) + [limit])
                else:
                    # å¦‚æœæ²¡æœ‰å·²æå–çš„å¸–å­ï¼Œç›´æ¥è·å–
                    cursor = conn.execute("""
                        SELECT * FROM filtered_posts
                        WHERE pain_score >= ?
                        ORDER BY pain_score DESC
                        LIMIT ?
                    """, (min_pain_score, limit))
                return [dict(row) for row in cursor.fetchall()]
        except Exception as e:
            logger.error(f"Failed to get filtered posts: {e}")
            return []

    # Pain events operations
    def insert_pain_event(self, pain_data: Dict[str, Any]) -> Optional[int]:
        """æ’å…¥ç—›ç‚¹äº‹ä»¶"""
        try:
            with self.get_connection("pain") as conn:
                cursor = conn.execute("""
                    INSERT INTO pain_events
                    (post_id, actor, context, problem, current_workaround,
                     frequency, emotional_signal, mentioned_tools, extraction_confidence)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
                """, (
                    pain_data["post_id"],
                    pain_data.get("actor", ""),
                    pain_data.get("context", ""),
                    pain_data["problem"],
                    pain_data.get("current_workaround", ""),
                    pain_data.get("frequency", ""),
                    pain_data.get("emotional_signal", ""),
                    json.dumps(pain_data.get("mentioned_tools", [])),
                    pain_data.get("extraction_confidence", 0.0)
                ))
                pain_event_id = cursor.lastrowid
                conn.commit()
                return pain_event_id
        except Exception as e:
            logger.error(f"Failed to insert pain event: {e}")
            return None

    def insert_pain_embedding(self, pain_event_id: int, embedding_vector: List[float], model_name: str) -> bool:
        """æ’å…¥ç—›ç‚¹åµŒå…¥å‘é‡"""
        try:
            import pickle
            embedding_blob = pickle.dumps(embedding_vector)

            with self.get_connection("pain") as conn:
                conn.execute("""
                    INSERT OR REPLACE INTO pain_embeddings
                    (pain_event_id, embedding_vector, embedding_model)
                    VALUES (?, ?, ?)
                """, (pain_event_id, embedding_blob, model_name))
                conn.commit()
                return True
        except Exception as e:
            logger.error(f"Failed to insert pain embedding for event {pain_event_id}: {e}")
            return False

    def get_pain_events_without_embeddings(self, limit: int = 100) -> List[Dict]:
        """è·å–æ²¡æœ‰åµŒå…¥å‘é‡çš„ç—›ç‚¹äº‹ä»¶"""
        try:
            with self.get_connection("pain") as conn:
                cursor = conn.execute("""
                    SELECT p.* FROM pain_events p
                    LEFT JOIN pain_embeddings e ON p.id = e.pain_event_id
                    WHERE e.pain_event_id IS NULL
                    ORDER BY p.extracted_at DESC
                    LIMIT ?
                """, (limit,))
                return [dict(row) for row in cursor.fetchall()]
        except Exception as e:
            logger.error(f"Failed to get pain events without embeddings: {e}")
            return []

    def get_all_pain_events_with_embeddings(self) -> List[Dict]:
        """è·å–æ‰€æœ‰æœ‰åµŒå…¥å‘é‡çš„ç—›ç‚¹äº‹ä»¶"""
        try:
            import pickle
            with self.get_connection("pain") as conn:
                cursor = conn.execute("""
                    SELECT p.*, e.embedding_vector, e.embedding_model
                    FROM pain_events p
                    JOIN pain_embeddings e ON p.id = e.pain_event_id
                    ORDER BY p.extracted_at DESC
                """)
                results = []
                for row in cursor.fetchall():
                    event_data = dict(row)
                    # ååºåˆ—åŒ–åµŒå…¥å‘é‡
                    if event_data["embedding_vector"]:
                        event_data["embedding_vector"] = pickle.loads(event_data["embedding_vector"])
                    results.append(event_data)
                return results
        except Exception as e:
            logger.error(f"Failed to get pain events with embeddings: {e}")
            return []

    # Clusters operations
    def insert_cluster(self, cluster_data: Dict[str, Any]) -> Optional[int]:
        """æ’å…¥èšç±»"""
        try:
            with self.get_connection("clusters") as conn:
                cursor = conn.execute("""
                    INSERT INTO clusters
                    (cluster_name, cluster_description, pain_event_ids, cluster_size,
                     avg_pain_score, workflow_confidence)
                    VALUES (?, ?, ?, ?, ?, ?)
                """, (
                    cluster_data["cluster_name"],
                    cluster_data.get("cluster_description", ""),
                    json.dumps(cluster_data["pain_event_ids"]),
                    cluster_data["cluster_size"],
                    cluster_data.get("avg_pain_score", 0.0),
                    cluster_data.get("workflow_confidence", 0.0)
                ))
                cluster_id = cursor.lastrowid
                conn.commit()
                return cluster_id
        except Exception as e:
            logger.error(f"Failed to insert cluster: {e}")
            return None

    def insert_opportunity(self, opportunity_data: Dict[str, Any]) -> Optional[int]:
        """æ’å…¥æœºä¼š"""
        try:
            with self.get_connection("clusters") as conn:
                cursor = conn.execute("""
                    INSERT INTO opportunities
                    (cluster_id, opportunity_name, description, current_tools,
                     missing_capability, why_existing_fail, target_users,
                     pain_frequency_score, market_size_score, mvp_complexity_score,
                     competition_risk_score, integration_complexity_score, total_score, killer_risks, recommendation)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                """, (
                    opportunity_data["cluster_id"],
                    opportunity_data["opportunity_name"],
                    opportunity_data["description"],
                    opportunity_data.get("current_tools", ""),
                    opportunity_data.get("missing_capability", ""),
                    opportunity_data.get("why_existing_fail", ""),
                    opportunity_data.get("target_users", ""),
                    opportunity_data.get("pain_frequency_score", 0.0),
                    opportunity_data.get("market_size_score", 0.0),
                    opportunity_data.get("mvp_complexity_score", 0.0),
                    opportunity_data.get("competition_risk_score", 0.0),
                    opportunity_data.get("integration_complexity_score", 0.0),
                    opportunity_data.get("total_score", 0.0),
                    json.dumps(opportunity_data.get("killer_risks", [])),
                    opportunity_data.get("recommendation", "")
                ))
                opportunity_id = cursor.lastrowid
                conn.commit()
                return opportunity_id
        except Exception as e:
            logger.error(f"Failed to insert opportunity: {e}")
            return None

    def get_top_opportunities(self, limit: int = 20) -> List[Dict]:
        """è·å–æœ€é«˜åˆ†çš„æœºä¼š"""
        try:
            with self.get_connection("clusters") as conn:
                cursor = conn.execute("""
                    SELECT o.*, c.cluster_name, c.cluster_description
                    FROM opportunities o
                    JOIN clusters c ON o.cluster_id = c.id
                    ORDER BY o.total_score DESC
                    LIMIT ?
                """, (limit,))
                return [dict(row) for row in cursor.fetchall()]
        except Exception as e:
            logger.error(f"Failed to get top opportunities: {e}")
            return []

    # Statistics operations
    def get_statistics(self) -> Dict[str, Any]:
        """è·å–æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯"""
        stats = {}

        try:
            # Raw posts count
            with self.get_connection("raw") as conn:
                cursor = conn.execute("SELECT COUNT(*) as count FROM posts")
                stats["raw_posts_count"] = cursor.fetchone()["count"]

            # Filtered posts count
            with self.get_connection("filtered") as conn:
                cursor = conn.execute("SELECT COUNT(*) as count FROM filtered_posts")
                stats["filtered_posts_count"] = cursor.fetchone()["count"]

                cursor = conn.execute("SELECT AVG(pain_score) as avg_score FROM filtered_posts")
                stats["avg_pain_score"] = cursor.fetchone()["avg_score"] or 0

            # Pain events count
            with self.get_connection("pain") as conn:
                cursor = conn.execute("SELECT COUNT(*) as count FROM pain_events")
                stats["pain_events_count"] = cursor.fetchone()["count"]

            # Clusters count
            with self.get_connection("clusters") as conn:
                cursor = conn.execute("SELECT COUNT(*) as count FROM clusters")
                stats["clusters_count"] = cursor.fetchone()["count"]

                cursor = conn.execute("SELECT COUNT(*) as count FROM opportunities")
                stats["opportunities_count"] = cursor.fetchone()["count"]

        except Exception as e:
            logger.error(f"Failed to get statistics: {e}")

        return stats

# å…¨å±€æ•°æ®åº“å®ä¾‹
db = PainPointDB()
```


================================================================================
æ–‡ä»¶: utils/embedding.py
================================================================================

```python
"""
Embedding utilities for Reddit Pain Point Finder
å‘é‡åŒ–å·¥å…·ï¼Œç”¨äºç—›ç‚¹äº‹ä»¶èšç±»
"""
import os
import logging
import pickle
import numpy as np
from typing import List, Dict, Any, Optional, Tuple
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.cluster import DBSCAN
import yaml
from openai import OpenAI
import backoff

logger = logging.getLogger(__name__)

class EmbeddingClient:
    """åµŒå…¥å‘é‡å®¢æˆ·ç«¯"""

    def __init__(self, config_path: str = "config/llm.yaml"):
        """åˆå§‹åŒ–åµŒå…¥å®¢æˆ·ç«¯"""
        self.config = self._load_config(config_path)
        self.client = self._init_client()
        self.model_name = self._get_model_name()
        self.embedding_cache = {}
        self.stats = {
            "embeddings_created": 0,
            "cache_hits": 0,
            "total_tokens": 0
        }

    def _load_config(self, config_path: str) -> Dict[str, Any]:
        """åŠ è½½é…ç½®"""
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                return yaml.safe_load(f)
        except Exception as e:
            logger.error(f"Failed to load config from {config_path}: {e}")
            raise

    def _init_client(self) -> OpenAI:
        """åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯"""
        api_key = os.getenv(self.config['api']['api_key_env'])
        if not api_key:
            raise ValueError(f"API key not found: {self.config['api']['api_key_env']}")

        return OpenAI(
            api_key=api_key,
            base_url=self.config['api']['base_url']
        )

    def _get_model_name(self) -> str:
        """è·å–åµŒå…¥æ¨¡å‹åç§°"""
        embedding_config = self.config.get("embedding", {})
        env_name = embedding_config.get("env_name")
        if env_name and os.getenv(env_name):
            return os.getenv(env_name)
        return embedding_config.get("model", "text-embedding-ada-002")

    @backoff.on_exception(
        backoff.expo,
        Exception,
        max_tries=3,
        base=1,
        max_value=60
    )
    def create_embedding(self, text: str) -> List[float]:
        """åˆ›å»ºæ–‡æœ¬åµŒå…¥å‘é‡"""
        try:
            # æ£€æŸ¥ç¼“å­˜
            if text in self.embedding_cache:
                self.stats["cache_hits"] += 1
                return self.embedding_cache[text]

            # è°ƒç”¨API
            response = self.client.embeddings.create(
                model=self.model_name,
                input=text
            )

            embedding = response.data[0].embedding

            # æ›´æ–°ç»Ÿè®¡
            self.stats["embeddings_created"] += 1
            self.stats["total_tokens"] += response.usage.total_tokens

            # ç¼“å­˜ç»“æœ
            self.embedding_cache[text] = embedding

            logger.info(f"Created embedding for text length {len(text)}: {len(embedding)} dimensions")
            return embedding

        except Exception as e:
            logger.error(f"Failed to create embedding: {e}")
            raise

    def create_batch_embeddings(self, texts: List[str], batch_size: int = None) -> List[List[float]]:
        """æ‰¹é‡åˆ›å»ºåµŒå…¥å‘é‡"""
        if batch_size is None:
            batch_size = self.config.get("embedding", {}).get("batch_size", 32)

        embeddings = []
        for i in range(0, len(texts), batch_size):
            batch = texts[i:i + batch_size]
            logger.info(f"Processing batch {i//batch_size + 1}/{(len(texts) + batch_size - 1)//batch_size}")

            for text in batch:
                embedding = self.create_embedding(text)
                embeddings.append(embedding)

        return embeddings

    def create_pain_event_embedding(self, pain_event: Dict[str, Any]) -> List[float]:
        """ä¸ºç—›ç‚¹äº‹ä»¶åˆ›å»ºåµŒå…¥å‘é‡"""
        # æ„å»ºåµŒå…¥æ–‡æœ¬ï¼Œé‡ç‚¹å…³æ³¨é—®é¢˜çš„æœ¬è´¨
        text_parts = []

        if pain_event.get("actor"):
            text_parts.append(pain_event["actor"])

        if pain_event.get("context"):
            text_parts.append(pain_event["context"])

        if pain_event.get("problem"):
            text_parts.append(pain_event["problem"])

        if pain_event.get("current_workaround"):
            text_parts.append(pain_event["current_workaround"])

        # ç”¨ " | " è¿æ¥å„ä¸ªéƒ¨åˆ†ï¼Œä¿æŒè¯­ä¹‰ç»“æ„
        embedding_text = " | ".join(text_parts)

        return self.create_embedding(embedding_text)

    def calculate_similarity_matrix(self, embeddings: List[List[float]]) -> np.ndarray:
        """è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ"""
        return cosine_similarity(embeddings)

    def find_similar_events(
        self,
        target_embedding: List[float],
        candidate_embeddings: List[List[float]],
        threshold: float = 0.7,
        top_k: int = 10
    ) -> List[Tuple[int, float]]:
        """æ‰¾åˆ°ç›¸ä¼¼çš„ç—›ç‚¹äº‹ä»¶"""
        similarities = cosine_similarity([target_embedding], candidate_embeddings)[0]

        # ç­›é€‰è¶…è¿‡é˜ˆå€¼çš„ç»“æœ
        results = []
        for idx, similarity in enumerate(similarities):
            if similarity >= threshold:
                results.append((idx, similarity))

        # æŒ‰ç›¸ä¼¼åº¦æ’åºï¼Œè¿”å›top_k
        results.sort(key=lambda x: x[1], reverse=True)
        return results[:top_k]

    def cluster_embeddings(
        self,
        embeddings: List[List[float]],
        eps: float = 0.5,
        min_samples: int = 3
    ) -> Dict[int, List[int]]:
        """ä½¿ç”¨DBSCANèšç±»åµŒå…¥å‘é‡"""
        if len(embeddings) < min_samples:
            return {0: list(range(len(embeddings)))}  # å¦‚æœæ ·æœ¬å¤ªå°‘ï¼Œå½’ä¸ºä¸€ç±»

        dbscan = DBSCAN(eps=eps, min_samples=min_samples, metric='cosine')
        cluster_labels = dbscan.fit_predict(embeddings)

        # æ„å»ºèšç±»å­—å…¸
        clusters = {}
        for idx, label in enumerate(cluster_labels):
            if label == -1:  # å™ªå£°ç‚¹
                continue
            if label not in clusters:
                clusters[label] = []
            clusters[label].append(idx)

        return clusters

    def analyze_cluster(
        self,
        cluster_indices: List[int],
        embeddings: List[List[float]],
        pain_events: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """åˆ†æä¸€ä¸ªèšç±»"""
        if not cluster_indices:
            return {}

        # è®¡ç®—èšç±»ä¸­å¿ƒ
        cluster_embeddings = [embeddings[i] for i in cluster_indices]
        centroid = np.mean(cluster_embeddings, axis=0)

        # è®¡ç®—æ¯ä¸ªç‚¹åˆ°ä¸­å¿ƒçš„è·ç¦»
        distances_to_center = [
            1 - cosine_similarity([embeddings[i]], [centroid])[0][0]
            for i in cluster_indices
        ]

        # è®¡ç®—èšç±»çš„å†…èšæ€§ï¼ˆå¹³å‡è·ç¦»ï¼‰
        cohesion = 1 - np.mean(distances_to_center)

        # è·å–è¯¥èšç±»çš„ç—›ç‚¹äº‹ä»¶
        cluster_events = [pain_events[i] for i in cluster_indices]

        return {
            "size": len(cluster_indices),
            "centroid": centroid.tolist(),
            "cohesion": cohesion,
            "events": cluster_events,
            "avg_distance_to_center": np.mean(distances_to_center),
            "max_distance_to_center": np.max(distances_to_center)
        }

    def get_embedding_statistics(self) -> Dict[str, Any]:
        """è·å–åµŒå…¥ç»Ÿè®¡ä¿¡æ¯"""
        return self.stats.copy()

    def save_embedding_cache(self, cache_path: str):
        """ä¿å­˜åµŒå…¥ç¼“å­˜"""
        try:
            with open(cache_path, 'wb') as f:
                pickle.dump(self.embedding_cache, f)
            logger.info(f"Saved embedding cache to {cache_path}")
        except Exception as e:
            logger.error(f"Failed to save embedding cache: {e}")

    def load_embedding_cache(self, cache_path: str):
        """åŠ è½½åµŒå…¥ç¼“å­˜"""
        try:
            if os.path.exists(cache_path):
                with open(cache_path, 'rb') as f:
                    self.embedding_cache = pickle.load(f)
                logger.info(f"Loaded embedding cache from {cache_path}: {len(self.embedding_cache)} entries")
        except Exception as e:
            logger.error(f"Failed to load embedding cache: {e}")

class PainEventClustering:
    """ç—›ç‚¹äº‹ä»¶èšç±»å·¥å…·"""

    def __init__(self, embedding_client: EmbeddingClient):
        """åˆå§‹åŒ–èšç±»å·¥å…·"""
        self.embedding_client = embedding_client
        self.clustering_config = self._load_clustering_config()

    def _load_clustering_config(self) -> Dict[str, Any]:
        """åŠ è½½èšç±»é…ç½®"""
        try:
            config_path = "config/clustering.yaml"
            with open(config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
            return config
        except Exception as e:
            logger.error(f"Failed to load clustering config: {e}")
            # è¿”å›é»˜è®¤é…ç½®
            return {
                "vector_similarity": {"similarity_threshold": 0.8, "top_k": 10},
                "dbscan": {"eps": 0.3, "min_samples": 2},
                "llm_validation": {"max_events_per_validation": 10, "confidence_threshold": 0.7},
                "post_processing": {"min_cluster_size": 2, "max_cluster_size": 15}
            }

    def cluster_pain_events(self, pain_events: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """èšç±»ç—›ç‚¹äº‹ä»¶"""
        if len(pain_events) < 2:
            return []

        logger.info(f"Clustering {len(pain_events)} pain events")

        # 1. åˆ›å»ºåµŒå…¥å‘é‡
        logger.info("Creating embeddings for pain events...")
        embeddings = []
        for event in pain_events:
            embedding = self.embedding_client.create_pain_event_embedding(event)
            embeddings.append(embedding)

        # 2. ä½¿ç”¨å‘é‡ç›¸ä¼¼åº¦è¿›è¡Œåˆæ­¥èšç±»
        logger.info("Performing vector similarity clustering...")
        similarity_threshold = self.clustering_config.get(
            "vector_similarity", {}
        ).get("similarity_threshold", 0.7)

        dbscan_eps = self.clustering_config.get(
            "dbscan", {}
        ).get("eps", 0.5)
        min_samples = self.clustering_config.get(
            "dbscan", {}
        ).get("min_samples", 3)

        # DBSCANèšç±»
        clusters = self.embedding_client.cluster_embeddings(
            embeddings, eps=dbscan_eps, min_samples=min_samples
        )

        # 3. åˆ†ææ¯ä¸ªèšç±»
        logger.info(f"Found {len(clusters)} clusters")
        cluster_results = []

        for cluster_id, indices in clusters.items():
            if len(indices) < 2:  # è·³è¿‡å•ä¸ªäº‹ä»¶çš„èšç±»
                continue

            cluster_analysis = self.embedding_client.analyze_cluster(
                indices, embeddings, pain_events
            )

            cluster_result = {
                "cluster_id": cluster_id,
                "pain_event_ids": indices,
                "cluster_size": len(indices),
                "cohesion": cluster_analysis["cohesion"],
                "events": cluster_analysis["events"]
            }

            cluster_results.append(cluster_result)

        # æŒ‰èšç±»å¤§å°æ’åº
        cluster_results.sort(key=lambda x: x["cluster_size"], reverse=True)

        logger.info(f"Successfully created {len(cluster_results)} clusters")
        return cluster_results

    def find_similar_events(
        self,
        target_event: Dict[str, Any],
        candidate_events: List[Dict[str, Any]],
        threshold: float = None,
        top_k: int = 10
    ) -> List[Dict[str, Any]]:
        """æ‰¾åˆ°ä¸ç›®æ ‡äº‹ä»¶ç›¸ä¼¼çš„å€™é€‰äº‹ä»¶"""
        if threshold is None:
            threshold = self.clustering_config.get(
                "vector_similarity", {}
            ).get("similarity_threshold", 0.7)

        # åˆ›å»ºç›®æ ‡äº‹ä»¶çš„åµŒå…¥
        target_embedding = self.embedding_client.create_pain_event_embedding(target_event)

        # åˆ›å»ºå€™é€‰äº‹ä»¶çš„åµŒå…¥
        candidate_embeddings = []
        for event in candidate_events:
            embedding = self.embedding_client.create_pain_event_embedding(event)
            candidate_embeddings.append(embedding)

        # æ‰¾åˆ°ç›¸ä¼¼äº‹ä»¶
        similar_indices = self.embedding_client.find_similar_events(
            target_embedding, candidate_embeddings, threshold, top_k
        )

        # è¿”å›ç›¸ä¼¼äº‹ä»¶åŠå…¶ç›¸ä¼¼åº¦
        results = []
        for idx, similarity in similar_indices:
            result = candidate_events[idx].copy()
            result["similarity_score"] = similarity
            results.append(result)

        return results

# å…¨å±€åµŒå…¥å®¢æˆ·ç«¯å®ä¾‹
embedding_client = EmbeddingClient()
pain_clustering = PainEventClustering(embedding_client)
```


================================================================================
æ–‡ä»¶: utils/llm_client.py
================================================================================

```python
"""
LLM Client for Reddit Pain Point Finder
åŸºäºSiliconFlow APIçš„LLMå®¢æˆ·ç«¯
"""
import os
import json
import logging
import time
from typing import Dict, List, Any, Optional, Union
import yaml
from openai import OpenAI
import backoff
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

logger = logging.getLogger(__name__)

class LLMClient:
    """SiliconFlow LLMå®¢æˆ·ç«¯"""

    def __init__(self, config_path: str = "config/llm.yaml"):
        """åˆå§‹åŒ–LLMå®¢æˆ·ç«¯"""
        self.config = self._load_config(config_path)
        self.client = self._init_client()
        self.stats = {
            "requests": 0,
            "tokens_used": 0,
            "cost": 0.0,
            "errors": 0
        }

    def _load_config(self, config_path: str) -> Dict[str, Any]:
        """åŠ è½½LLMé…ç½®"""
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                return yaml.safe_load(f)
        except Exception as e:
            logger.error(f"Failed to load LLM config from {config_path}: {e}")
            raise

    def _init_client(self) -> OpenAI:
        """åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯"""
        api_key = os.getenv(self.config['api']['api_key_env'])
        if not api_key:
            raise ValueError(f"API key not found in environment variable: {self.config['api']['api_key_env']}")

        return OpenAI(
            api_key=api_key,
            base_url=self.config['api']['base_url']
        )

    def get_model_name(self, model_type: str = "main") -> str:
        """è·å–æŒ‡å®šç±»å‹çš„æ¨¡å‹åç§°"""
        if model_type in self.config.get("models", {}):
            model_config = self.config["models"][model_type]
            # å¦‚æœæœ‰ç¯å¢ƒå˜é‡é…ç½®ï¼Œä¼˜å…ˆä½¿ç”¨
            env_name = model_config.get("env_name")
            if env_name and os.getenv(env_name):
                return os.getenv(env_name)
            return model_config["name"]

        # ä»task_mappingä¸­æŸ¥æ‰¾
        task_mapping = self.config.get("task_mapping", {})
        if model_type in task_mapping:
            mapped_model = task_mapping[model_type]["model"]
            return self.get_model_name(mapped_model)

        # é»˜è®¤è¿”å›mainæ¨¡å‹
        return self.config["models"]["main"]["name"]

    def get_model_config(self, model_type: str = "main") -> Dict[str, Any]:
        """è·å–æ¨¡å‹é…ç½®"""
        # ä»task_mappingä¸­æŸ¥æ‰¾
        task_mapping = self.config.get("task_mapping", {})
        if model_type in task_mapping:
            mapped_model = task_mapping[model_type]["model"]
            base_config = self.config["models"][mapped_model].copy()
            # è¦†ç›–ä»»åŠ¡ç‰¹å®šé…ç½®
            base_config.update(task_mapping[model_type])
            return base_config

        # ç›´æ¥ä»modelsä¸­æŸ¥æ‰¾
        if model_type in self.config.get("models", {}):
            return self.config["models"][model_type].copy()

        # é»˜è®¤è¿”å›mainæ¨¡å‹é…ç½®
        return self.config["models"]["main"].copy()

    @backoff.on_exception(
        backoff.expo,
        Exception,
        max_tries=3,
        base=1,
        max_value=60
    )
    def chat_completion(
        self,
        messages: List[Dict[str, str]],
        model_type: str = "main",
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
        json_mode: bool = False
    ) -> Dict[str, Any]:
        """èŠå¤©è¡¥å…¨è¯·æ±‚"""
        try:
            model_config = self.get_model_config(model_type)
            model_name = self.get_model_name(model_type)

            # å‚æ•°é…ç½®
            params = {
                "model": model_name,
                "messages": messages,
                "temperature": temperature if temperature is not None else model_config.get("temperature", 0.1),
                "max_tokens": max_tokens if max_tokens is not None else model_config.get("max_tokens", 2000),
                "timeout": model_config.get("timeout", 30)
            }

            # JSONæ¨¡å¼
            if json_mode:
                params["response_format"] = {"type": "json_object"}

            # è®°å½•è¯·æ±‚å¼€å§‹æ—¶é—´
            start_time = time.time()

            # å‘é€è¯·æ±‚
            response = self.client.chat.completions.create(**params)

            # è®¡ç®—è¯·æ±‚æ—¶é—´
            request_time = time.time() - start_time

            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            self.stats["requests"] += 1
            if hasattr(response.usage, 'total_tokens'):
                self.stats["tokens_used"] += response.usage.total_tokens

            # æå–å“åº”å†…å®¹
            content = response.choices[0].message.content

            # å¦‚æœæ˜¯JSONæ¨¡å¼ï¼Œå°è¯•è§£æ
            if json_mode:
                try:
                    content = json.loads(content)
                except json.JSONDecodeError as e:
                    logger.error(f"Failed to parse JSON response: {e}")
                    logger.error(f"Raw content: {content}")
                    # å°è¯•ä¿®å¤JSON
                    content = self._try_fix_json(content)

            result = {
                "content": content,
                "model": model_name,
                "usage": {
                    "prompt_tokens": response.usage.prompt_tokens if response.usage else 0,
                    "completion_tokens": response.usage.completion_tokens if response.usage else 0,
                    "total_tokens": response.usage.total_tokens if response.usage else 0
                },
                "request_time": request_time
            }

            logger.info(f"LLM request completed: {result['usage']['total_tokens']} tokens in {request_time:.2f}s")
            return result

        except Exception as e:
            self.stats["errors"] += 1
            logger.error(f"LLM request failed: {e}")
            raise

    def _try_fix_json(self, content: str) -> Dict[str, Any]:
        """å°è¯•ä¿®å¤æŸåçš„JSON"""
        try:
            # å°è¯•æå–JSONéƒ¨åˆ†
            start_idx = content.find('{')
            end_idx = content.rfind('}') + 1
            if start_idx != -1 and end_idx > start_idx:
                json_str = content[start_idx:end_idx]
                return json.loads(json_str)
            else:
                raise ValueError("No JSON found in response")
        except Exception as e:
            logger.error(f"Failed to fix JSON: {e}")
            return {"error": "Failed to parse JSON", "raw_content": content}

    def extract_pain_points(
        self,
        title: str,
        body: str,
        subreddit: str,
        upvotes: int,
        comments_count: int
    ) -> Dict[str, Any]:
        """ä»Redditå¸–å­ä¸­æå–ç—›ç‚¹"""
        prompt = self._get_pain_extraction_prompt()

        messages = [
            {"role": "system", "content": prompt},
            {"role": "user", "content": f"""
Title: {title}
Body: {body}
Subreddit: {subreddit}
Upvotes: {upvotes}
Comments: {comments_count}
"""}
        ]

        return self.chat_completion(
            messages=messages,
            model_type="pain_extraction",
            json_mode=True
        )

    def cluster_pain_events(
        self,
        pain_events: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """èšç±»ç—›ç‚¹äº‹ä»¶"""
        prompt = self._get_workflow_clustering_prompt()

        # æ„å»ºç—›ç‚¹äº‹ä»¶æ–‡æœ¬
        events_text = "\n\n".join([
            f"Event {i+1}: {event.get('problem', '')} (Context: {event.get('context', '')}, Workaround: {event.get('current_workaround', '')})"
            for i, event in enumerate(pain_events)
        ])

        messages = [
            {"role": "system", "content": prompt},
            {"role": "user", "content": f"Pain events:\n{events_text}"}
        ]

        return self.chat_completion(
            messages=messages,
            model_type="clustering",
            json_mode=True
        )

    def map_opportunity(
        self,
        cluster_summary: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ä»ç—›ç‚¹èšç±»æ˜ å°„æœºä¼š"""
        prompt = self._get_opportunity_mapping_prompt()

        messages = [
            {"role": "system", "content": prompt},
            {"role": "user", "content": f"Pain cluster:\n{json.dumps(cluster_summary, indent=2)}"}
        ]

        return self.chat_completion(
            messages=messages,
            model_type="opportunity_mapping",
            json_mode=True
        )

    def score_viability(
        self,
        opportunity_description: str
    ) -> Dict[str, Any]:
        """è¯„ä¼°æœºä¼šå¯è¡Œæ€§"""
        prompt = self._get_viability_scoring_prompt()

        messages = [
            {"role": "system", "content": prompt},
            {"role": "user", "content": f"Idea:\n{opportunity_description}"}
        ]

        return self.chat_completion(
            messages=messages,
            model_type="viability_scoring",
            json_mode=True
        )

    def validate_pain_signal(
        self,
        text: str
    ) -> Dict[str, Any]:
        """éªŒè¯ç—›ç‚¹ä¿¡å·"""
        prompt = self._get_signal_validation_prompt()

        messages = [
            {"role": "system", "content": prompt},
            {"role": "user", "content": text}
        ]

        return self.chat_completion(
            messages=messages,
            model_type="signal_validation",
            json_mode=True
        )

    def _get_pain_extraction_prompt(self) -> str:
        """è·å–ç—›ç‚¹æŠ½å–æç¤º"""
        return """You are an information extraction engine.

Your task:
From the following Reddit post, extract concrete PAIN EVENTS.
A pain event is a specific recurring problem experienced by the author,
not opinions, not general complaints.

Rules:
- Do NOT summarize the post
- Do NOT give advice
- If no concrete pain exists, return an empty list
- Be literal and conservative
- Focus on actionable problems people face repeatedly

Output JSON only with this format:
{
  "pain_events": [
    {
      "actor": "who experiences the problem",
      "context": "what they are trying to do",
      "problem": "the concrete difficulty",
      "current_workaround": "how they currently cope (if any)",
      "frequency": "how often it happens (explicit or inferred)",
      "emotional_signal": "frustration, anxiety, exhaustion, etc.",
      "mentioned_tools": ["tool1", "tool2"],
      "confidence": 0.8
    }
  ],
  "extraction_summary": "brief summary of findings"
}

Fields explanation:
- actor: who has this problem (developer, manager, user, etc.)
- context: the situation or workflow where the problem occurs
- problem: specific, concrete issue (not "things are slow" but "compilation takes 30 minutes")
- current_workaround: current solutions people use (if mentioned)
- frequency: how often this happens (daily, weekly, occasionally, etc.)
- emotional_signal: the emotion expressed (frustration, anger, disappointment, etc.)
- mentioned_tools: tools, software, or methods explicitly mentioned
- confidence: how confident you are this is a real pain point (0-1)"""

    def _get_workflow_clustering_prompt(self) -> str:
        """è·å–å·¥ä½œæµèšç±»æç¤º"""
        return """You are analyzing user pain events.

Given the following pain events, determine whether they belong to THE SAME UNDERLYING WORKFLOW problem.

A workflow means:
- the same repeated activity
- where different people fail in similar ways
- with similar root causes

If they belong to the same workflow:
- give the workflow a short descriptive name
- provide a brief description of the workflow
- estimate confidence (0-1)

If they should NOT be clustered:
- say they should not be clustered
- explain why briefly

Return JSON only with this format:
{
  "same_workflow": true/false,
  "workflow_name": "name if same workflow",
  "workflow_description": "description if same workflow",
  "confidence": 0.8,
  "reasoning": "brief explanation"
}

Be conservative - only cluster if they're clearly the same workflow."""

    def _get_opportunity_mapping_prompt(self) -> str:
        """è·å–æœºä¼šæ˜ å°„æç¤º"""
        return """You are a brutally practical product thinker for solo founders.

Given a cluster of pain events that belong to the same workflow:

1. Identify what tools people CURRENTLY use to survive this problem
2. Identify what capability is missing
3. Explain why existing tools fail (too heavy, too generic, etc.)
4. Propose ONE narrow micro-tool opportunity

Rules:
- No platforms (unless you can justify the MVP)
- No marketplaces
- Assume a solo founder building an MVP in 1-3 months
- Focus on specific, painful problems with clear solutions
- If no viable tool opportunity exists, say so

Return JSON only with this format:
{
  "current_tools": ["tool1", "tool2", "manual methods"],
  "missing_capability": "what's missing that would solve this",
  "why_existing_fail": "why current solutions don't work well",
  "opportunity": {
    "name": "short descriptive name",
    "description": "what the micro-tool does",
    "target_users": "who would use this",
    "pain_frequency": "how often this pain occurs (1-10)",
    "market_size": "how many people have this problem (1-10)",
    "mvp_complexity": "how hard to build MVP (1-10, lower is better)",
    "competition_risk": "risk of competitors (1-10, lower is better)",
    "integration_complexity": "how hard to integrate (1-10, lower is better)"
  }
}

Focus on narrow, specific problems that a solo founder can actually solve."""

    def _get_viability_scoring_prompt(self) -> str:
        """è·å–å¯è¡Œæ€§è¯„åˆ†æç¤º"""
        return """You are an experienced solo-founder investor.

Score the following idea for a ONE-PERSON COMPANY.

Criteria:
- Pain frequency: How often does this pain occur? (daily=10, rarely=1)
- Clear buyer: Can we easily identify who would pay? (clear=10, vague=1)
- MVP buildable: Can one person build MVP in 1-3 months? (easy=10, hard=1)
- Crowded market: How competitive is this space? (empty=10, saturated=1)
- Integration: How easy to integrate with existing tools? (easy=10, hard=1)

Score each criteria 0-10, then calculate total score.

Also list the TOP 3 killer risks that could kill this project.

Return JSON only with this format:
{
  "scores": {
    "pain_frequency": 8,
    "clear_buyer": 7,
    "mvp_buildable": 6,
    "crowded_market": 5,
    "integration": 7
  },
  "total_score": 6.6,
  "killer_risks": [
    "Risk 1: specific and concrete",
    "Risk 2: specific and concrete",
    "Risk 3: specific and concrete"
  ],
  "recommendation": "pursue/modify/abandon with brief reason"
}

Be realistic and conservative in scoring."""

    def _get_signal_validation_prompt(self) -> str:
        """è·å–ä¿¡å·éªŒè¯æç¤º"""
        return """You are a pain signal validator.

Given this text, determine if it contains a genuine pain point.

A genuine pain point:
- Describes a specific problem or difficulty
- Shows frustration or struggle
- Is not just venting or seeking help
- Represents a recurring issue

Return JSON only with this format:
{
  "is_pain_point": true/false,
  "confidence": 0.8,
  "pain_type": "frustration/inefficiency/complexity/workflow/cost/other",
  "specificity": 0.9,  # How specific is the problem (0-1)
  "emotional_intensity": 0.7,  # How strong is the emotion (0-1)
  "keywords": ["struggling", "frustrated", "can't figure out"]
}

Be conservative - only flag clear pain points."""

    def get_statistics(self) -> Dict[str, Any]:
        """è·å–ä½¿ç”¨ç»Ÿè®¡"""
        return self.stats.copy()

    def reset_statistics(self):
        """é‡ç½®ç»Ÿè®¡"""
        self.stats = {
            "requests": 0,
            "tokens_used": 0,
            "cost": 0.0,
            "errors": 0
        }

# å…¨å±€LLMå®¢æˆ·ç«¯å®ä¾‹
llm_client = LLMClient()
```


================================================================================
æå–å®Œæˆ
================================================================================
æ€»å…±æå–äº† 13 ä¸ªæ–‡ä»¶
