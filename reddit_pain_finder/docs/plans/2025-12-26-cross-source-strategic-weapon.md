# Cross-Source Strategic Weapon Enhancement

> **For Claude:** REQUIRED SUB-SKILL: Use superpowers:executing-plans to implement this plan task-by-task.

**Goal:** Transform cross-source alignment into a strategic priority engine that highlights independently validated pain points across Reddit and Hacker News with prominent visual indicators and easy query capabilities.

**Architecture:**
1. **Visual Indicators**: Add prominent badges in reports for cross-source validated opportunities
2. **Priority Sorting**: Automatically sort cross-source validated opportunities to the top
3. **Query Interface**: Add dedicated query methods and CLI tool to retrieve all cross-source validated pain points
4. **Scoring Boost**: Leverage existing boost_score mechanism with enhanced visibility

**Tech Stack:**
- Python 3.10+
- SQLite database (no schema changes required)
- Markdown/JSON report generation
- CLI argument parsing

---

## Current Implementation Analysis

### âœ… Already Working
- `aligned_problems` table stores cross-source aligned problems
- Three-tier validation logic (Level 1-3)
- `validated_problem = True` for Level 1 and Level 2
- `boost_score` implemented (2.0, 1.0, 0.5)
- Cross-source bonus applied in final scoring

### âŒ Needs Enhancement
1. **Report visibility**: Current indicator is just "âœ… Yes/âŒ No" - not prominent enough
2. **Missing badges**: No "Independent validation across Reddit + Hacker News" badge
3. **No query tool**: No easy way to list all cross-source validated pain points
4. **No sorting**: Cross-source validated opportunities not prioritized in reports

---

## Task 1: Enhanced Report Visual Indicators

**Files:**
- Modify: `pipeline/decision_shortlist.py:485-549`

**Step 1: Read current report generation logic**

Read the file to understand the current Markdown report generation.

**Step 2: Add helper method for badge generation**

Add this method to the `DecisionShortlistGenerator` class after line 484:

```python
def _get_cross_source_badge(self, cross_source: Dict) -> str:
    """ç”Ÿæˆè·¨æºéªŒè¯çš„å¾½ç« æ ‡è¯†

    Args:
        cross_source: è·¨æºéªŒè¯ä¿¡æ¯å­—å…¸

    Returns:
        å¾½ç« å­—ç¬¦ä¸²ï¼ˆMarkdownæ ¼å¼ï¼‰
    """
    if not cross_source.get('has_cross_source'):
        return ""

    validation_level = cross_source.get('validation_level', 0)

    if validation_level == 1:
        # Level 1: æœ€å¼ºä¿¡å· - å¤šå¹³å°ç‹¬ç«‹éªŒè¯
        return """
<div align="center">

### ğŸ¯ INDEPENDENT VALIDATION ACROSS REDDIT + HACKER NEWS

**This pain point has been independently validated across multiple communities**

</div>
"""
    elif validation_level == 2:
        # Level 2: ä¸­ç­‰ä¿¡å· - å¤š subreddit éªŒè¯
        return """
### âœ“ Multi-Subreddit Validation
*Validated across 3+ subreddits with strong cluster size*
"""
    elif validation_level == 3:
        # Level 3: å¼±ä¿¡å·
        return """
### â— Weak Cross-Source Signal
*Initial cross-community detection signal*
"""
    else:
        return ""
```

**Step 3: Modify report generation to include badges**

In the `_export_markdown_report` method, replace lines 515-528 with:

```python
            # æ·»åŠ è·¨æºéªŒè¯å¾½ç« ï¼ˆåœ¨æœ€å‰é¢ï¼Œæœ€é†’ç›®ï¼‰
            badge = self._get_cross_source_badge(cross_source)
            if badge:
                report_lines.extend([
                    f"\n{badge}",
                    f"**Validation Level**: {cross_source.get('validation_level', 0)}  ",
                    f"**Boost Applied**: +{cross_source.get('boost_score', 0.0):.1f} to final score",
                    ""
                ])

            report_lines.extend([
                f"**Final Score**: {candidate['final_score']:.2f}/10.0  ",
                f"**Viability Score**: {candidate['viability_score']:.1f}  ",
                f"**Cluster Size**: {candidate['cluster_size']}  ",
                f"**Trust Level**: {candidate['trust_level']:.2f}  ",
                f"**Validated Problem**: {'âœ… Yes' if cross_source.get('validated_problem') else 'âŒ No'}"
            ])
```

**Step 4: Test the enhanced report generation**

Run: `python -m pytest tests/test_decision_shortlist.py -v -k report`

Expected: PASS with new badge formatting in Markdown output

**Step 5: Commit**

```bash
git add pipeline/decision_shortlist.py
git commit -m "feat: add prominent badges for cross-source validation in reports"
```

---

## Task 2: Prioritize Cross-Source Validated Opportunities

**Files:**
- Modify: `pipeline/decision_shortlist.py:372-459`

**Step 1: Read current sorting logic**

Review the `generate_shortlist` method to understand how candidates are currently sorted.

**Step 2: Add sorting key function**

Add this method to the `DecisionShortlistGenerator` class after line 371:

```python
def _sort_priority_key(self, candidate: Dict) -> tuple:
    """ç”Ÿæˆæ’åºé”®ï¼Œç¡®ä¿è·¨æºéªŒè¯çš„æœºä¼šæ’åœ¨å‰é¢

    æ’åºä¼˜å…ˆçº§ï¼š
    1. è·¨æºéªŒè¯ç­‰çº§ï¼ˆLevel 1 > Level 2 > Level 3 > No validationï¼‰
    2. æœ€ç»ˆè¯„åˆ†ï¼ˆé™åºï¼‰
    3. èšç±»è§„æ¨¡ï¼ˆé™åºï¼‰

    Args:
        candidate: å€™é€‰æœºä¼šå­—å…¸

    Returns:
        æ’åºé”®å…ƒç»„
    """
    cross_source = candidate.get('cross_source_validation', {})
    validation_level = cross_source.get('validation_level', 0)

    # éªŒè¯ç­‰çº§è¶Šé«˜è¶Šä¼˜å…ˆï¼ˆç”¨è´Ÿæ•°å®ç°é™åºï¼‰
    # Level 1-3 ä¼˜å…ˆäºæ— éªŒè¯ï¼ˆ0ï¼‰
    priority_score = -validation_level

    # æœ€ç»ˆè¯„åˆ†é™åº
    final_score = -candidate.get('final_score', 0)

    # èšç±»è§„æ¨¡é™åº
    cluster_size = -candidate.get('cluster_size', 0)

    return (priority_score, final_score, cluster_size)
```

**Step 3: Apply sorting in generate_shortlist**

In the `generate_shortlist` method, find the sorting logic (around line 430-435) and replace with:

```python
        # æŒ‰ç…§ä¼˜å…ˆçº§æ’åºï¼šè·¨æºéªŒè¯ > æœ€ç»ˆè¯„åˆ† > èšç±»è§„æ¨¡
        filtered_candidates.sort(key=self._sort_priority_key)
```

**Step 4: Test the sorting logic**

Run: `python -m pytest tests/test_decision_shortlist.py -v -k sort`

Expected: PASS with cross-source validated candidates appearing first

**Step 5: Commit**

```bash
git add pipeline/decision_shortlist.py
git commit -m "feat: prioritize cross-source validated opportunities in shortlist"
```

---

## Task 3: Add Database Query Method for Cross-Source Opportunities

**Files:**
- Modify: `utils/db.py` (after line 1310)

**Step 1: Add new query method**

Add this method to the `DatabaseManager` class:

```python
    def get_cross_source_validated_opportunities(
        self,
        min_validation_level: int = 1,
        include_validated_only: bool = True
    ) -> List[Dict[str, Any]]:
        """æŸ¥è¯¢æ‰€æœ‰è·¨æºéªŒè¯çš„æœºä¼š

        Args:
            min_validation_level: æœ€ä½éªŒè¯ç­‰çº§ï¼ˆ1-3ï¼‰ï¼Œé»˜è®¤ä¸º 1
            include_validated_only: æ˜¯å¦ä»…åŒ…å« validated_problem=True çš„ï¼Œé»˜è®¤ä¸º True

        Returns:
            è·¨æºéªŒè¯çš„æœºä¼šåˆ—è¡¨
        """
        try:
            with self.get_connection("opportunities") as conn:
                query = """
                    SELECT
                        o.opportunity_name,
                        o.final_score,
                        o.viability_score,
                        o.cluster_size,
                        o.trust_level,
                        o.target_users,
                        o.missing_capability,
                        o.why_existing_fail,
                        o.cluster_name,
                        c.source_type,
                        c.alignment_status,
                        c.aligned_problem_id
                    FROM opportunities o
                    LEFT JOIN clusters c ON o.cluster_name = c.cluster_name
                    WHERE 1=1
                """

                params = []

                # æ·»åŠ è·¨æºéªŒè¯è¿‡æ»¤
                if include_validated_only:
                    # Level 1: source_type='aligned' æˆ–æœ‰ aligned_problem_id
                    # Level 2: cluster_size >= 10 AND è·¨ >= 3 subreddits
                    # Level 3: cluster_size >= 8 AND è·¨ >= 2 subreddits

                    # è¿™é‡Œæˆ‘ä»¬ç®€åŒ–å¤„ç†ï¼šæŸ¥è¯¢æ‰€æœ‰å¯èƒ½è·¨æºéªŒè¯çš„èšç±»
                    # ç„¶ååœ¨ Python ä¸­è¿›è¡Œè¯¦ç»†è¿‡æ»¤
                    pass

                query += " ORDER BY o.final_score DESC"

                cursor = conn.execute(query, params)
                results = [dict(row) for row in cursor.fetchall()]

                # åœ¨ Python ä¸­è¿›è¡Œè·¨æºéªŒè¯è¿‡æ»¤
                filtered_results = []
                for result in results:
                    validation_info = self._check_cross_source_validation_sync(
                        result['cluster_name'],
                        result.get('source_type'),
                        result.get('aligned_problem_id'),
                        result['cluster_size']
                    )

                    validation_level = validation_info.get('validation_level', 0)

                    # è¿‡æ»¤æ¡ä»¶
                    if validation_level >= min_validation_level:
                        if include_validated_only:
                            if validation_info.get('validated_problem'):
                                result['cross_source_validation'] = validation_info
                                filtered_results.append(result)
                        else:
                            result['cross_source_validation'] = validation_info
                            filtered_results.append(result)

                return filtered_results

        except Exception as e:
            logger.error(f"Failed to get cross-source validated opportunities: {e}")
            return []

    def _check_cross_source_validation_sync(
        self,
        cluster_name: str,
        source_type: Optional[str],
        aligned_problem_id: Optional[str],
        cluster_size: int
    ) -> Dict[str, Any]:
        """åŒæ­¥ç‰ˆæœ¬çš„è·¨æºéªŒè¯æ£€æŸ¥ï¼ˆç”¨äºæ•°æ®åº“æŸ¥è¯¢ï¼‰

        Args:
            cluster_name: èšç±»åç§°
            source_type: æ¥æºç±»å‹
            aligned_problem_id: å¯¹é½é—®é¢˜ID
            cluster_size: èšç±»è§„æ¨¡

        Returns:
            éªŒè¯ä¿¡æ¯å­—å…¸
        """
        # Level 1: æ£€æŸ¥ aligned source_type æˆ– aligned_problem_id
        if source_type == 'aligned' or aligned_problem_id:
            return {
                "has_cross_source": True,
                "validation_level": 1,
                "boost_score": 2.0,
                "validated_problem": True,
                "evidence": "Independent validation across Reddit + Hacker News"
            }

        # Level 2 & 3: éœ€è¦ subreddit è®¡æ•°ï¼ˆä» pain_events ä¸­æŸ¥è¯¢ï¼‰
        try:
            with self.get_connection("clusters") as conn:
                cursor = conn.execute("""
                    SELECT DISTINCT subreddit
                    FROM pain_events
                    WHERE cluster_name = ?
                """, (cluster_name,))

                subreddits = set(row[0] for row in cursor.fetchall())
                subreddit_count = len(subreddits)

                # Level 2
                if cluster_size >= 10 and subreddit_count >= 3:
                    return {
                        "has_cross_source": True,
                        "validation_level": 2,
                        "boost_score": 1.0,
                        "validated_problem": True,
                        "evidence": f"Validated across {subreddit_count}+ subreddits"
                    }

                # Level 3
                if cluster_size >= 8 and subreddit_count >= 2:
                    return {
                        "has_cross_source": True,
                        "validation_level": 3,
                        "boost_score": 0.5,
                        "validated_problem": False,
                        "evidence": f"Detected across {subreddit_count}+ subreddits"
                    }

        except Exception as e:
            logger.warning(f"Failed to check cross-source validation for {cluster_name}: {e}")

        # æ— è·¨æºéªŒè¯
        return {
            "has_cross_source": False,
            "validation_level": 0,
            "boost_score": 0.0,
            "validated_problem": False,
            "evidence": "No cross-source validation"
        }
```

**Step 2: Test the new query method**

Run: `python -c "from utils.db import DatabaseManager; db = DatabaseManager(); results = db.get_cross_source_validated_opportunities(); print(f'Found {len(results)} cross-source validated opportunities')"`

Expected: Print count of cross-source validated opportunities (may be 0 if no data)

**Step 3: Commit**

```bash
git add utils/db.py
git commit -m "feat: add query method for cross-source validated opportunities"
```

---

## Task 4: Create CLI Tool to Display Cross-Source Pain Points

**Files:**
- Create: `scripts/show_cross_source_pain_points.py`

**Step 1: Write the CLI tool**

```python
#!/usr/bin/env python3
"""
Cross-Source Pain Points Viewer

æ˜¾ç¤ºæ‰€æœ‰è·¨æºéªŒè¯çš„ç—›ç‚¹ï¼Œå›ç­”ï¼š"ç°é˜¶æ®µä¸–ç•Œä¸Š'è¢«ä¸åŒç¤¾ç¾¤ç‹¬ç«‹æåŠ'çš„ç—›ç‚¹æœ‰å“ªäº›ï¼Ÿ"
"""

import argparse
import json
from typing import List, Dict
from utils.db import DatabaseManager
from utils.logger import logger


def format_opportunity(opportunity: Dict, detailed: bool = False) -> str:
    """æ ¼å¼åŒ–å•ä¸ªæœºä¼šæ˜¾ç¤º

    Args:
        opportunity: æœºä¼šæ•°æ®
        detailed: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯

    Returns:
        æ ¼å¼åŒ–çš„å­—ç¬¦ä¸²
    """
    cross_source = opportunity.get('cross_source_validation', {})
    validation_level = cross_source.get('validation_level', 0)

    # éªŒè¯ç­‰çº§å¾½ç« 
    level_badges = {
        1: "ğŸ¯ LEVEL 1 - Multi-Platform Validation",
        2: "âœ“ LEVEL 2 - Multi-Subreddit Validation",
        3: "â— LEVEL 3 - Weak Cross-Source Signal"
    }

    badge = level_badges.get(validation_level, "")

    lines = [
        f"\n{'='*80}",
        f"ğŸ“Œ {opportunity['opportunity_name']}",
        f"{'='*80}",
        f"\n{badge}" if badge else "",
        f"\nğŸ“Š Scores:",
        f"  â€¢ Final Score: {opportunity['final_score']:.2f}/10.0",
        f"  â€¢ Viability Score: {opportunity['viability_score']:.1f}/10.0",
        f"  â€¢ Cluster Size: {opportunity['cluster_size']}",
        f"  â€¢ Trust Level: {opportunity['trust_level']:.2f}",
        f"\nâœ… Validation: {cross_source.get('evidence', 'N/A')}",
        f"   Boost Applied: +{cross_source.get('boost_score', 0.0):.1f}",
        f"   Validated Problem: {'Yes' if cross_source.get('validated_problem') else 'No'}",
    ]

    if detailed:
        lines.extend([
            f"\nğŸ¯ Target Users:",
            f"  {opportunity.get('target_users', 'N/A')}",
            f"\nâŒ Missing Capability:",
            f"  {opportunity.get('missing_capability', 'N/A')}",
            f"\nğŸ’¡ Why Existing Solutions Fail:",
            f"  {opportunity.get('why_existing_fail', 'N/A')}",
            f"\nğŸ“‹ Cluster Info:",
            f"  â€¢ Cluster Name: {opportunity.get('cluster_name', 'N/A')}",
            f"  â€¢ Source Type: {opportunity.get('source_type', 'N/A')}",
            f"  â€¢ Alignment Status: {opportunity.get('alignment_status', 'N/A')}",
        ])

    return '\n'.join(lines)


def print_summary(opportunities: List[Dict]):
    """æ‰“å°ç»Ÿè®¡æ‘˜è¦

    Args:
        opportunities: æœºä¼šåˆ—è¡¨
    """
    total = len(opportunities)

    if total == 0:
        print("\nâš ï¸  No cross-source validated pain points found.")
        return

    # æŒ‰ç­‰çº§ç»Ÿè®¡
    level_counts = {1: 0, 2: 0, 3: 0}
    validated_count = 0

    for opp in opportunities:
        cs = opp.get('cross_source_validation', {})
        level = cs.get('validation_level', 0)
        if level in level_counts:
            level_counts[level] += 1
        if cs.get('validated_problem'):
            validated_count += 1

    print("\n" + "="*80)
    print("ğŸ“Š CROSS-SOURCE VALIDATED PAIN POINTS SUMMARY")
    print("="*80)
    print(f"\nTotal Opportunities: {total}")
    print(f"\n  ğŸ¯ Level 1 (Multi-Platform): {level_counts[1]}")
    print(f"  âœ“ Level 2 (Multi-Subreddit): {level_counts[2]}")
    print(f"  â— Level 3 (Weak Signal): {level_counts[3]}")
    print(f"\n  âœ… Validated Problems: {validated_count}")
    print(f"  âŒ Not Validated: {total - validated_count}")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="Display cross-source validated pain points",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # æ˜¾ç¤ºæ‰€æœ‰è·¨æºéªŒè¯çš„ç—›ç‚¹ï¼ˆæ‘˜è¦ï¼‰
  python scripts/show_cross_source_pain_points.py

  # æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
  python scripts/show_cross_source_pain_points.py --detailed

  # ä»…æ˜¾ç¤º Level 1 çš„å¤šå¹³å°éªŒè¯
  python scripts/show_cross_source_pain_points.py --min-level 1

  # ä»…æ˜¾ç¤º validated_problem=True çš„
  python scripts/show_cross_source_pain_points.py --validated-only

  # å¯¼å‡ºåˆ° JSON
  python scripts/show_cross_source_pain_points.py --export cross_source.json
        """
    )

    parser.add_argument(
        '--min-level',
        type=int,
        choices=[1, 2, 3],
        default=1,
        help='Minimum validation level (default: 1)'
    )

    parser.add_argument(
        '--validated-only',
        action='store_true',
        help='Show only validated_problem=True opportunities'
    )

    parser.add_argument(
        '--detailed',
        action='store_true',
        help='Show detailed information for each opportunity'
    )

    parser.add_argument(
        '--export',
        type=str,
        metavar='FILE',
        help='Export results to JSON file'
    )

    parser.add_argument(
        '--limit',
        type=int,
        default=None,
        help='Limit number of results (default: show all)'
    )

    args = parser.parse_args()

    # æŸ¥è¯¢æ•°æ®åº“
    logger.info("Querying cross-source validated pain points...")
    db = DatabaseManager()

    opportunities = db.get_cross_source_validated_opportunities(
        min_validation_level=args.min_level,
        include_validated_only=args.validated_only
    )

    # åº”ç”¨é™åˆ¶
    if args.limit:
        opportunities = opportunities[:args.limit]

    # æ‰“å°æ‘˜è¦
    print_summary(opportunities)

    # å¯¼å‡ºåˆ° JSON
    if args.export:
        export_data = {
            'generated_at': datetime.now().isoformat(),
            'filter': {
                'min_validation_level': args.min_level,
                'validated_only': args.validated_only
            },
            'total_count': len(opportunities),
            'opportunities': opportunities
        }

        with open(args.export, 'w', encoding='utf-8') as f:
            json.dump(export_data, f, indent=2, ensure_ascii=False)

        logger.info(f"âœ… Exported to {args.export}")

    # æ‰“å°æ¯ä¸ªæœºä¼š
    for opp in opportunities:
        print(format_opportunity(opp, detailed=args.detailed))

    print(f"\n{'='*80}\n")
    logger.info(f"âœ… Displayed {len(opportunities)} cross-source validated pain points")


if __name__ == '__main__':
    from datetime import datetime
    main()
```

**Step 2: Make the script executable**

Run: `chmod +x scripts/show_cross_source_pain_points.py`

**Step 3: Test the CLI tool**

Run: `python scripts/show_cross_source_pain_points.py --help`

Expected: Display help message with all options

**Step 4: Test with actual data**

Run: `python scripts/show_cross_source_pain_points.py`

Expected: Display all cross-source validated pain points (or message if none found)

**Step 5: Commit**

```bash
git add scripts/show_cross_source_pain_points.py
git commit -m "feat: add CLI tool to display cross-source validated pain points"
```

---

## Task 5: Update JSON Report Format

**Files:**
- Modify: `pipeline/decision_shortlist.py:551-596`

**Step 1: Add cross-source validation summary to JSON report**

Modify the `_export_json_report` method to include enhanced cross-source information. Replace lines 576-589 with:

```python
        for candidate in shortlist:
            cross_source = candidate.get('cross_source_validation', {})

            export_candidate = {
                'opportunity_name': candidate.get('opportunity_name'),
                'final_score': candidate.get('final_score'),
                'viability_score': candidate.get('viability_score'),
                'cluster_size': candidate.get('cluster_size'),
                'trust_level': candidate.get('trust_level'),
                'target_users': candidate.get('target_users'),
                'missing_capability': candidate.get('missing_capability'),
                'why_existing_fail': candidate.get('why_existing_fail'),
                'readable_content': candidate.get('readable_content', {}),
                'cross_source_validation': {
                    'has_cross_source': cross_source.get('has_cross_source', False),
                    'validation_level': cross_source.get('validation_level', 0),
                    'validated_problem': cross_source.get('validated_problem', False),
                    'boost_score': cross_source.get('boost_score', 0.0),
                    'evidence': cross_source.get('evidence', ''),
                    'badge_text': self._get_cross_source_badge_text(cross_source)
                }
            }
            export_data['candidates'].append(export_candidate)
```

**Step 2: Add helper method for badge text**

Add this method to the class:

```python
def _get_cross_source_badge_text(self, cross_source: Dict) -> str:
    """è·å–è·¨æºéªŒè¯å¾½ç« çš„çº¯æ–‡æœ¬ç‰ˆæœ¬

    Args:
        cross_source: è·¨æºéªŒè¯ä¿¡æ¯å­—å…¸

    Returns:
        å¾½ç« æ–‡æœ¬
    """
    if not cross_source.get('has_cross_source'):
        return ""

    validation_level = cross_source.get('validation_level', 0)

    badge_texts = {
        1: "ğŸ¯ INDEPENDENT VALIDATION ACROSS REDDIT + HACKER NEWS",
        2: "âœ“ Multi-Subreddit Validation",
        3: "â— Weak Cross-Source Signal"
    }

    return badge_texts.get(validation_level, "")
```

**Step 3: Test JSON report generation**

Run: `python -m pytest tests/test_decision_shortlist.py::test_json_report -v`

Expected: PASS with enhanced cross-source information in JSON output

**Step 4: Commit**

```bash
git add pipeline/decision_shortlist.py
git commit -m "feat: enhance JSON report with cross-source validation details"
```

---

## Task 6: Add Documentation

**Files:**
- Create: `docs/cross_source_validation_guide.md`

**Step 1: Write comprehensive documentation**

```markdown
# Cross-Source Validation Guide

## Overview

Cross-source validation is a strategic engine that identifies pain points independently validated across multiple communities (Reddit, Hacker News, etc.). This system transforms social listening into a prioritized opportunity radar.

## What is Cross-Source Validation?

A pain point is "cross-source validated" when the **same underlying problem** is discussed across different communities, despite differences in:
- Language and terminology
- Community maturity
- Technical depth
- Cultural context

### Why This Matters

When developers on Reddit and entrepreneurs on Hacker News **independently** complain about the same problem, that's a **strong market signal**.

It means:
- âœ… The problem is **real and persistent**
- âœ… It affects **multiple user segments**
- âœ… It's **not platform-specific noise**
- âœ… There's **unmet demand** across different contexts

## Validation Levels

### Level 1: Multi-Platform Validation (ğŸ¯ Strongest)

**Condition**: Pain point appears across **different platforms** (Reddit + Hacker News)

**Indicators**:
- `source_type = 'aligned'` in clusters table
- OR exists in `aligned_problems` table with `alignment_score >= 0.7`

**Boost**: +2.0 to final score
**Validated Problem**: Yes

**Example**:
> Reddit: "I hate managing environment variables across different projects"
> HackerNews: "Configuration management is a nightmare in microservices"

â†’ Both are complaining about **configuration management**, just using different words.

---

### Level 2: Multi-Subreddit Validation (âœ“ Medium)

**Condition**:
- `cluster_size >= 10`
- Appears across **3+ different subreddits**

**Boost**: +1.0 to final score
**Validated Problem**: Yes

**Example**:
Same problem discussed in:
- r/programming
- r/devops
- r/webdev

---

### Level 3: Weak Cross-Source Signal (â— Weak)

**Condition**:
- `cluster_size >= 8`
- Appears across **2+ different subreddits**

**Boost**: +0.5 to final score
**Validated Problem**: No (needs more validation)

---

## How to Use

### 1. View All Cross-Source Validated Pain Points

```bash
# Show all cross-source validated pain points
python scripts/show_cross_source_pain_points.py

# Show only Level 1 (strongest signals)
python scripts/show_cross_source_pain_points.py --min-level 1

# Show detailed information
python scripts/show_cross_source_pain_points.py --detailed

# Export to JSON
python scripts/show_cross_source_pain_points.py --export cross_source.json
```

### 2. In Decision Shortlist Reports

Decision shortlist reports automatically:
- âœ… Prioritize cross-source validated opportunities at the top
- âœ… Display prominent badges (ğŸ¯ / âœ“ / â—)
- âœ… Show validation level and boost applied
- âœ… Include "Independent validation across Reddit + Hacker News" for Level 1

### 3. Query Programmatically

```python
from utils.db import DatabaseManager

db = DatabaseManager()

# Get all cross-source validated opportunities
opportunities = db.get_cross_source_validated_opportunities()

# Get only Level 1 (strongest)
opportunities = db.get_cross_source_validated_opportunities(
    min_validation_level=1
)

# Get only validated_problem=True
opportunities = db.get_cross_source_validated_opportunities(
    include_validated_only=True
)
```

## FAQ

### Q: How is cross-source alignment detected?

**A**: We use LLM-based semantic analysis:
1. Extract cluster summaries from each source (Reddit, HN Ask, HN Show)
2. Ask LLM: "Are these describing the same underlying problem?"
3. LLM provides alignment score (0.0-1.0) and explanation
4. Threshold: `alignment_score >= 0.7`

### Q: Why doesn't Level 3 count as "validated_problem"?

**A**: Level 3 is a **weak signal** - it indicates potential cross-source validation, but needs more evidence. Only Level 1 and Level 2 are strong enough to be "validated problems".

### Q: Can I adjust the boost scores?

**A**: Yes! Edit `config/thresholds.yaml`:

```yaml
decision_shortlist:
  final_score_weights:
    cross_source_bonus: 5.0  # Adjust base bonus
```

The actual boost is: `cross_source_bonus * boost_score * 0.1`
- Level 1: 5.0 * 2.0 * 0.1 = 1.0
- Level 2: 5.0 * 1.0 * 0.1 = 0.5
- Level 3: 5.0 * 0.5 * 0.1 = 0.25

### Q: What's the difference between `aligned_problems` and `clusters`?

**A**:
- `clusters`: Raw groupings from a single source (Reddit, HN Ask, HN Show)
- `aligned_problems**: Unified problems after LLM detects cross-source alignment

Each `aligned_problem` links to 2+ original `clusters` via `cluster_ids` field.

---

## Technical Implementation

### Database Schema

#### `aligned_problems` table

```sql
CREATE TABLE aligned_problems (
    id TEXT PRIMARY KEY,              -- aligned_AP_XX_timestamp
    aligned_problem_id TEXT UNIQUE,   -- AP_XX
    sources TEXT,                     -- JSON: ["reddit", "hackernews"]
    core_problem TEXT,                -- Unified problem description
    why_they_look_different TEXT,     -- LLM explanation
    evidence TEXT,                    -- JSON: Evidence from each source
    cluster_ids TEXT,                 -- JSON: Original cluster IDs
    alignment_score REAL DEFAULT 0.0, -- 0.0-1.0, threshold: 0.7
    created_at TIMESTAMP
);
```

#### `clusters` table (alignment columns)

```sql
ALTER TABLE clusters ADD COLUMN:
    alignment_status TEXT,           -- 'unprocessed' | 'aligned' | 'processed'
    aligned_problem_id TEXT          -- Foreign key to aligned_problems
);
```

### Key Code Files

- **Alignment Logic**: `pipeline/align_cross_sources.py`
- **Scoring**: `pipeline/decision_shortlist.py` (lines 126-198, 231-263)
- **Database Queries**: `utils/db.py` (lines 1270-1310)
- **CLI Tool**: `scripts/show_cross_source_pain_points.py`

---

## The Strategic Question

**"ç°é˜¶æ®µä¸–ç•Œä¸Š'è¢«ä¸åŒç¤¾ç¾¤ç‹¬ç«‹æåŠ'çš„ç—›ç‚¹æœ‰å“ªäº›ï¼Ÿ"**

Now you can answer this in seconds:

```bash
python scripts/show_cross_source_pain_points.py --min-level 1
```

This gives you a prioritized list of pain points validated across Reddit and Hacker News - your **opportunity radar** for product discovery.
```

**Step 2: Commit documentation**

```bash
git add docs/cross_source_validation_guide.md
git commit -m "docs: add comprehensive cross-source validation guide"
```

---

## Task 7: Integration Testing

**Files:**
- Create: `tests/test_cross_source_validation.py`

**Step 1: Write integration tests**

```python
import pytest
from utils.db import DatabaseManager
from pipeline.decision_shortlist import DecisionShortlistGenerator


class TestCrossSourceValidation:
    """è·¨æºéªŒè¯åŠŸèƒ½é›†æˆæµ‹è¯•"""

    @pytest.fixture
    def db(self):
        """æ•°æ®åº“å®ä¾‹"""
        return DatabaseManager()

    @pytest.fixture
    def shortlist_generator(self):
        """å†³ç­–æ¸…å•ç”Ÿæˆå™¨"""
        return DecisionShortlistGenerator()

    def test_query_cross_source_validated_opportunities(self, db):
        """æµ‹è¯•æŸ¥è¯¢è·¨æºéªŒè¯æœºä¼š"""
        # æµ‹è¯•æŸ¥è¯¢æ‰€æœ‰
        opportunities = db.get_cross_source_validated_opportunities()
        assert isinstance(opportunities, list)

        # å¦‚æœæœ‰æ•°æ®ï¼ŒéªŒè¯å­—æ®µ
        if opportunities:
            opp = opportunities[0]
            assert 'cross_source_validation' in opp
            assert 'opportunity_name' in opp
            assert 'final_score' in opp

    def test_query_min_validation_level(self, db):
        """æµ‹è¯•æœ€ä½éªŒè¯ç­‰çº§è¿‡æ»¤"""
        # Level 1
        level1 = db.get_cross_source_validated_opportunities(
            min_validation_level=1
        )

        # Level 2
        level2 = db.get_cross_source_validated_opportunities(
            min_validation_level=2
        )

        # Level 1 åº”è¯¥åŒ…å« Level 2ï¼ˆæˆ–è€…æ›´å¤šï¼‰
        assert len(level1) >= len(level2)

    def test_sorting_priority(self, shortlist_generator):
        """æµ‹è¯•æ’åºä¼˜å…ˆçº§"""
        # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®
        mock_candidates = [
            {
                'opportunity_name': 'No Validation',
                'final_score': 9.0,
                'cluster_size': 100,
                'cross_source_validation': {
                    'has_cross_source': False,
                    'validation_level': 0
                }
            },
            {
                'opportunity_name': 'Level 2 Validation',
                'final_score': 7.0,
                'cluster_size': 50,
                'cross_source_validation': {
                    'has_cross_source': True,
                    'validation_level': 2
                }
            },
            {
                'opportunity_name': 'Level 1 Validation',
                'final_score': 8.0,
                'cluster_size': 30,
                'cross_source_validation': {
                    'has_cross_source': True,
                    'validation_level': 1
                }
            }
        ]

        # åº”ç”¨æ’åº
        sorted_candidates = sorted(
            mock_candidates,
            key=shortlist_generator._sort_priority_key
        )

        # éªŒè¯é¡ºåºï¼šLevel 1 > Level 2 > No Validation
        assert sorted_candidates[0]['opportunity_name'] == 'Level 1 Validation'
        assert sorted_candidates[1]['opportunity_name'] == 'Level 2 Validation'
        assert sorted_candidates[2]['opportunity_name'] == 'No Validation'

    def test_badge_generation(self, shortlist_generator):
        """æµ‹è¯•å¾½ç« ç”Ÿæˆ"""
        # Level 1
        badge1 = shortlist_generator._get_cross_source_badge({
            'has_cross_source': True,
            'validation_level': 1
        })
        assert 'INDEPENDENT VALIDATION ACROSS REDDIT + HACKER NEWS' in badge1

        # Level 2
        badge2 = shortlist_generator._get_cross_source_badge({
            'has_cross_source': True,
            'validation_level': 2
        })
        assert 'Multi-Subreddit Validation' in badge2

        # No validation
        badge0 = shortlist_generator._get_cross_source_badge({
            'has_cross_source': False
        })
        assert badge0 == ""

    def test_cross_source_boost_in_scoring(self, shortlist_generator):
        """æµ‹è¯•è·¨æºéªŒè¯åœ¨è¯„åˆ†ä¸­çš„åŠ æˆ"""
        # æ¨¡æ‹Ÿæ•°æ®
        cluster = {
            'cluster_size': 20,
            'cluster_name': 'test_cluster',
            'source_type': 'reddit',
            'trust_level': 0.8,
            'alignment_status': 'unprocessed'
        }

        # æ— è·¨æºéªŒè¯
        score1 = shortlist_generator._calculate_final_score(
            viability_score=7.0,
            cluster=cluster,
            cross_source_info={
                'has_cross_source': False,
                'boost_score': 0.0
            }
        )

        # æœ‰è·¨æºéªŒè¯ (Level 1, boost=2.0)
        score2 = shortlist_generator._calculate_final_score(
            viability_score=7.0,
            cluster=cluster,
            cross_source_info={
                'has_cross_source': True,
                'boost_score': 2.0
            }
        )

        # æœ‰è·¨æºéªŒè¯çš„è¯„åˆ†åº”è¯¥æ›´é«˜
        assert score2 > score1


if __name__ == '__main__':
    pytest.main([__file__, '-v'])
```

**Step 2: Run tests**

Run: `python -m pytest tests/test_cross_source_validation.py -v`

Expected: All tests pass

**Step 3: Commit**

```bash
git add tests/test_cross_source_validation.py
git commit -m "test: add cross-source validation integration tests"
```

---

## Verification Checklist

After implementation, verify:

- [ ] Reports show prominent badges for cross-source validation
- [ ] Level 1 displays "ğŸ¯ INDEPENDENT VALIDATION ACROSS REDDIT + HACKER NEWS"
- [ ] Cross-source validated opportunities appear at the top of shortlists
- [ ] CLI tool works: `python scripts/show_cross_source_pain_points.py`
- [ ] Can query cross-source opportunities programmatically
- [ ] JSON reports include enhanced cross-source information
- [ ] All tests pass
- [ ] Documentation is complete and clear

---

## Success Criteria

âœ… **Can easily answer**: "What pain points are independently validated across different communities?"

Run this command:
```bash
python scripts/show_cross_source_pain_points.py --min-level 1
```

Expected output:
- List of all Level 1 cross-source validated pain points
- With prominent badges and evidence
- Prioritized by final score

This is your **strategic opportunity radar**.
