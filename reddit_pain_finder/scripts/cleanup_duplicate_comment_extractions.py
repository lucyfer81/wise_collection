#!/usr/bin/env python3
"""
æ¸…ç†é‡å¤æŠ½å–çš„comment pain events
åˆ é™¤é‡å¤çš„commentæŠ½å–è®°å½•ï¼Œåªä¿ç•™æ¯ä¸ªcommentæœ€æ—©æŠ½å–çš„é‚£ä¸€æ¡
"""
import sys
import os
import logging

# Add parent directory to path
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from utils.db import db

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def analyze_duplicates():
    """åˆ†æé‡å¤æŠ½å–æƒ…å†µ"""
    with db.get_connection("pain") as conn:
        cursor = conn.execute("""
            SELECT source_id, COUNT(*) as count, MIN(extracted_at) as first_extracted
            FROM pain_events
            WHERE source_type = 'comment'
            GROUP BY source_id
            HAVING count > 1
            ORDER BY count DESC
        """)
        return [dict(row) for row in cursor.fetchall()]


def cleanup_duplicates(dry_run=True):
    """æ¸…ç†é‡å¤æŠ½å–

    Args:
        dry_run: å¦‚æœä¸ºTrueï¼Œåªåˆ†æä¸å®é™…åˆ é™¤

    Returns:
        æ¸…ç†ç»Ÿè®¡ä¿¡æ¯
    """
    logger.info("=" * 80)
    logger.info("æ¸…ç†é‡å¤æŠ½å–çš„comment pain events")
    if dry_run:
        logger.info("DRY RUN MODE - ä¸ä¼šå®é™…åˆ é™¤æ•°æ®")
    logger.info("=" * 80)

    # 1. åˆ†æé‡å¤æƒ…å†µ
    duplicates = analyze_duplicates()

    if not duplicates:
        logger.info("âœ… æ²¡æœ‰å‘ç°é‡å¤æŠ½å–çš„comments")
        return {
            "duplicates_found": 0,
            "total_events": 0,
            "to_keep": 0,
            "to_delete": 0
        }

    total_duplicate_events = sum(d["count"] for d in duplicates)
    to_keep = len(duplicates)
    to_delete = total_duplicate_events - to_keep

    logger.info(f"\nå‘ç° {len(duplicates)} æ¡commentsè¢«é‡å¤æŠ½å–")
    logger.info(f"æ€»é‡å¤äº‹ä»¶æ•°: {total_duplicate_events}")
    logger.info(f"éœ€è¦ä¿ç•™: {to_keep} æ¡ï¼ˆæ¯ä¸ªcommentæœ€æ—©çš„ä¸€æ¡ï¼‰")
    logger.info(f"éœ€è¦åˆ é™¤: {to_delete} æ¡")
    logger.info("")

    # 2. æ˜¾ç¤ºé‡å¤è¯¦æƒ…ï¼ˆå‰10æ¡ï¼‰
    logger.info("é‡å¤æŠ½å–è¯¦æƒ…ï¼ˆå‰10æ¡ï¼‰:")
    logger.info("-" * 80)
    for i, d in enumerate(duplicates[:10], 1):
        logger.info(f'{i}. comment_id={d["source_id"]}: {d["count"]}æ¬¡æŠ½å–, æœ€æ—©={d["first_extracted"]}')

    if len(duplicates) > 10:
        logger.info(f"... è¿˜æœ‰ {len(duplicates) - 10} æ¡")

    # 3. æ‰§è¡Œæ¸…ç†ï¼ˆå¦‚æœä¸æ˜¯dry runï¼‰
    if dry_run:
        logger.info("\n" + "=" * 80)
        logger.info("DRY RUNå®Œæˆ - å®é™…è¿è¡Œæ—¶å°†åˆ é™¤ä»¥ä¸Šé‡å¤æ•°æ®")
        logger.info("=" * 80)
        return {
            "duplicates_found": len(duplicates),
            "total_events": total_duplicate_events,
            "to_keep": to_keep,
            "to_delete": to_delete
        }

    # å®é™…åˆ é™¤
    logger.info("\n" + "=" * 80)
    logger.info("å¼€å§‹æ¸…ç†...")
    logger.info("=" * 80)

    deleted_count = 0
    with db.get_connection("pain") as conn:
        for i, dup in enumerate(duplicates, 1):
            comment_id = dup["source_id"]

            # æ‰¾åˆ°è¯¥comment_idæœ€å°çš„IDï¼ˆä¿ç•™ï¼‰
            cursor = conn.execute("""
                SELECT MIN(id) as keep_id
                FROM pain_events
                WHERE source_type = 'comment' AND source_id = ?
            """, (comment_id,))
            result = cursor.fetchone()
            keep_id = result["keep_id"] if result else None

            if not keep_id:
                logger.warning(f"âš ï¸ comment {comment_id} æ²¡æœ‰æ‰¾åˆ°è®°å½•ï¼Œè·³è¿‡")
                continue

            # åˆ é™¤è¯¥comment_idä¸‹ï¼ŒIDä¸æ˜¯æœ€å°IDçš„æ‰€æœ‰è®°å½•
            cursor = conn.execute("""
                DELETE FROM pain_events
                WHERE source_type = 'comment'
                  AND source_id = ?
                  AND id != ?
            """, (comment_id, keep_id))

            rows_deleted = cursor.rowcount
            deleted_count += rows_deleted

            if i % 5 == 0 or i == len(duplicates):
                logger.info(f"è¿›åº¦: {i}/{len(duplicates)} comments, å·²åˆ é™¤: {deleted_count} æ¡")

        conn.commit()

    logger.info("\n" + "=" * 80)
    logger.info("âœ… æ¸…ç†å®Œæˆ!")
    logger.info("=" * 80)

    return {
        "duplicates_found": len(duplicates),
        "total_events": total_duplicate_events,
        "to_keep": to_keep,
        "to_delete": to_delete,
        "deleted_count": deleted_count
    }


def verify_cleanup():
    """éªŒè¯æ¸…ç†ç»“æœ"""
    logger.info("\néªŒè¯æ¸…ç†ç»“æœ...")

    with db.get_connection("pain") as conn:
        # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰é‡å¤
        cursor = conn.execute("""
            SELECT source_id, COUNT(*) as count
            FROM pain_events
            WHERE source_type = 'comment'
            GROUP BY source_id
            HAVING count > 1
        """)
        remaining_duplicates = cursor.fetchall()

        if remaining_duplicates:
            logger.warning(f"âš ï¸ ä»æœ‰ {len(remaining_duplicates)} æ¡commentså­˜åœ¨é‡å¤æŠ½å–!")
            return False
        else:
            logger.info("âœ… æ²¡æœ‰é‡å¤æŠ½å–çš„comments")

        # ç»Ÿè®¡å½“å‰çš„comment events
        cursor = conn.execute("""
            SELECT COUNT(DISTINCT source_id) as unique_comments, COUNT(*) as total_events
            FROM pain_events
            WHERE source_type = 'comment'
        """)
        stats = cursor.fetchone()

        logger.info(f"å½“å‰ç»Ÿè®¡:")
        logger.info(f"  å”¯ä¸€è¯„è®ºæ•°: {stats['unique_comments']}")
        logger.info(f"  æ€»äº‹ä»¶æ•°: {stats['total_events']}")

        return True


def main():
    """ä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(
        description="æ¸…ç†é‡å¤æŠ½å–çš„comment pain events"
    )
    parser.add_argument(
        "--dry-run",
        action="store_true",
        default=True,
        help="åˆ†æä½†ä¸å®é™…åˆ é™¤ï¼ˆé»˜è®¤å¯ç”¨ï¼‰"
    )
    parser.add_argument(
        "--execute",
        action="store_true",
        help="å®é™…æ‰§è¡Œæ¸…ç†ï¼ˆé»˜è®¤åªåˆ†æï¼‰"
    )

    args = parser.parse_args()

    try:
        # å¦‚æœç”¨æˆ·æŒ‡å®šäº† --executeï¼Œåˆ™å…³é—­ dry_run
        dry_run = not args.execute

        # æ‰§è¡Œæ¸…ç†
        result = cleanup_duplicates(dry_run=dry_run)

        # å¦‚æœä¸æ˜¯dry runï¼ŒéªŒè¯ç»“æœ
        if not dry_run:
            verify_cleanup()

        # è¾“å‡ºæ‘˜è¦
        logger.info("\n" + "=" * 80)
        logger.info("æ¸…ç†æ‘˜è¦")
        logger.info("=" * 80)
        logger.info(f"å‘ç°çš„é‡å¤comments: {result['duplicates_found']}")
        logger.info(f"æ€»äº‹ä»¶æ•°ï¼ˆæ¸…ç†å‰ï¼‰: {result['total_events']}")
        logger.info(f"ä¿ç•™: {result['to_keep']}")
        logger.info(f"åˆ é™¤: {result.get('deleted_count', result['to_delete'])}")
        logger.info("=" * 80)

        if dry_run:
            logger.info("\nğŸ’¡ æç¤º: è¿™æ˜¯DRY RUNï¼Œæ²¡æœ‰å®é™…åˆ é™¤æ•°æ®")
            logger.info("ğŸ’¡ è¦å®é™…æ‰§è¡Œæ¸…ç†ï¼Œè¯·è¿è¡Œ:")
            logger.info("   python3 scripts/cleanup_duplicate_comment_extractions.py --execute")

    except Exception as e:
        logger.error(f"æ¸…ç†å¤±è´¥: {e}", exc_info=True)
        sys.exit(1)


if __name__ == "__main__":
    main()
