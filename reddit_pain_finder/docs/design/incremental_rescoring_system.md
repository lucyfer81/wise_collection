# å¢é‡æ›´æ–°é‡æ–°è¯„åˆ†ç³»ç»Ÿè®¾è®¡æ–‡æ¡£

**ç‰ˆæœ¬**: 1.0
**æ—¥æœŸ**: 2026-01-04
**ä½œè€…**: Claude (UltraThink Mode)
**çŠ¶æ€**: è®¾è®¡é˜¶æ®µ

---

## ğŸ“‹ ç›®å½•

1. [é—®é¢˜åˆ†æ](#é—®é¢˜åˆ†æ)
2. [è®¾è®¡ç›®æ ‡](#è®¾è®¡ç›®æ ‡)
3. [æ ¸å¿ƒæ¦‚å¿µ](#æ ¸å¿ƒæ¦‚å¿µ)
4. [ç³»ç»Ÿæ¶æ„](#ç³»ç»Ÿæ¶æ„)
5. [æ•°æ®æ¨¡å‹è®¾è®¡](#æ•°æ®æ¨¡å‹è®¾è®¡)
6. [è§¦å‘æ£€æµ‹æœºåˆ¶](#è§¦å‘æ£€æµ‹æœºåˆ¶)
7. [è¯„åˆ†ç­–ç•¥è®¾è®¡](#è¯„åˆ†ç­–ç•¥è®¾è®¡)
8. [å®ç°è®¡åˆ’](#å®ç°è®¡åˆ’)
9. [é£é™©è¯„ä¼°](#é£é™©è¯„ä¼°)
10. [æ€§èƒ½è€ƒè™‘](#æ€§èƒ½è€ƒè™‘)

---

## é—®é¢˜åˆ†æ

### å½“å‰é—®é¢˜

ä»æ•°æ®åˆ†æä¸­å‘ç°çš„3ä¸ªæ ¸å¿ƒé—®é¢˜ï¼š

#### é—®é¢˜1ï¼šå¢é‡æ›´æ–°çš„clustersè¢«å¿½ç•¥

```sql
-- æœ€è¿‘24å°æ—¶æ–°å¢eventsçš„clusters
Cluster 5:  +62 events (1107 total) â†’ raw_total_score = 8.31 (å·²è¯„åˆ†)
Cluster 26: +16 events (30 total)   â†’ raw_total_score = 7.67 (å·²è¯„åˆ†)
Cluster 11: +11 events (52 total)   â†’ raw_total_score = 7.27 (å·²è¯„åˆ†)
Cluster 22: +10 events (32 total)   â†’ raw_total_score = 0.0  (æœªè¯„åˆ†)
```

**é—®é¢˜**ï¼š
- Cluster 5, 26, 11åœ¨ä»Šå¤©è·å¾—äº†10-62ä¸ªæ–°events
- å®ƒä»¬å·²ç»æœ‰opportunitiesï¼Œæ‰€ä»¥map_opportunitiesè·³è¿‡å®ƒä»¬
- å³ä½¿è·å¾—äº†æ–°æ•°æ®ï¼Œopportunityçš„è¯„åˆ†ä»ç„¶æ˜¯æ—§çš„
- **æ— æ³•åæ˜ æœ€æ–°çš„clusterçŠ¶æ€**

#### é—®é¢˜2ï¼šæ–°åˆ›å»ºçš„å°clustersè¢«è¿‡æ»¤è§„åˆ™é˜»æ­¢è¯„åˆ†

```sql
-- æ–°clustersï¼ˆä»Šå¤©åˆ›å»ºï¼‰
Cluster 35: 4 events â†’ raw_total_score = 0.0 (abandon - èšç±»è§„æ¨¡è¿‡å°)
Cluster 36: 4 events â†’ raw_total_score = 0.0 (abandon - èšç±»è§„æ¨¡è¿‡å°)
```

**é—®é¢˜**ï¼š
- Filtering rulesåœ¨LLMè¯„åˆ†**ä¹‹å‰**åº”ç”¨
- å°clustersæ ¹æœ¬æ²¡æœ‰æœºä¼šè¢«è¯„ä¼°
- å³ä½¿å®ƒä»¬å¯èƒ½åŒ…å«æœ‰ä»·å€¼çš„insights
- **31ä¸ªopportunitiesï¼ˆ86%ï¼‰æ²¡æœ‰è¢«è¯„åˆ†**

#### é—®é¢˜3ï¼šDecision_shortliståªèƒ½æ˜¾ç¤ºå†å²æ•°æ®

```python
# decision_shortlist.py:79
WHERE o.raw_total_score >= 6.0  # æ–°opportunitieséƒ½æ˜¯0.0
```

**é—®é¢˜**ï¼š
- æ–°opportunitiesçš„raw_total_score = 0.0ï¼Œä¸æ»¡è¶³é˜ˆå€¼
- åªèƒ½è¿”å›å†å²è¯„åˆ†çš„opportunities (ID: 2, 5)
- **Reportåæ˜ çš„ä¸æ˜¯æœ€æ–°çš„clusterçŠ¶æ€**

### æ ¹æœ¬åŸå› 

**Pipelineè®¾è®¡æ²¡æœ‰è€ƒè™‘"å¢é‡æ›´æ–°"åœºæ™¯**ï¼š

1. `map_opportunities` åªä¸º"æ²¡æœ‰opportunityçš„clusters"åˆ›å»ºæ–°opportunities
2. `score_viability` çš„filtering rulesåœ¨è¯„åˆ†**ä¹‹å‰**è¿‡æ»¤
3. `decision_shortlist` æ²¡æœ‰è€ƒè™‘"æœ€è¿‘æ›´æ–°çš„clusters"

---

## è®¾è®¡ç›®æ ‡

### æ ¸å¿ƒç›®æ ‡

1. **æ£€æµ‹æ˜¾è‘—å˜åŒ–**: è‡ªåŠ¨æ£€æµ‹clustersçš„æ˜¾è‘—å˜åŒ–ï¼ˆæ–°å¢eventsã€è·¨æºéªŒè¯ç­‰ï¼‰
2. **æ™ºèƒ½é‡æ–°è¯„åˆ†**: ä¸ºæ˜¾è‘—å˜åŒ–çš„clustersé‡æ–°è¯„åˆ†ï¼Œè€Œä¸æ˜¯ç›²ç›®é‡æ–°è¯„åˆ†æ‰€æœ‰clusters
3. **é¿å…æ— é™å¾ªç¯**: é˜²æ­¢æ¯æ¬¡pipelineè¿è¡Œéƒ½é‡æ–°è¯„åˆ†æ‰€æœ‰clusters
4. **ä¿æŒå†å²è®°å½•**: ä¿ç•™è¯„åˆ†å†å²ï¼Œä¾¿äºåˆ†æè¶‹åŠ¿å’Œå›æ»š
5. **æˆæœ¬å¯æ§**: LLMè°ƒç”¨æˆæœ¬è¦åˆç†ï¼Œä¸èƒ½å› ä¸ºé‡æ–°è¯„åˆ†å¯¼è‡´æˆæœ¬çˆ†ç‚¸

### éç›®æ ‡ï¼ˆæ˜ç¡®ä¸åšï¼‰

1. ~~å®æ—¶æ›´æ–°~~ - ä¸éœ€è¦å®æ—¶ï¼Œæ‰¹é‡å¤„ç†å³å¯
2. ~~å®Œå…¨é‡å†™pipeline~~ - åœ¨ç°æœ‰æ¶æ„ä¸Šå¢é‡æ”¹è¿›
3. ~~è‡ªåŠ¨åˆ é™¤æ—§opportunities~~ - ä¿ç•™å†å²è®°å½•

---

## æ ¸å¿ƒæ¦‚å¿µ

### 1. Clusterå¿«ç…§ (Cluster Snapshot)

åœ¨æŸä¸ªæ—¶é—´ç‚¹è®°å½•clusterçš„å…³é”®æŒ‡æ ‡ï¼Œç”¨äºæ£€æµ‹å˜åŒ–ï¼š

```python
{
    "cluster_id": 5,
    "snapshot_time": "2026-01-04 04:10:08",
    "cluster_size": 1107,
    "unique_authors": 523,
    "cross_subreddit_count": 63,
    "avg_frequency_score": 7.2,
    "latest_event_extracted_at": "2026-01-04 04:10:08"
}
```

### 2. æ˜¾è‘—å˜åŒ– (Significant Change)

æ»¡è¶³ä»¥ä¸‹**ä»»ä¸€æ¡ä»¶**å³è®¤ä¸ºclusterå‘ç”Ÿäº†æ˜¾è‘—å˜åŒ–ï¼š

```yaml
# è§¦å‘é˜ˆå€¼
significant_change_thresholds:
  min_new_events: 5              # æœ€å°‘5ä¸ªæ–°events
  min_new_events_ratio: 0.1      # æˆ–æ–°å¢10%çš„events
  min_new_authors: 3             # æˆ–æœ€å°‘3ä¸ªæ–°ä½œè€…
  min_cross_subreddit_delta: 2   # æˆ–è·¨subredditæ•°å¢åŠ 2
  min_days_since_last_score: 7   # æˆ–è·ç¦»ä¸Šæ¬¡è¯„åˆ†å·²è¿‡7å¤©
```

### 3. è¯„åˆ†æ‰¹æ¬¡ (Scoring Batch)

ä¸€ç»„éœ€è¦é‡æ–°è¯„åˆ†çš„opportunitiesï¼Œæ‰¹é‡å¤„ç†ä»¥æé«˜æ•ˆç‡ï¼š

```python
{
    "batch_id": "batch_20260104_122553",
    "trigger_type": "incremental_update",
    "clusters": [5, 26, 11, 22],
    "created_at": "2026-01-04 12:25:53",
    "status": "pending"
}
```

### 4. è¯„åˆ†ç‰ˆæœ¬ (Scoring Version)

opportunityçš„å¤šä¸ªç‰ˆæœ¬ï¼Œä¿ç•™è¯„åˆ†å†å²ï¼š

```python
{
    "opportunity_id": 5,
    "version": 2,
    "raw_total_score": 8.5,  # æ–°è¯„åˆ†
    "cluster_size_at_score": 1107,  # è¯„åˆ†æ—¶çš„clusterå¤§å°
    "scored_at": "2026-01-04 12:30:00",
    "change_reason": "Added 62 new events",
    "previous_version": 1
}
```

---

## ç³»ç»Ÿæ¶æ„

### æ•´ä½“æ¶æ„å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Pipeline Run (Full/Incremental)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   Cluster Stage (Incremental Update)   â”‚
        â”‚   - åˆå¹¶æ–°eventsåˆ°å·²æœ‰clusters          â”‚
        â”‚   - æ›´æ–°clusterå¿«ç…§                    â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  Change Detection Stage (NEW!)         â”‚
        â”‚  - æ£€æµ‹æ˜¾è‘—å˜åŒ–çš„clusters               â”‚
        â”‚  - åˆ›å»ºè¯„åˆ†æ‰¹æ¬¡                        â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                            â–¼                 â–¼                 â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  Map Opportunities     â”‚  â”‚  Update Existing â”‚  â”‚ Create New Opps â”‚
        â”‚  (ä¸ºæ–°clusters)        â”‚  â”‚  Opportunities   â”‚  â”‚ (æ–°clusters)     â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   Score Viability (Modified)           â”‚
        â”‚   - Filteringåœ¨è¯„åˆ†ä¹‹ååº”ç”¨              â”‚
        â”‚   - ä¸ºæ‰¹æ¬¡ä¸­çš„oppsè¯„åˆ†                  â”‚
        â”‚   - ä¿å­˜è¯„åˆ†ç‰ˆæœ¬                        â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   Decision Shortlist (Enhanced)        â”‚
        â”‚   - ä¼˜å…ˆè€ƒè™‘æœ€è¿‘è¯„åˆ†çš„opportunities      â”‚
        â”‚   - è€ƒè™‘clusterçš„freshness             â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### å…³é”®å˜æ›´ç‚¹

#### 1. æ–°å¢ï¼šChange Detection Stage

```python
# pipeline/change_detection.py (NEW!)

class ChangeDetector:
    """æ£€æµ‹clustersçš„æ˜¾è‘—å˜åŒ–"""

    def detect_significant_changes(
        self,
        hours: int = 24  # æ£€æŸ¥æœ€è¿‘Nå°æ—¶çš„å˜åŒ–
    ) -> List[Dict[str, Any]]:
        """æ£€æµ‹æœ€è¿‘Nå°æ—¶å†…å‘ç”Ÿæ˜¾è‘—å˜åŒ–çš„clusters"""

        return [
            {
                "cluster_id": 5,
                "change_type": "significant_new_events",
                "new_events_count": 62,
                "new_events_ratio": 0.059,  # 62/1045
                "previous_snapshot": {...},
                "current_snapshot": {...}
            },
            ...
        ]
```

#### 2. ä¿®æ”¹ï¼šMap Opportunities Stage

```python
# pipeline/map_opportunity.py (MODIFIED)

def map_opportunities_for_clusters(
    self,
    clusters_to_update: List[int] = None  # NEW: æŒ‡å®šéœ€è¦æ›´æ–°çš„clusters
) -> Dict[str, Any]:
    """ä¸ºclustersæ˜ å°„opportunities

    Args:
        clusters_to_update: æŒ‡å®šéœ€è¦æ›´æ–°opportunitiesçš„cluster IDs
                           Noneè¡¨ç¤ºåªä¸ºæ–°clustersåˆ›å»ºï¼ˆé»˜è®¤è¡Œä¸ºï¼‰
    """

    if clusters_to_update:
        # ä¸ºæŒ‡å®šçš„clustersé‡æ–°ç”Ÿæˆopportunities
        clusters = db.get_clusters_by_ids(clusters_to_update)
        # åˆ é™¤æ—§çš„opportunities
        for cluster_id in clusters_to_update:
            db.delete_opportunities_for_cluster(cluster_id)
    else:
        # é»˜è®¤è¡Œä¸ºï¼šåªä¸ºæ²¡æœ‰opportunitiesçš„clustersåˆ›å»º
        clusters = db.get_clusters_for_opportunity_mapping()
```

#### 3. ä¿®æ”¹ï¼šScore Viability Stage

```python
# pipeline/score_viability.py (MODIFIED)

class ViabilityScorer:

    def score_opportunities(
        self,
        limit: int = 100,
        batch_id: str = None,  # NEW: è¯„åˆ†æ‰¹æ¬¡ID
        skip_filtering: bool = False  # NEW: æ˜¯å¦è·³è¿‡filtering
    ) -> Dict[str, Any]:

        # 1. å…ˆè¿›è¡ŒLLMè¯„åˆ†ï¼ˆä¸å—filteringå½±å“ï¼‰
        for opportunity in opportunities:
            enhanced = self._enhance_opportunity_data(opportunity)
            llm_result = self._score_with_llm(enhanced)
            # ä¿å­˜è¯„åˆ†ç»“æœ
            self._save_scoring_version(opportunity, llm_result, batch_id)

        # 2. ç„¶ååº”ç”¨filtering rulesï¼ˆåªç”¨äºæ ‡è®°ï¼Œä¸å½±å“è¯„åˆ†ï¼‰
        if not skip_filtering and self.filtering_rules.get("enabled"):
            opportunities = self._apply_filtering_rules(opportunities)
```

#### 4. å¢å¼ºï¼šDecision Shortlist Stage

```python
# pipeline/decision_shortlist.py (ENHANCED)

class DecisionShortlistGenerator:

    def _apply_hard_filters(self) -> List[Dict[str, Any]]:
        """åº”ç”¨ç¡¬æ€§è¿‡æ»¤è§„åˆ™ï¼ˆè€ƒè™‘æ–°é²œåº¦ï¼‰"""

        # NEW: æ·»åŠ "æ–°é²œåº¦"åŠ åˆ†
        with db.get_connection("clusters") as conn:
            cursor = conn.execute("""
                SELECT
                    o.*,
                    c.cluster_size,
                    -- æ–°å¢ï¼šè®¡ç®—æ–°é²œåº¦åˆ†æ•°
                    CASE
                        WHEN o.scored_at > datetime('now', '-24 hours') THEN 1.5
                        WHEN o.scored_at > datetime('now', '-3 days') THEN 1.2
                        WHEN o.scored_at > datetime('now', '-7 days') THEN 1.0
                        ELSE 0.8
                    END as freshness_factor
                FROM opportunities o
                JOIN clusters c ON o.cluster_id = c.id
                WHERE o.raw_total_score * freshness_factor >= ?
                ORDER BY o.raw_total_score * freshness_factor DESC
            """, (min_viability,))
```

---

## æ•°æ®æ¨¡å‹è®¾è®¡

### 1. æ–°å¢è¡¨ï¼š`cluster_snapshots`

è®°å½•clusterçš„å…³é”®æŒ‡æ ‡å¿«ç…§ï¼Œç”¨äºæ£€æµ‹å˜åŒ–ï¼š

```sql
CREATE TABLE cluster_snapshots (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    cluster_id INTEGER NOT NULL,
    snapshot_time TIMESTAMP NOT NULL,

    -- ClusteræŒ‡æ ‡
    cluster_size INTEGER NOT NULL,
    unique_authors INTEGER NOT NULL,
    cross_subreddit_count INTEGER NOT NULL,
    avg_frequency_score REAL,
    latest_event_extracted_at TIMESTAMP,

    -- å…ƒæ•°æ®
    snapshot_reason TEXT,  -- 'initial', 'before_rescoring', 'periodic'
    pipeline_run_id TEXT,

    FOREIGN KEY (cluster_id) REFERENCES clusters(id)
);

CREATE INDEX idx_cluster_snapshots_cluster_id
    ON cluster_snapshots(cluster_id, snapshot_time DESC);
```

### 2. æ–°å¢è¡¨ï¼š`scoring_batches`

è®°å½•è¯„åˆ†æ‰¹æ¬¡ï¼š

```sql
CREATE TABLE scoring_batches (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    batch_id TEXT UNIQUE NOT NULL,
    trigger_type TEXT NOT NULL,  -- 'incremental_update', 'full_rebuild', 'manual'

    -- æ‰¹æ¬¡ä¿¡æ¯
    clusters_count INTEGER NOT NULL,
    cluster_ids TEXT NOT NULL,  -- JSON array

    -- çŠ¶æ€è¿½è¸ª
    status TEXT NOT NULL,  -- 'pending', 'in_progress', 'completed', 'failed'
    created_at TIMESTAMP NOT NULL,
    started_at TIMESTAMP,
    completed_at TIMESTAMP,
    error_message TEXT,

    -- ç»Ÿè®¡
    opportunities_scored INTEGER DEFAULT 0,
    opportunities_passed_filter INTEGER DEFAULT 0,
    avg_score REAL,

    FOREIGN KEY (batch_id) REFERENCES pipeline_run_results(batch_id)
);
```

### 3. æ–°å¢è¡¨ï¼š`opportunity_versions`

ä¿ç•™opportunityçš„è¯„åˆ†å†å²ï¼š

```sql
CREATE TABLE opportunity_versions (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    opportunity_id INTEGER NOT NULL,
    version INTEGER NOT NULL,

    -- ClusterçŠ¶æ€å¿«ç…§ï¼ˆè¯„åˆ†æ—¶ï¼‰
    cluster_size_at_score INTEGER NOT NULL,
    unique_authors_at_score INTEGER NOT NULL,
    cross_subreddit_at_score INTEGER NOT NULL,

    -- è¯„åˆ†ç»“æœ
    raw_total_score REAL NOT NULL,
    total_score REAL NOT NULL,
    trust_level REAL NOT NULL,
    component_scores TEXT,  -- JSON
    killer_risks TEXT,  -- JSON array
    recommendation TEXT,

    -- å…ƒæ•°æ®
    scored_at TIMESTAMP NOT NULL,
    change_reason TEXT,  -- ä¸ºä»€ä¹ˆé‡æ–°è¯„åˆ†
    batch_id TEXT,  -- å…³è”åˆ°scoring_batches
    pipeline_run_id TEXT,

    FOREIGN KEY (opportunity_id) REFERENCES opportunities(id),
    FOREIGN KEY (batch_id) REFERENCES scoring_batches(batch_id)
);

CREATE INDEX idx_opportunity_versions_opp_id_version
    ON opportunity_versions(opportunity_id, version DESC);
```

### 4. ä¿®æ”¹è¡¨ï¼š`opportunities`

æ·»åŠ æ–°å­—æ®µï¼š

```sql
-- æ·»åŠ æ–°å­—æ®µåˆ°opportunitiesè¡¨
ALTER TABLE opportunities ADD COLUMN current_version INTEGER DEFAULT 1;
ALTER TABLE opportunities ADD COLUMN last_rescored_at TIMESTAMP;
ALTER TABLE opportunities ADD COLUMN rescore_count INTEGER DEFAULT 0;
ALTER TABLE opportunities ADD COLUMN scored_at TIMESTAMP;
```

---

## è§¦å‘æ£€æµ‹æœºåˆ¶

### ç®—æ³•è®¾è®¡

```python
# pipeline/change_detection.py

def detect_significant_changes(
    self,
    hours: int = 24
) -> List[Dict[str, Any]]:

    # 1. è·å–é˜ˆå€¼é…ç½®
    thresholds = self.config.get('significant_change_thresholds', {})

    # 2. è·å–æ‰€æœ‰clustersçš„æœ€æ–°å¿«ç…§
    latest_snapshots = db.get_latest_cluster_snapshots()

    # 3. å¯¹æ¯ä¸ªclusteræ£€æŸ¥å˜åŒ–
    significant_changes = []

    for cluster in db.get_all_clusters():
        cluster_id = cluster['id']

        # 3.1 è·å–ä¸Šä¸€ä¸ªå¿«ç…§
        previous_snapshot = latest_snapshots.get(cluster_id)

        if not previous_snapshot:
            # æ–°clusterï¼Œéœ€è¦é¦–æ¬¡è¯„åˆ†
            significant_changes.append({
                "cluster_id": cluster_id,
                "change_type": "new_cluster",
                "reason": "First time scoring"
            })
            continue

        # 3.2 è®¡ç®—å˜åŒ–æŒ‡æ ‡
        current_metrics = self._calculate_cluster_metrics(cluster_id)
        previous_metrics = previous_snapshot

        # 3.3 æ£€æŸ¥æ˜¯å¦æ»¡è¶³æ˜¾è‘—å˜åŒ–æ¡ä»¶
        change_detected = False
        change_reasons = []

        # æ£€æŸ¥1: æ–°å¢eventsæ•°é‡
        new_events = current_metrics['cluster_size'] - previous_metrics['cluster_size']
        if (new_events >= thresholds['min_new_events'] or
            new_events / previous_metrics['cluster_size'] >= thresholds['min_new_events_ratio']):
            change_detected = True
            change_reasons.append(f"Added {new_events} new events")

        # æ£€æŸ¥2: æ–°å¢ä½œè€…
        new_authors = current_metrics['unique_authors'] - previous_metrics['unique_authors']
        if new_authors >= thresholds['min_new_authors']:
            change_detected = True
            change_reasons.append(f"Added {new_authors} new authors")

        # æ£€æŸ¥3: è·¨æºéªŒè¯å¢åŠ 
        cross_subreddit_delta = (current_metrics['cross_subreddit_count'] -
                                previous_metrics['cross_subreddit_count'])
        if cross_subreddit_delta >= thresholds['min_cross_subreddit_delta']:
            change_detected = True
            change_reasons.append(
                f"Cross-subreddit count increased by {cross_subreddit_delta}"
            )

        # æ£€æŸ¥4: è·ç¦»ä¸Šæ¬¡è¯„åˆ†çš„æ—¶é—´
        if previous_snapshot.get('last_scored_at'):
            days_since_last_score = (
                datetime.now() -
                datetime.fromisoformat(previous_snapshot['last_scored_at'])
            ).days
            if days_since_last_score >= thresholds['min_days_since_last_score']:
                change_detected = True
                change_reasons.append(
                    f"{days_since_last_score} days since last score"
                )

        if change_detected:
            significant_changes.append({
                "cluster_id": cluster_id,
                "change_type": "significant_update",
                "reasons": change_reasons,
                "previous_snapshot": previous_metrics,
                "current_snapshot": current_metrics
            })

    return significant_changes
```

### è§¦å‘æ¡ä»¶é…ç½®

```yaml
# config/thresholds.yaml (æ–°å¢)

# æ˜¾è‘—å˜åŒ–æ£€æµ‹é˜ˆå€¼
significant_change_thresholds:
  # æ–°å¢eventsè§¦å‘æ¡ä»¶ï¼ˆæ»¡è¶³ä»»ä¸€å³å¯ï¼‰
  min_new_events: 5              # ç»å¯¹å€¼ï¼šæœ€å°‘5ä¸ªæ–°events
  min_new_events_ratio: 0.1      # ç›¸å¯¹å€¼ï¼šæ–°å¢10%çš„events

  # æ–°å¢ä½œè€…è§¦å‘æ¡ä»¶
  min_new_authors: 3             # æœ€å°‘3ä¸ªæ–°ä½œè€…

  # è·¨æºéªŒè¯è§¦å‘æ¡ä»¶
  min_cross_subreddit_delta: 2   # è·¨subredditæ•°å¢åŠ 2

  # æ—¶é—´è§¦å‘æ¡ä»¶
  min_days_since_last_score: 7   # è·ç¦»ä¸Šæ¬¡è¯„åˆ†å·²è¿‡7å¤©

  # å‘¨æœŸæ€§å…¨é‡æ›´æ–°
  periodic_full_rescore_days: 30  # æ¯30å¤©å…¨é‡é‡æ–°è¯„åˆ†ä¸€æ¬¡
```

---

## è¯„åˆ†ç­–ç•¥è®¾è®¡

### ç­–ç•¥1ï¼šå¢é‡æ›´æ–°è¯„åˆ†

**é€‚ç”¨åœºæ™¯**ï¼šClustersè·å¾—äº†æ–°çš„eventsæˆ–æŒ‡æ ‡

**æµç¨‹**ï¼š
```python
# 1. æ£€æµ‹å˜åŒ–
changes = detector.detect_significant_changes(hours=24)

# 2. åˆ›å»ºè¯„åˆ†æ‰¹æ¬¡
batch_id = db.create_scoring_batch(
    trigger_type="incremental_update",
    cluster_ids=[c['cluster_id'] for c in changes]
)

# 3. ä¸ºè¿™äº›clustersé‡æ–°ç”Ÿæˆopportunities
mapper.map_opportunities_for_clusters(
    clusters_to_update=[c['cluster_id'] for c in changes]
)

# 4. è¯„åˆ†ï¼ˆè·³è¿‡filteringï¼Œå› ä¸ºè¿™æ˜¯æ›´æ–°ï¼‰
scorer.score_opportunities(
    batch_id=batch_id,
    skip_filtering=True  # å…³é”®ï¼šè·³è¿‡filtering
)

# 5. ä¿å­˜è¯„åˆ†ç‰ˆæœ¬
db.save_opportunity_versions(batch_id)
```

**å…³é”®ç‚¹**ï¼š
- åªä¸ºæ˜¾è‘—å˜åŒ–çš„clustersé‡æ–°è¯„åˆ†
- è·³è¿‡filtering rulesï¼ˆå› ä¸ºæ˜¯æ›´æ–°ï¼Œä¸æ˜¯æ–°åˆ›å»ºï¼‰
- ä¿ç•™è¯„åˆ†å†å²ï¼Œä¾¿äºå›æ»š

### ç­–ç•¥2ï¼šé¦–æ¬¡è¯„åˆ†ï¼ˆå®½æ¾filteringï¼‰

**é€‚ç”¨åœºæ™¯**ï¼šæ–°åˆ›å»ºçš„clustersé¦–æ¬¡è¯„åˆ†

**æµç¨‹**ï¼š
```python
# 1. è·å–æ–°åˆ›å»ºçš„clusters
new_clusters = db.get_new_clusters(hours=24)

# 2. åˆ›å»ºopportunities
mapper.map_opportunities_for_clusters(
    clusters_to_update=[c['id'] for c in new_clusters]
)

# 3. è¯„åˆ†ï¼ˆåº”ç”¨å®½æ¾çš„filteringï¼‰
scorer.score_opportunities(
    batch_id=batch_id,
    # å…³é”®ï¼šä½¿ç”¨å®½æ¾çš„filteringè§„åˆ™
    filtering_rules_override={
        "min_cluster_size": 3,        # ä»5é™è‡³3
        "min_unique_authors": 2,      # ä»4é™è‡³2
        "min_avg_frequency_score": 4.0  # ä»5.0é™è‡³4.0
    }
)
```

**å…³é”®ç‚¹**ï¼š
- æ–°clustersä½¿ç”¨å®½æ¾çš„filteringè§„åˆ™
- è®©æ›´å¤šopportunitiesè¿›å…¥è¯„åˆ†æµç¨‹
- å³ä½¿æœ€ç»ˆè¢«æ ‡è®°ä¸º"abandon"ï¼Œä¹Ÿæœ‰LLMè¯„åˆ†ç»“æœ

### ç­–ç•¥3ï¼šå‘¨æœŸæ€§å…¨é‡æ›´æ–°

**é€‚ç”¨åœºæ™¯**ï¼šå®šæœŸå…¨é‡é‡æ–°è¯„åˆ†

**æµç¨‹**ï¼š
```python
# æ¯30å¤©æ‰§è¡Œä¸€æ¬¡
if should_run_full_rescore():
    # 1. æ ‡è®°æ‰€æœ‰opportunitieséœ€è¦æ›´æ–°
    all_clusters = db.get_all_cluster_ids()

    # 2. åˆ›å»ºå…¨é‡è¯„åˆ†æ‰¹æ¬¡
    batch_id = db.create_scoring_batch(
        trigger_type="full_rebuild",
        cluster_ids=all_clusters
    )

    # 3. é€æ‰¹å¤„ç†ï¼ˆé¿å…LLM APIé™æµï¼‰
    for i in range(0, len(all_clusters), batch_size):
        batch = all_clusters[i:i+batch_size]
        mapper.map_opportunities_for_clusters(clusters_to_update=batch)
        scorer.score_opportunities(batch_id=batch_id, skip_filtering=True)
```

**å…³é”®ç‚¹**ï¼š
- å®šæœŸå…¨é‡æ›´æ–°ï¼Œæ•æ‰é•¿æœŸè¶‹åŠ¿å˜åŒ–
- åˆ†æ‰¹å¤„ç†ï¼Œé¿å…APIé™æµ
- å¯ä»¥åœ¨å¤œé—´æˆ–ä½å³°æœŸæ‰§è¡Œ

---

## å®ç°è®¡åˆ’

### Phase 1: æ•°æ®æ¨¡å‹ï¼ˆ1-2å¤©ï¼‰

**ä»»åŠ¡æ¸…å•**ï¼š
- [ ] åˆ›å»º`cluster_snapshots`è¡¨
- [ ] åˆ›å»º`scoring_batches`è¡¨
- [ ] åˆ›å»º`opportunity_versions`è¡¨
- [ ] ä¿®æ”¹`opportunities`è¡¨ï¼ˆæ·»åŠ æ–°å­—æ®µï¼‰
- [ ] ç¼–å†™æ•°æ®åº“è¿ç§»è„šæœ¬
- [ ] ç¼–å†™å•å…ƒæµ‹è¯•ï¼ˆæ•°æ®åº“æ“ä½œï¼‰

**éªŒæ”¶æ ‡å‡†**ï¼š
- æ‰€æœ‰è¡¨åˆ›å»ºæˆåŠŸï¼Œç´¢å¼•æ­£ç¡®
- è¿ç§»è„šæœ¬å¯ä»¥å®‰å…¨åœ°å‡çº§ç°æœ‰æ•°æ®åº“
- å•å…ƒæµ‹è¯•è¦†ç›–æ‰€æœ‰CRUDæ“ä½œ

### Phase 2: Change Detectionï¼ˆ2-3å¤©ï¼‰

**ä»»åŠ¡æ¸…å•**ï¼š
- [ ] å®ç°`ChangeDetector`ç±»
- [ ] å®ç°`detect_significant_changes()`æ–¹æ³•
- [ ] å®ç°`_calculate_cluster_metrics()`æ–¹æ³•
- [ ] æ·»åŠ é…ç½®é¡¹åˆ°`thresholds.yaml`
- [ ] ç¼–å†™å•å…ƒæµ‹è¯•ï¼ˆå„ç§è§¦å‘æ¡ä»¶ï¼‰
- [ ] é›†æˆåˆ°pipelineï¼ˆæ–°å¢stageï¼‰

**éªŒæ”¶æ ‡å‡†**ï¼š
- èƒ½æ­£ç¡®æ£€æµ‹æ–°å¢eventsã€ä½œè€…ã€è·¨æºç­‰å˜åŒ–
- å•å…ƒæµ‹è¯•è¦†ç›–æ‰€æœ‰è§¦å‘æ¡ä»¶
- é›†æˆæµ‹è¯•ï¼špipelineèƒ½æ­£å¸¸è¿è¡Œ

### Phase 3: Enhanced Scoringï¼ˆ3-4å¤©ï¼‰

**ä»»åŠ¡æ¸…å•**ï¼š
- [ ] ä¿®æ”¹`ViabilityScorer.score_opportunities()`æ–¹æ³•
  - [ ] æ·»åŠ `skip_filtering`å‚æ•°
  - [ ] æ·»åŠ `batch_id`å‚æ•°
  - [ ] æ·»åŠ `filtering_rules_override`å‚æ•°
- [ ] å®ç°`_save_scoring_version()`æ–¹æ³•
- [ ] ä¿®æ”¹`_apply_filtering_rules()`ä¸ºè¯„åˆ†åå¤„ç†
- [ ] ç¼–å†™å•å…ƒæµ‹è¯•ï¼ˆå„ç§è¯„åˆ†åœºæ™¯ï¼‰

**éªŒæ”¶æ ‡å‡†**ï¼š
- Filteringåœ¨LLMè¯„åˆ†**ä¹‹å**åº”ç”¨
- è¯„åˆ†ç‰ˆæœ¬æ­£ç¡®ä¿å­˜åˆ°`opportunity_versions`è¡¨
- å•å…ƒæµ‹è¯•è¦†ç›–ï¼šé¦–æ¬¡è¯„åˆ†ã€é‡æ–°è¯„åˆ†ã€filteringå„ç§æƒ…å†µ

### Phase 4: Enhanced Decision Shortlistï¼ˆ2å¤©ï¼‰

**ä»»åŠ¡æ¸…å•**ï¼š
- [ ] ä¿®æ”¹`DecisionShortlistGenerator._apply_hard_filters()`
- [ ] æ·»åŠ æ–°é²œåº¦è®¡ç®—é€»è¾‘
- [ ] æ›´æ–°æ’åºç®—æ³•ï¼ˆç»¼åˆè€ƒè™‘åˆ†æ•°å’Œæ–°é²œåº¦ï¼‰
- [ ] æ·»åŠ é…ç½®é¡¹ï¼ˆæ–°é²œåº¦æƒé‡ï¼‰
- [ ] ç¼–å†™å•å…ƒæµ‹è¯•

**éªŒæ”¶æ ‡å‡†**ï¼š
- æœ€è¿‘è¯„åˆ†çš„opportunitiesæœ‰æ›´é«˜ä¼˜å…ˆçº§
- å¯ä»¥é…ç½®æ–°é²œåº¦æƒé‡
- Reportèƒ½åæ˜ æœ€æ–°æ•°æ®

### Phase 5: Pipeline Integrationï¼ˆ2-3å¤©ï¼‰

**ä»»åŠ¡æ¸…å•**ï¼š
- [ ] æ›´æ–°`pipeline/main.py`
- [ ] æ·»åŠ `--rescore`å‚æ•°ï¼ˆæ”¯æŒæ‰‹åŠ¨è§¦å‘é‡æ–°è¯„åˆ†ï¼‰
- [ ] å®ç°æ‰¹æ¬¡å¤„ç†é€»è¾‘
- [ ] æ·»åŠ è¿›åº¦æŠ¥å‘Šå’Œæ—¥å¿—
- [ ] ç¼–å†™é›†æˆæµ‹è¯•

**éªŒæ”¶æ ‡å‡†**ï¼š
- `--stage full`è‡ªåŠ¨åŒ…å«change detection
- `--rescore cluster_id`æ”¯æŒæ‰‹åŠ¨è§¦å‘å•ä¸ªclusteré‡æ–°è¯„åˆ†
- é›†æˆæµ‹è¯•ç«¯åˆ°ç«¯è¿è¡Œ

### Phase 6: Testing & Validationï¼ˆ2-3å¤©ï¼‰

**ä»»åŠ¡æ¸…å•**ï¼š
- [ ] å•å…ƒæµ‹è¯•ï¼ˆç›®æ ‡è¦†ç›–ç‡ï¼š80%+ï¼‰
- [ ] é›†æˆæµ‹è¯•
- [ ] æ€§èƒ½æµ‹è¯•ï¼ˆç¡®ä¿ä¸æ˜¾è‘—å˜æ…¢ï¼‰
- [ ] å›å½’æµ‹è¯•ï¼ˆç¡®ä¿ç°æœ‰åŠŸèƒ½ä¸å—å½±å“ï¼‰
- [ ] æ‰‹åŠ¨æµ‹è¯•ï¼ˆä½¿ç”¨çœŸå®æ•°æ®ï¼‰

**éªŒæ”¶æ ‡å‡†**ï¼š
- æ‰€æœ‰æµ‹è¯•é€šè¿‡
- æ€§èƒ½æµ‹è¯•ï¼špipelineè¿è¡Œæ—¶é—´å¢åŠ  < 20%
- æ‰‹åŠ¨æµ‹è¯•ï¼šReportèƒ½åæ˜ æœ€è¿‘æ›´æ–°çš„clusters

### Phase 7: Documentation & Deploymentï¼ˆ1-2å¤©ï¼‰

**ä»»åŠ¡æ¸…å•**ï¼š
- [ ] æ›´æ–°READMEï¼ˆæ–°åŠŸèƒ½è¯´æ˜ï¼‰
- [ ] ç¼–å†™ä½¿ç”¨æ–‡æ¡£ï¼ˆå¦‚ä½•è§¦å‘é‡æ–°è¯„åˆ†ï¼‰
- [ ] ç¼–å†™æ¶æ„æ–‡æ¡£ï¼ˆç³»ç»Ÿè®¾è®¡ï¼‰
- [ ] å‡†å¤‡éƒ¨ç½²è„šæœ¬
- [ ] å‡†å¤‡å›æ»šæ–¹æ¡ˆ

**éªŒæ”¶æ ‡å‡†**ï¼š
- æ–‡æ¡£å®Œæ•´ã€æ¸…æ™°
- éƒ¨ç½²è„šæœ¬æµ‹è¯•é€šè¿‡
- å›æ»šæ–¹æ¡ˆéªŒè¯

**æ€»ä¼°ç®—æ—¶é—´**: 13-19å¤©

---

## é£é™©è¯„ä¼°

### é£é™©1ï¼šLLM APIæˆæœ¬å¢åŠ 

**é£é™©ç­‰çº§**: ğŸ”´ é«˜

**æè¿°**ï¼š
- é‡æ–°è¯„åˆ†ä¼šå¢åŠ LLM APIè°ƒç”¨
- å¦‚æœæ¯æ¬¡pipelineè¿è¡Œéƒ½è§¦å‘å¤§é‡é‡æ–°è¯„åˆ†ï¼Œæˆæœ¬å¯èƒ½å¤±æ§

**ç¼“è§£æªæ–½**ï¼š
1. è®¾ç½®æ¯æ—¥/æ¯æœˆLLMè°ƒç”¨é¢„ç®—ä¸Šé™
2. ä½¿ç”¨æ›´ä¸¥æ ¼çš„è§¦å‘é˜ˆå€¼ï¼ˆå‡å°‘ä¸å¿…è¦çš„é‡æ–°è¯„åˆ†ï¼‰
3. æ‰¹é‡å¤„ç†ï¼Œå‡å°‘APIè°ƒç”¨å¼€é”€
4. å®ç°æ™ºèƒ½ç¼“å­˜ï¼ˆç›¸ä¼¼clusterså¤ç”¨è¯„åˆ†ç»“æœï¼‰

**ç›‘æ§æŒ‡æ ‡**ï¼š
- æ¯æ—¥LLM APIè°ƒç”¨æ¬¡æ•°
- æ¯æ—¥LLM APIæˆæœ¬
- æ¯ä¸ªpipelineè¿è¡Œçš„è¯„åˆ†æ¬¡æ•°

### é£é™©2ï¼šè¯„åˆ†ä¸ä¸€è‡´

**é£é™©ç­‰çº§**: ğŸŸ¡ ä¸­

**æè¿°**ï¼š
- åŒä¸€ä¸ªclusteråœ¨ä¸åŒæ—¶é—´è¯„åˆ†ï¼Œå¯èƒ½å¾—åˆ°ä¸åŒçš„åˆ†æ•°
- ç”¨æˆ·å¯èƒ½ä¼šå›°æƒ‘ï¼šä¸ºä»€ä¹ˆåŒä¸€ä¸ªopportunityåˆ†æ•°å˜äº†ï¼Ÿ

**ç¼“è§£æªæ–½**ï¼š
1. ä¿ç•™è¯„åˆ†å†å²ï¼Œå¯ä»¥çœ‹åˆ°å˜åŒ–è¶‹åŠ¿
2. åœ¨opportunityä¸­æ ‡æ³¨"ä¸Šæ¬¡è¯„åˆ†æ—¶é—´"
3. åœ¨Reportä¸­æ˜¾ç¤ºè¯„åˆ†æ—¶é—´
4. æ·»åŠ "è¯„åˆ†å˜åŒ–åŸå› "è¯´æ˜

**ç›‘æ§æŒ‡æ ‡**ï¼š
- è¯„åˆ†æ–¹å·®ï¼ˆåŒä¸€opportunityä¸åŒç‰ˆæœ¬çš„åˆ†æ•°å·®å¼‚ï¼‰
- ç”¨æˆ·åé¦ˆï¼ˆæ˜¯å¦å¯¹è¯„åˆ†å˜åŒ–æ„Ÿåˆ°å›°æƒ‘ï¼‰

### é£é™©3ï¼šæ•°æ®åº“æ€§èƒ½ä¸‹é™

**é£é™©ç­‰çº§**: ğŸŸ¡ ä¸­

**æè¿°**ï¼š
- æ–°å¢3ä¸ªè¡¨ï¼Œå¯èƒ½å¢åŠ æŸ¥è¯¢æ—¶é—´
- `opportunity_versions`è¡¨ä¼šå¿«é€Ÿå¢é•¿

**ç¼“è§£æªæ–½**ï¼š
1. æ·»åŠ é€‚å½“çš„ç´¢å¼•
2. å®šæœŸæ¸…ç†æ—§çš„è¯„åˆ†ç‰ˆæœ¬ï¼ˆåªä¿ç•™æœ€è¿‘Nä¸ªç‰ˆæœ¬ï¼‰
3. ä½¿ç”¨æ•°æ®åº“æŸ¥è¯¢ä¼˜åŒ–ï¼ˆé¿å…N+1æŸ¥è¯¢ï¼‰

**ç›‘æ§æŒ‡æ ‡**ï¼š
- æ•°æ®åº“æŸ¥è¯¢æ—¶é—´
- Pipelineå„ä¸ªstageçš„è¿è¡Œæ—¶é—´

### é£é™©4ï¼šPipelineæ‰§è¡Œæ—¶é—´å¢åŠ 

**é£é™©ç­‰çº§**: ğŸŸ¢ ä½

**æè¿°**ï¼š
- æ–°å¢Change Detection stage
- é‡æ–°è¯„åˆ†ä¼šå¢åŠ LLM APIè°ƒç”¨æ—¶é—´

**ç¼“è§£æªæ–½**ï¼š
1. Change Detectionä½¿ç”¨çº¯SQLæŸ¥è¯¢ï¼Œåº”è¯¥å¾ˆå¿«ï¼ˆ< 1ç§’ï¼‰
2. é‡æ–°è¯„åˆ†æ˜¯å¹¶è¡Œçš„ï¼Œå¯ä»¥æ§åˆ¶å¹¶å‘æ•°
3. å¯ä»¥è®¾ç½®"æœ€å¤§é‡æ–°è¯„åˆ†æ•°"ä¸Šé™

**ç›‘æ§æŒ‡æ ‡**ï¼š
- Pipelineæ€»è¿è¡Œæ—¶é—´
- å„stageè¿è¡Œæ—¶é—´å æ¯”

---

## æ€§èƒ½è€ƒè™‘

### æ•°æ®åº“æŸ¥è¯¢ä¼˜åŒ–

```sql
-- 1. æ‰¹é‡è·å–clusteræŒ‡æ ‡ï¼ˆé¿å…N+1æŸ¥è¯¢ï¼‰
WITH cluster_metrics AS (
    SELECT
        pe.cluster_id,
        COUNT(DISTINCT pe.id) as cluster_size,
        COUNT(DISTINCT fp.author) as unique_authors,
        COUNT(DISTINCT fp.subreddit) as cross_subreddit_count,
        MAX(pe.extracted_at) as latest_event_extracted_at
    FROM pain_events pe
    JOIN filtered_posts fp ON pe.post_id = fp.id
    WHERE pe.cluster_id IN (1, 2, 3, ...)  -- æ‰¹é‡æŸ¥è¯¢
    GROUP BY pe.cluster_id
)
SELECT * FROM cluster_metrics;

-- 2. ä½¿ç”¨ç´¢å¼•åŠ é€ŸæŸ¥è¯¢
CREATE INDEX idx_pain_events_cluster_id
    ON pain_events(cluster_id);
CREATE INDEX idx_opportunities_cluster_id
    ON opportunities(cluster_id);
```

### LLM APIè°ƒç”¨ä¼˜åŒ–

```python
# 1. æ‰¹é‡å¤„ç†ï¼ˆå‡å°‘APIè°ƒç”¨å¼€é”€ï¼‰
async def score_batch(opportunities: List[Dict]) -> List[Dict]:
    """æ‰¹é‡è¯„åˆ†ï¼Œå¹¶å‘æ‰§è¡Œ"""
    tasks = [
        llm_client.score_viability(opp)
        for opp in opportunities
    ]
    results = await asyncio.gather(*tasks)
    return results

# 2. æ™ºèƒ½å»é‡ï¼ˆç›¸ä¼¼clusterså¤ç”¨è¯„åˆ†ï¼‰
def is_similar_cluster(cluster1: Dict, cluster2: Dict) -> bool:
    """åˆ¤æ–­ä¸¤ä¸ªclustersæ˜¯å¦ç›¸ä¼¼ï¼ˆå¯ä»¥å¤ç”¨è¯„åˆ†ï¼‰"""
    # åŸºäºcluster_nameå’Œcentroid_summaryçš„ç›¸ä¼¼åº¦
    similarity = calculate_text_similarity(
        cluster1['cluster_name'],
        cluster2['cluster_name']
    )
    return similarity > 0.9
```

### å¹¶å‘æ§åˆ¶

```python
# 3. é™åˆ¶å¹¶å‘æ•°ï¼ˆé¿å…APIé™æµï¼‰
MAX_CONCURRENT_LLM_CALLS = 5

async def score_with_rate_limit(opportunities: List[Dict]):
    semaphore = asyncio.Semaphore(MAX_CONCURRENT_LLM_CALLS)

    async def score_one(opp):
        async with semaphore:
            return await llm_client.score_viability(opp)

    tasks = [score_one(opp) for opp in opportunities]
    return await asyncio.gather(*tasks)
```

### ç¼“å­˜ç­–ç•¥

```python
# 4. ç¼“å­˜clusterå¿«ç…§ï¼ˆå‡å°‘é‡å¤è®¡ç®—ï¼‰
from functools import lru_cache

@lru_cache(maxsize=128)
def get_cluster_metrics(cluster_id: int, snapshot_time: str) -> Dict:
    """è·å–clusteræŒ‡æ ‡ï¼ˆå¸¦ç¼“å­˜ï¼‰"""
    return db._calculate_cluster_metrics_uncached(cluster_id)
```

---

## é…ç½®ç¤ºä¾‹

### å®Œæ•´çš„é…ç½®æ–‡ä»¶

```yaml
# config/thresholds.yaml (å®Œæ•´ç‰ˆ)

# ... ç°æœ‰é…ç½® ...

# æ˜¾è‘—å˜åŒ–æ£€æµ‹é˜ˆå€¼ï¼ˆæ–°å¢ï¼‰
significant_change_thresholds:
  # æ–°å¢eventsè§¦å‘æ¡ä»¶
  min_new_events: 5
  min_new_events_ratio: 0.1

  # æ–°å¢ä½œè€…è§¦å‘æ¡ä»¶
  min_new_authors: 3

  # è·¨æºéªŒè¯è§¦å‘æ¡ä»¶
  min_cross_subreddit_delta: 2

  # æ—¶é—´è§¦å‘æ¡ä»¶
  min_days_since_last_score: 7

  # å‘¨æœŸæ€§å…¨é‡æ›´æ–°
  periodic_full_rescore_days: 30

# å¢é‡è¯„åˆ†é…ç½®ï¼ˆæ–°å¢ï¼‰
incremental_scoring:
  enabled: true

  # æ˜¯å¦è‡ªåŠ¨è§¦å‘é‡æ–°è¯„åˆ†
  auto_trigger_enabled: true

  # æ¯æ¬¡pipelineè¿è¡Œæœ€å¤§é‡æ–°è¯„åˆ†æ•°
  max_rescores_per_run: 10

  # æ˜¯å¦ä¿ç•™è¯„åˆ†å†å²
  keep_scoring_history: true

  # ä¿ç•™å¤šå°‘ä¸ªå†å²ç‰ˆæœ¬
  max_history_versions: 5

  # æ–°clustersé¦–æ¬¡è¯„åˆ†çš„å®½æ¾filteringè§„åˆ™
  new_cluster_filtering_override:
    min_cluster_size: 3
    min_unique_authors: 2
    min_cross_subreddit_count: 1
    min_avg_frequency_score: 4.0

# Decision shortlisté…ç½®ï¼ˆä¿®æ”¹ï¼‰
decision_shortlist:
  # æ–°å¢ï¼šæ–°é²œåº¦æƒé‡
  freshness:
    enabled: true
    weights:
      last_24h: 1.5      # æœ€è¿‘24å°æ—¶è¯„åˆ†çš„ï¼Œæƒé‡Ã—1.5
      last_3_days: 1.2   # æœ€è¿‘3å¤©è¯„åˆ†çš„ï¼Œæƒé‡Ã—1.2
      last_7_days: 1.0   # æœ€è¿‘7å¤©è¯„åˆ†çš„ï¼Œæƒé‡Ã—1.0
      older: 0.8         # æ›´æ—©çš„ï¼Œæƒé‡Ã—0.8

  # åŸæœ‰é…ç½®ï¼ˆä¿æŒä¸å˜ï¼‰
  min_viability_score: 6.0
  min_cluster_size: 4
  min_trust_level: 0.5
  # ...
```

---

## ä½¿ç”¨ç¤ºä¾‹

### ç¤ºä¾‹1ï¼šè‡ªåŠ¨å¢é‡æ›´æ–°

```bash
# è¿è¡Œå®Œæ•´pipelineï¼ˆè‡ªåŠ¨æ£€æµ‹å¹¶é‡æ–°è¯„åˆ†æ˜¾è‘—å˜åŒ–çš„clustersï¼‰
python pain_point_analyzer.py --stage full

# æ—¥å¿—è¾“å‡ºï¼š
# INFO: Detected 3 clusters with significant changes
# INFO:   - Cluster 5: Added 62 new events (5.9% increase)
# INFO:   - Cluster 26: Added 16 new events (114% increase)
# INFO:   - Cluster 11: Added 11 new events (26.8% increase)
# INFO: Created scoring batch: batch_20260104_122553
# INFO: Re-scoring 3 opportunities (skipping filtering for updates)
# INFO:   Opportunity 5: 8.31 â†’ 8.45 (â†‘ 0.14)
# INFO:   Opportunity 26: 7.67 â†’ 7.82 (â†‘ 0.15)
# INFO:   Opportunity 11: 7.27 â†’ 7.35 (â†‘ 0.08)
```

### ç¤ºä¾‹2ï¼šæ‰‹åŠ¨è§¦å‘é‡æ–°è¯„åˆ†

```bash
# é‡æ–°è¯„åˆ†æŒ‡å®šçš„cluster
python pain_point_analyzer.py --rescore 5

# æ—¥å¿—è¾“å‡ºï¼š
# INFO: Manual re-scoring triggered for cluster 5
# INFO: Cluster size: 1107 events
# INFO: Previous score: 8.31 (scored on 2025-12-31)
# INFO: Re-scoring...
# INFO: New score: 8.45 (â†‘ 0.14)
# INFO: Saved as version 2
```

### ç¤ºä¾‹3ï¼šæŸ¥çœ‹è¯„åˆ†å†å²

```bash
# æŸ¥çœ‹æŸä¸ªopportunityçš„è¯„åˆ†å†å²
python -m utils.opportunity_history --opportunity-id 5

# è¾“å‡ºï¼š
# Opportunity: LifeSpark (ID: 5)
#
# Version 2 (Current):
#   Score: 8.45
#   Cluster Size: 1107
#   Scored At: 2026-01-04 12:30:00
#   Change Reason: Added 62 new events
#
# Version 1:
#   Score: 8.31
#   Cluster Size: 1045
#   Scored At: 2025-12-31 12:09:36
#   Change Reason: Initial scoring
```

### ç¤ºä¾‹4ï¼šè‡ªå®šä¹‰è§¦å‘é˜ˆå€¼

```bash
# ä½¿ç”¨æ›´å®½æ¾çš„è§¦å‘é˜ˆå€¼
python pain_point_analyzer.py \
    --stage full \
    --min-new-events 3 \
    --min-new-events-ratio 0.05

# æ—¥å¿—è¾“å‡ºï¼š
# INFO: Using custom thresholds: min_new_events=3, min_new_events_ratio=0.05
# INFO: Detected 8 clusters with significant changes (with custom thresholds)
# ...
```

---

## æ€»ç»“

### æ ¸å¿ƒè®¾è®¡åŸåˆ™

1. **æ¸è¿›å¼å¢å¼º**: åœ¨ç°æœ‰æ¶æ„ä¸Šå¢é‡æ”¹è¿›ï¼Œä¸é‡å†™æ•´ä¸ªpipeline
2. **æ•°æ®é©±åŠ¨**: åŸºäºçœŸå®æ•°æ®åˆ†æè®¾è®¡è§¦å‘æ¡ä»¶
3. **æˆæœ¬å¯æ§**: é€šè¿‡é˜ˆå€¼å’Œæ‰¹æ¬¡æ§åˆ¶LLMè°ƒç”¨æˆæœ¬
4. **å¯è§‚æµ‹æ€§**: ä¿ç•™è¯„åˆ†å†å²ï¼Œä¾¿äºåˆ†æå’Œè°ƒè¯•
5. **å‘åå…¼å®¹**: ä¸ç ´åç°æœ‰åŠŸèƒ½ï¼Œæ–°åŠŸèƒ½å¯é€‰å¯ç”¨

### é¢„æœŸæ•ˆæœ

**é—®é¢˜è§£å†³**ï¼š
- âœ… Clustersè·å¾—æ–°eventsåï¼Œä¼šè‡ªåŠ¨è§¦å‘é‡æ–°è¯„åˆ†
- âœ… æ–°åˆ›å»ºçš„å°clustersæœ‰æœºä¼šè¢«LLMè¯„åˆ†ï¼ˆå®½æ¾filteringï¼‰
- âœ… Decision shortlistèƒ½åæ˜ æœ€æ–°çš„clusterçŠ¶æ€

**æ€§èƒ½å½±å“**ï¼š
- â±ï¸ Pipelineè¿è¡Œæ—¶é—´å¢åŠ ï¼šä¼°è®¡10-20%ï¼ˆä¸»è¦æ˜¯æ–°å¢stageå’ŒLLMè°ƒç”¨ï¼‰
- ğŸ’¾ æ•°æ®åº“å¤§å°å¢åŠ ï¼šæ¯ä¸ªopportunityçº¦å¢åŠ 5-10ä¸ªç‰ˆæœ¬è®°å½•/å¹´
- ğŸ’° LLM APIæˆæœ¬å¢åŠ ï¼šä¼°è®¡10-30%ï¼ˆå–å†³äºé‡æ–°è¯„åˆ†é¢‘ç‡ï¼‰

**ä¸‹ä¸€æ­¥**ï¼š
1. Reviewå¹¶æ‰¹å‡†æ­¤è®¾è®¡æ–‡æ¡£
2. å¼€å§‹Phase 1: æ•°æ®æ¨¡å‹å®ç°
3. æ¯ä¸ªPhaseå®Œæˆåè¿›è¡Œreviewå’Œè°ƒæ•´

---

**æ–‡æ¡£ç‰ˆæœ¬**: 1.0
**æœ€åæ›´æ–°**: 2026-01-04
**ä½œè€…**: Claude (UltraThink Mode)
