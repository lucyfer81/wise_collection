#!/usr/bin/env python3
"""
è·¨æºå¯¹é½åŠŸèƒ½æ¼”ç¤ºå’Œæµ‹è¯•
"""
import sys
import json
import time
from utils.db import WiseCollectionDB
from utils.llm_client import LLMClient
from pipeline.align_cross_sources import CrossSourceAligner

def create_test_data():
    """åˆ›å»ºæµ‹è¯•èšç±»æ•°æ®"""
    db = WiseCollectionDB(':memory:', unified=True)
    db._init_unified_database()

    # æ¨¡æ‹ŸRedditå’ŒHackerNewsçš„èšç±»æ•°æ®
    test_clusters = [
        {
            'cluster_name': 'reddit_deployment_pain',
            'source_type': 'reddit',
            'centroid_summary': 'Developers struggling with complex deployment pipelines and manual configuration management',
            'common_pain': 'Manual deployment steps, configuration drift, deployment failures',
            'pain_event_ids': json.dumps(['1', '2', '3', '4', '5']),
            'cluster_size': 5
        },
        {
            'cluster_name': 'hn_deployment_challenges',
            'source_type': 'hn_ask',
            'centroid_summary': 'What are the best practices for deployment automation? Current process is error-prone',
            'common_pain': 'Deployment automation issues, lack of CI/CD',
            'pain_event_ids': json.dumps(['6', '7', '8']),
            'cluster_size': 3
        },
        {
            'cluster_name': 'reddit_api_documentation',
            'source_type': 'reddit',
            'centroid_summary': 'Poor API documentation making integration difficult for developers',
            'common_pain': 'Missing examples, unclear endpoints',
            'pain_event_ids': json.dumps(['9', '10']),
            'cluster_size': 2
        },
        {
            'cluster_name': 'hn_database_performance',
            'source_type': 'hn_ask',
            'centroid_summary': 'Database queries running slowly on large datasets, optimization strategies needed',
            'common_pain': 'Query optimization challenges',
            'pain_event_ids': json.dumps(['11', '12']),
            'cluster_size': 2
        }
    ]

    # æ’å…¥æµ‹è¯•èšç±»
    with db.get_connection("raw") as conn:
        for cluster in test_clusters:
            conn.execute("""
                INSERT INTO clusters (cluster_name, source_type, centroid_summary,
                                    common_pain, pain_event_ids, cluster_size)
                VALUES (?, ?, ?, ?, ?, ?)
            """, (
                cluster['cluster_name'],
                cluster['source_type'],
                cluster['centroid_summary'],
                cluster['common_pain'],
                cluster['pain_event_ids'],
                cluster['cluster_size']
            ))
        conn.commit()

    print(f"âœ… åˆ›å»ºäº† {len(test_clusters)} ä¸ªæµ‹è¯•èšç±»")
    return db

def test_alignment_workflow():
    """æµ‹è¯•å®Œæ•´çš„å¯¹é½å·¥ä½œæµç¨‹"""
    print("ğŸš€ å¼€å§‹æµ‹è¯•è·¨æºå¯¹é½å·¥ä½œæµç¨‹...")

    # åˆ›å»ºæµ‹è¯•æ•°æ®
    db = create_test_data()

    # åˆå§‹åŒ–LLMå®¢æˆ·ç«¯ï¼ˆä½¿ç”¨mocké…ç½®ï¼‰
    try:
        llm_client = LLMClient({
            'models': {
                'main': 'gpt-4',
                'medium': 'gpt-3.5-turbo',
                'small': 'gpt-3.5-turbo'
            },
            'api_key': 'mock-key-for-testing'
        })
        print("âœ… LLMå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
    except Exception as e:
        print(f"âš ï¸  LLMå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥ï¼ˆå¯èƒ½ç¼ºå°‘APIå¯†é’¥ï¼‰: {e}")
        # åˆ›å»ºä¸€ä¸ªç®€å•çš„mock LLMå®¢æˆ·ç«¯ç”¨äºæµ‹è¯•
        class MockLLMClient:
            def get_completion(self, prompt, model_type="main", max_tokens=1000, temperature=0.1):
                return """[]"""
        llm_client = MockLLMClient()
        print("âœ… ä½¿ç”¨Mock LLMå®¢æˆ·ç«¯è¿›è¡Œæµ‹è¯•")

    # åˆ›å»ºå¯¹é½å™¨
    aligner = CrossSourceAligner(db, llm_client)
    print("âœ… è·¨æºå¯¹é½å™¨åˆ›å»ºæˆåŠŸ")

    # æµ‹è¯•è·å–æœªå¤„ç†çš„èšç±»
    unprocessed_clusters = aligner.get_unprocessed_clusters()
    print(f"âœ… è·å–åˆ° {len(unprocessed_clusters)} ä¸ªæœªå¤„ç†èšç±»")

    if unprocessed_clusters:
        print("ğŸ“‹ èšç±»è¯¦æƒ…:")
        for i, cluster in enumerate(unprocessed_clusters, 1):
            print(f"  {i}. {cluster['cluster_name']} ({cluster['source_type']})")
            print(f"     æ‘˜è¦: {cluster['centroid_summary'][:80]}...")
            print(f"     å¤§å°: {cluster['cluster_size']} ä¸ªäº‹ä»¶")

    # æµ‹è¯•èšç±»å‡†å¤‡
    if unprocessed_clusters:
        prepared_cluster = aligner.prepare_cluster_for_alignment(unprocessed_clusters[0])
        print(f"\nâœ… èšç±»å‡†å¤‡æµ‹è¯•æˆåŠŸ")
        print(f"   æºç±»å‹: {prepared_cluster['source_type']}")
        print(f"   æ‘˜è¦: {prepared_cluster['cluster_summary'][:60]}...")
        print(f"   å…¸å‹è§£å†³æ–¹æ¡ˆ: {prepared_cluster['typical_workaround'][:60]}...")
        print(f"   ä¸Šä¸‹æ–‡: {prepared_cluster['context']}")

    # æµ‹è¯•è·¨æºå¯¹é½ï¼ˆä¸å®é™…è°ƒç”¨LLMï¼‰
    if len(unprocessed_clusters) >= 2:
        print(f"\nğŸ” æµ‹è¯•è·¨æºå¯¹é½é€»è¾‘...")

        # æŒ‰æºç±»å‹åˆ†ç»„
        source_groups = {}
        for cluster in unprocessed_clusters:
            source_type = cluster['source_type']
            if source_type not in source_groups:
                source_groups[source_type] = []
            prepared_cluster = aligner.prepare_cluster_for_alignment(cluster)
            if prepared_cluster:
                source_groups[source_type].append(prepared_cluster)

        print(f"âœ… æºç±»å‹åˆ†ç»„:")
        for source_type, clusters in source_groups.items():
            print(f"   {source_type}: {len(clusters)} ä¸ªèšç±»")

        # æ„å»ºå¯¹é½prompt
        if len(source_groups) >= 2:
            prompt = aligner._build_alignment_prompt(source_groups)
            print(f"âœ… å¯¹é½Promptæ„å»ºæˆåŠŸ ({len(prompt)} å­—ç¬¦)")
            print(f"   Prompté¢„è§ˆ: {prompt[:200]}...")

    print("\nğŸ¯ æµ‹è¯•æ•°æ®åº“æ“ä½œ...")

    # æµ‹è¯•æ’å…¥å¯¹é½é—®é¢˜
    test_alignment = {
        'id': f'test_alignment_{int(time.time())}',
        'aligned_problem_id': 'AP_TEST_01',
        'sources': ['reddit', 'hn_ask'],
        'core_problem': 'Complex deployment pipeline management challenges',
        'why_they_look_different': 'Reddit focuses on emotional frustration while HN asks for technical solutions',
        'evidence': [
            {
                'source': 'reddit',
                'cluster_summary': 'Developers struggling with complex deployment pipelines',
                'evidence_quote': 'Manual deployment steps, configuration drift'
            },
            {
                'source': 'hn_ask',
                'cluster_summary': 'Best practices for deployment automation',
                'evidence_quote': 'Current process is error-prone'
            }
        ],
        'cluster_ids': ['reddit_deployment_pain', 'hn_deployment_challenges']
    }

    db.insert_aligned_problem(test_alignment)
    print("âœ… å¯¹é½é—®é¢˜æ’å…¥æˆåŠŸ")

    # æµ‹è¯•è·å–å¯¹é½é—®é¢˜
    aligned_problems = db.get_aligned_problems()
    print(f"âœ… è·å–åˆ° {len(aligned_problems)} ä¸ªå¯¹é½é—®é¢˜")

    if aligned_problems:
        problem = aligned_problems[0]
        print(f"   é—®é¢˜ID: {problem['aligned_problem_id']}")
        print(f"   æ¶‰åŠæº: {problem['sources']}")
        print(f"   æ ¸å¿ƒé—®é¢˜: {problem['core_problem']}")
        print(f"   è¯æ®æ•°é‡: {len(problem['evidence'])}")

    # æµ‹è¯•æ›´æ–°èšç±»çŠ¶æ€
    db.update_cluster_alignment_status('reddit_deployment_pain', 'aligned', 'AP_TEST_01')
    db.update_cluster_alignment_status('hn_deployment_challenges', 'aligned', 'AP_TEST_01')
    db.update_cluster_alignment_status('reddit_api_documentation', 'processed', None)
    print("âœ… èšç±»çŠ¶æ€æ›´æ–°æˆåŠŸ")

    # æµ‹è¯•è·å–ç”¨äºæœºä¼šæ˜ å°„çš„èšç±»
    opportunity_clusters = db.get_clusters_for_opportunity_mapping()
    print(f"âœ… è·å–åˆ° {len(opportunity_clusters)} ä¸ªç”¨äºæœºä¼šæ˜ å°„çš„èšç±»")

    aligned_clusters = [c for c in opportunity_clusters if c['source_type'] == 'aligned']
    regular_clusters = [c for c in opportunity_clusters if c['source_type'] != 'aligned']
    print(f"   åŒ…å« {len(aligned_clusters)} ä¸ªå¯¹é½èšç±»å’Œ {len(regular_clusters)} ä¸ªå¸¸è§„èšç±»")

    print("\nğŸ‰ è·¨æºå¯¹é½åŠŸèƒ½æµ‹è¯•å®Œæˆï¼")
    print("\nğŸ“Š æµ‹è¯•æ€»ç»“:")
    print("   âœ… æ•°æ®åº“æ¶æ„ - é€šè¿‡")
    print("   âœ… å¯¹é½æ¨¡å— - é€šè¿‡")
    print("   âœ… æ•°æ®åº“æ“ä½œ - é€šè¿‡")
    print("   âœ… èšç±»å¤„ç† - é€šè¿‡")
    print("   âœ… æºç±»å‹åˆ†ç»„ - é€šè¿‡")
    print("   âœ… Promptæ„å»º - é€šè¿‡")
    print("   âœ… å¯¹é½é—®é¢˜ç®¡ç† - é€šè¿‡")
    print("   âœ… èšç±»çŠ¶æ€è·Ÿè¸ª - é€šè¿‡")
    print("   âœ… æœºä¼šæ˜ å°„æ”¯æŒ - é€šè¿‡")

    return True

if __name__ == "__main__":
    try:
        success = test_alignment_workflow()
        if success:
            print("\nâœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼è·¨æºå¯¹é½åŠŸèƒ½å·²å°±ç»ªã€‚")
        else:
            print("\nâŒ æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥å®ç°ã€‚")
            sys.exit(1)
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)