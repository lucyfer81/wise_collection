# Decision Shortlist Layer è®¾è®¡æ–¹æ¡ˆ

## ğŸ¯ ç›®æ ‡ï¼ˆä¸€å¥è¯ï¼‰

æ¯æ¬¡ pipeline è¿è¡Œåï¼Œç³»ç»Ÿä¸æ˜¯ç»™ä½ ä¸€å † pain / clusters / scoresï¼Œè€Œæ˜¯åªè¾“å‡ºä¸€ä¸ªï¼š

> **Top 3â€“5 å¯æ‰§è¡Œäº§å“æœºä¼šæ¸…å•**

ä¸”æ¯ä¸€æ¡ï¼š

- ä¸€è¡Œ Problem
- ä¸€è¡Œ MVP
- ä¸€å¥ Why Now

èƒ½åœ¨ **10 åˆ†é’Ÿå†…å†³å®šï¼šåš or ä¸åšã€‚**

---

## ğŸ“‹ è®¾è®¡æ¦‚è¿°

æœ¬æ–‡æ¡£è¯¦ç»†æè¿°äº† Decision Shortlist Layer çš„è®¾è®¡æ–¹æ¡ˆï¼Œè¯¥æ¨¡å—ä½œä¸º pipeline çš„ç¬¬ 9 é˜¶æ®µï¼Œè´Ÿè´£ä»æ‰€æœ‰è¯„åˆ†æœºä¼šä¸­ç­›é€‰å‡º Top 3-5 ä¸ªæœ€å€¼å¾—æ‰§è¡Œçš„äº§å“æœºä¼šï¼Œå¹¶ä¸ºæ¯ä¸ªæœºä¼šç”Ÿæˆç®€æ´çš„å†³ç­–ä¿¡æ¯ã€‚

---

## ç¬¬ä¸€éƒ¨åˆ†ï¼šæ¶æ„è®¾è®¡

Decision Shortlist Layer å°†ä½œä¸º pipeline çš„ç¬¬ 9 ä¸ªé˜¶æ®µï¼ˆstage 9ï¼‰ï¼Œåœ¨æ‰€æœ‰æ•°æ®å¤„ç†å®Œæˆåæ‰§è¡Œã€‚æ ¸å¿ƒç»„ä»¶ï¼š

### 1. æ ¸å¿ƒç±»ï¼šDecisionShortlistGenerator

ä½ç½®ï¼š`pipeline/decision_shortlist.py`

ä¸»è¦æ–¹æ³•ï¼š
- `generate_shortlist()`: ä¸»æ–¹æ³•ï¼Œè¿”å› Top 3-5 ä¸ªæœºä¼š
- `_apply_hard_filters()`: ç¡¬æ€§è¿‡æ»¤ï¼ˆviability_score >= 7.0, cluster_size >= 6, trust_level >= 0.7ï¼‰
- `_check_cross_source_validation()`: ä¸‰å±‚ä¼˜å…ˆçº§çš„è·¨æºéªŒè¯
- `_calculate_final_score()`: å¯¹æ•°ç¼©æ”¾åŠ æƒè®¡ç®—ï¼ˆlogarithmic scalingï¼‰
- `_apply_diversity_boost()`: å¤šæ ·æ€§æƒ©ç½šæœºåˆ¶ï¼ˆé¿å…åŒè´¨åŒ–ï¼‰
- `_select_top_candidates()`: åŠ¨æ€é€‰æ‹© Top 3-5 ä¸ªï¼ˆè€ƒè™‘å¤šæ ·æ€§ï¼‰
- `_generate_readable_content()`: è°ƒç”¨ LLM ç”Ÿæˆ Problem/MVP/Why Now
- `_export_markdown()`: ç”Ÿæˆäººç±»å¯è¯»æŠ¥å‘Šï¼ˆå«ç©ºåˆ—è¡¨å¤„ç†ï¼‰
- `_export_json()`: ç”Ÿæˆæœºå™¨å¯ç”¨ JSON

### 2. æ•°æ®æµ

```
è¾“å…¥é˜¶æ®µï¼š
  â”œâ”€ opportunities è¡¨ï¼ˆå·²è¯„åˆ†çš„æœºä¼šï¼‰
  â”œâ”€ clusters è¡¨ï¼ˆèšç±»ä¿¡æ¯ï¼‰
  â””â”€ aligned_problems è¡¨ï¼ˆè·¨æºå¯¹é½ä¿¡æ¯ï¼‰

å¤„ç†é˜¶æ®µï¼š
  â”œâ”€ ç¡¬æ€§è¿‡æ»¤ï¼ˆviability_score >= 7.0, cluster_size >= 6, trust_level >= 0.7ï¼‰
  â”œâ”€ è·¨æºéªŒè¯ï¼ˆä¸‰å±‚ä¼˜å…ˆçº§ï¼‰
  â”œâ”€ æœ€ç»ˆè¯„åˆ†ï¼ˆåŠ æƒè®¡ç®—ï¼‰
  â”œâ”€ æ’åºï¼ˆæŒ‰ final_score é™åºï¼‰
  â””â”€ LLM ç”Ÿæˆï¼ˆProblem / MVP / Why Nowï¼‰

è¾“å‡ºé˜¶æ®µï¼š
  â”œâ”€ Markdown æŠ¥å‘Šï¼ˆreports/shortlist_report_YYYYMMDD.mdï¼‰
  â””â”€ JSON æ–‡ä»¶ï¼ˆdata/decision_shortlist.jsonï¼‰
```

### 3. é…ç½®é›†æˆ

åœ¨ `config/thresholds.yaml` ä¸­æ·»åŠ  `decision_shortlist` éƒ¨åˆ†ï¼ŒåŒ…å«ï¼š
- é˜ˆå€¼é…ç½®ï¼ˆmin_viability_score, min_cluster_size, min_trust_levelï¼‰
- è·¨æºéªŒè¯åŠ åˆ†ï¼ˆlevel_1/2/3 boostï¼‰
- æœ€ç»ˆè¯„åˆ†æƒé‡
- LLM prompt æ¨¡æ¿

---

## ç¬¬äºŒéƒ¨åˆ†ï¼šç¡¬æ€§è¿‡æ»¤è§„åˆ™

### 1. è¿‡æ»¤é˜ˆå€¼

ä» `config/thresholds.yaml` è¯»å–ï¼š
```yaml
decision_shortlist:
  min_viability_score: 7.0
  min_cluster_size: 6
  min_trust_level: 0.7
  ignored_clusters: []  # å¯é€‰ï¼šè¦å¿½ç•¥çš„ cluster åç§°åˆ—è¡¨
```

### 2. SQL æŸ¥è¯¢é€»è¾‘

```sql
SELECT
    o.*,
    c.cluster_name,
    c.cluster_size,
    c.source_type,
    c.pain_event_ids
FROM opportunities o
JOIN clusters c ON o.cluster_id = c.id
WHERE o.total_score >= {min_viability_score}
  AND c.cluster_size >= {min_cluster_size}
  AND o.trust_level >= {min_trust_level}
  AND c.cluster_name NOT IN ({ignored_clusters})
ORDER BY o.total_score DESC
```

### 3. è¿‡æ»¤ç»Ÿè®¡

è®°å½•æ¯ä¸ªè¿‡æ»¤æ¡ä»¶çš„è¿‡æ»¤æ•°é‡ï¼Œç”¨äºæ—¥å¿—è¾“å‡ºï¼š
- å›  viability_score ä¸è¶³è¢«è¿‡æ»¤ï¼šX ä¸ª
- å›  cluster_size ä¸è¶³è¢«è¿‡æ»¤ï¼šY ä¸ª
- å›  trust_level ä¸è¶³è¢«è¿‡æ»¤ï¼šZ ä¸ª
- é€šè¿‡æ‰€æœ‰è¿‡æ»¤ï¼šN ä¸ª

å¦‚æœæ²¡æœ‰é€šè¿‡ç¡¬æ€§è¿‡æ»¤çš„æœºä¼šï¼Œè¿”å›ç©ºåˆ—è¡¨å¹¶è®°å½•è­¦å‘Šæ—¥å¿—ã€‚

---

## ç¬¬ä¸‰éƒ¨åˆ†ï¼šè·¨æºéªŒè¯é€»è¾‘

### 1. ä¸‰å±‚ä¼˜å…ˆçº§éªŒè¯

| çº§åˆ« | æ¡ä»¶ | åŠ åˆ† | validated_problem |
|------|------|------|-------------------|
| **Level 1 - å¼ºä¿¡å·** | cluster åœ¨ `aligned_problems` è¡¨ä¸­<br>æˆ– `source_type == 'aligned'` | +2.0 | True |
| **Level 2 - ä¸­ç­‰ä¿¡å·** | `cluster_size >= 10` AND è·¨ >= 3 ä¸ªä¸åŒ subreddit | +1.0 | True |
| **Level 3 - å¼±ä¿¡å·** | `cluster_size >= 8` AND è·¨ >= 2 ä¸ªä¸åŒ subreddit | +0.5 | False |

### 2. å®ç°é€»è¾‘

```python
def _check_cross_source_validation(self, opportunity: Dict, cluster: Dict) -> Dict:
    """æ£€æŸ¥è·¨æºéªŒè¯ï¼Œè¿”å›éªŒè¯ä¿¡æ¯å’ŒåŠ åˆ†"""

    # Level 1: æ£€æŸ¥ aligned_problems è¡¨æˆ– source_type
    if cluster['source_type'] == 'aligned':
        return {
            "has_cross_source": True,
            "validation_level": 1,
            "boost_score": 2.0,
            "validated_problem": True,
            "evidence": "Aligned from cross-source analysis"
        }

    # æ£€æŸ¥ aligned_problems è¡¨
    aligned_problem = self._check_aligned_problems_table(cluster['cluster_name'])
    if aligned_problem:
        return {
            "has_cross_source": True,
            "validation_level": 1,
            "boost_score": 2.0,
            "validated_problem": True,
            "evidence": f"Found in aligned_problems: {aligned_problem['aligned_problem_id']}"
        }

    # Level 2: æ£€æŸ¥ cluster_size + è·¨ subreddit
    pain_event_ids = json.loads(cluster['pain_event_ids'])
    subreddit_count = self._count_subreddits(pain_event_ids)

    if cluster['cluster_size'] >= 10 and subreddit_count >= 3:
        return {
            "has_cross_source": True,
            "validation_level": 2,
            "boost_score": 1.0,
            "validated_problem": True,
            "evidence": f"Large cluster ({cluster['cluster_size']}) across {subreddit_count} subreddits"
        }

    # Level 3: å¼±ä¿¡å·
    if cluster['cluster_size'] >= 8 and subreddit_count >= 2:
        return {
            "has_cross_source": True,
            "validation_level": 3,
            "boost_score": 0.5,
            "validated_problem": False,
            "evidence": f"Medium cluster ({cluster['cluster_size']}) across {subreddit_count} subreddits"
        }

    # æ— è·¨æºéªŒè¯
    return {
        "has_cross_source": False,
        "validation_level": 0,
        "boost_score": 0.0,
        "validated_problem": False,
        "evidence": "No cross-source validation"
    }
```

### 3. è¾…åŠ©æ–¹æ³•

```python
def _check_aligned_problems_table(self, cluster_name: str) -> Optional[Dict]:
    """æ£€æŸ¥ cluster æ˜¯å¦åœ¨ aligned_problems è¡¨ä¸­"""
    with db.get_connection("clusters") as conn:
        cursor = conn.execute("""
            SELECT aligned_problem_id, sources, alignment_score
            FROM aligned_problems
            WHERE cluster_ids LIKE ?
        """, (f'%{cluster_name}%',))
        result = cursor.fetchone()
        return dict(result) if result else None

def _count_subreddits(self, pain_event_ids: List[int]) -> int:
    """è®¡ç®—æ¶‰åŠçš„ä¸åŒ subreddit æ•°é‡"""
    with db.get_connection("pain") as conn:
        placeholders = ','.join('?' for _ in pain_event_ids)
        cursor = conn.execute(f"""
            SELECT COUNT(DISTINCT fp.subreddit) as count
            FROM pain_events pe
            JOIN filtered_posts fp ON pe.post_id = fp.id
            WHERE pe.id IN ({placeholders})
        """, pain_event_ids)
        return cursor.fetchone()['count']
```

---

## ç¬¬å››éƒ¨åˆ†ï¼šæœ€ç»ˆè¯„åˆ†ä¸æ’åº

### 1. å¯¹æ•°ç¼©æ”¾è¯„åˆ†å…¬å¼

**è®¾è®¡ç†å¿µ**ï¼šä½¿ç”¨å¯¹æ•°å‡½æ•°å¤„ç† cluster_sizeï¼Œé¿å…å¤§è§„æ¨¡èšç±»è¿‡åº¦å½±å“è¯„åˆ†ï¼ŒåŒæ—¶ä¿ç•™è§„æ¨¡ä½œä¸ºæœ‰ä»·å€¼çš„ä¿¡å·ã€‚

```python
import math

def _calculate_final_score(self, opportunity: Dict, cross_source_info: Dict) -> float:
    """è®¡ç®—æœ€ç»ˆå¾—åˆ†ï¼ˆå¯¹æ•°ç¼©æ”¾ + é…ç½®åŒ–æƒé‡ï¼‰"""

    # ä»é…ç½®è¯»å–æƒé‡
    weights = self.config['decision_shortlist']['final_score_weights']

    # åŸºç¡€è¯„åˆ†
    viability_score = opportunity['total_score']  # å·²ç»ç”± ViabilityScorer è®¡ç®— (0-10)
    trust_level = opportunity.get('trust_level', 0.5)  # (0-1)

    # å¯¹æ•°ç¼©æ”¾ï¼šlog10(cluster_size)
    # ä¸¾ä¾‹ï¼š
    #   cluster_size = 10  â†’ log10(10) = 1.0
    #   cluster_size = 100 â†’ log10(100) = 2.0
    #   cluster_size = 200 â†’ log10(200) = 2.3
    # è¿™æ · 100 å’Œ 200 çš„å·®è·ç¼©å°ä¸º 0.3ï¼Œè€Œä¸æ˜¯çº¿æ€§ç¼©æ”¾çš„ 10 å€å·®è·
    cluster_size = opportunity['cluster_size']
    cluster_size_log = math.log10(max(cluster_size, 1))  # é¿å…log(0)

    # è·¨æºåŠ åˆ†
    cross_source_bonus = cross_source_info['boost_score']  # 0-2.0

    # åŠ æƒè®¡ç®—ï¼ˆæ‰€æœ‰æƒé‡éƒ½åœ¨é…ç½®æ–‡ä»¶ä¸­ï¼‰
    final_score = (
        viability_score * weights['viability_score'] +
        cluster_size_log * weights['cluster_size_log_factor'] +
        trust_level * weights['trust_level']
    )

    # è·¨æºéªŒè¯åŠ åˆ†ï¼ˆå¯é€‰ï¼Œå¦‚æœä¸ä½¿ç”¨å¯¹æ•°ç¼©æ”¾æ¨¡å‹ï¼‰
    if cross_source_info['has_cross_source']:
        final_score += weights['cross_source_bonus'] * cross_source_bonus

    # å½’ä¸€åŒ–åˆ° 0-10 èŒƒå›´ï¼ˆå¯é€‰ï¼Œå–å†³äºæƒé‡é…ç½®ï¼‰
    # å¦‚æœæƒé‡æ€»å’Œå¤§äº 1ï¼Œå¯èƒ½éœ€è¦å½’ä¸€åŒ–
    return min(final_score, 10.0)
```

**ä¼˜åŠ¿**ï¼š
- å¯¹æ•°ç¼©æ”¾é¿å…æç«¯å€¼å½±å“ï¼šå¤§å°ä¸º 200 çš„èšç±»ä¸ä¼šæ¯”å¤§å°ä¸º 20 çš„èšç±»è·å¾—ä¸æˆæ¯”ä¾‹çš„é«˜åˆ†
- æ‰€æœ‰æƒé‡å¯é…ç½®ï¼šä»"ç»éªŒé©±åŠ¨"å‡çº§ä¸º"æ¨¡å‹é©±åŠ¨"ï¼Œå¯ä»¥ç³»ç»Ÿæ€§åœ°è°ƒä¼˜
- ä¿ç•™è§„æ¨¡ä¿¡å·ï¼šå¤§èšç±»ä»ç„¶è·å¾—æ›´é«˜åˆ†æ•°ï¼Œä½†ä¸ä¼šä¸»å¯¼å†³ç­–

### 2. é…ç½®åŒ–æƒé‡ç³»ç»Ÿ

åœ¨ `config/thresholds.yaml` ä¸­ï¼š

```yaml
decision_shortlist:
  # æœ€ç»ˆè¯„åˆ†æƒé‡ï¼ˆæ‰€æœ‰æƒé‡ç³»æ•°å‡å¯è°ƒæ•´ï¼‰
  final_score_weights:
    viability_score: 1.0           # å¯è¡Œæ€§è¯„åˆ†æƒé‡ï¼ˆ0-10 åˆ†ï¼Œä¹˜ä»¥ 1.0ï¼‰
    cluster_size_log_factor: 2.5   # log10(cluster_size) çš„æƒé‡ç³»æ•°
    trust_level: 1.5               # ä¿¡ä»»åº¦æƒé‡ï¼ˆ0-1ï¼Œä¹˜ä»¥ 1.5ï¼‰
    cross_source_bonus: 5.0        # è·¨æºéªŒè¯åŸºç¡€åŠ åˆ†

  # ç¤ºä¾‹è®¡ç®—ï¼š
  # å‡è®¾æŸä¸ªæœºä¼šï¼š
  #   viability_score = 8.0
  #   cluster_size = 50
  #   trust_level = 0.8
  #   cross_source_level = 1 (boost = 2.0)
  #
  # è®¡ç®—ï¼š
  #   final_score = 8.0 * 1.0 + log10(50) * 2.5 + 0.8 * 1.5 + 2.0 * 5.0 * 0.2
  #              = 8.0 + 1.7 * 2.5 + 1.2 + 2.0
  #              = 8.0 + 4.25 + 1.2 + 2.0
  #              = 15.45 â†’ å½’ä¸€åŒ–åˆ° 10 åˆ†èŒƒå›´ = 10.0ï¼ˆæˆ–ç›´æ¥é™åˆ¶ä¸Šé™ï¼‰
```

**æƒé‡è°ƒæ•´æŒ‡å—**ï¼š
- `viability_score`: æé«˜æ­¤æƒé‡ â†’ æ›´é‡è§† LLM è¯„ä¼°çš„å¯è¡Œæ€§
- `cluster_size_log_factor`: æé«˜æ­¤æƒé‡ â†’ æ›´é‡è§†æ•°æ®è§„æ¨¡ï¼ˆä½†å¯¹æ•°ç¼©æ”¾ä¼šå‡å¼±æç«¯å½±å“ï¼‰
- `trust_level`: æé«˜æ­¤æƒé‡ â†’ æ›´é‡è§†æ•°æ®æºè´¨é‡
- `cross_source_bonus`: æé«˜æ­¤æƒé‡ â†’ æ›´é‡è§†è·¨æºéªŒè¯

### 3. åŸºç¡€æ’åºé€»è¾‘

æŒ‰ `final_score` é™åºæ’åˆ—ï¼ˆä¸è€ƒè™‘å¤šæ ·æ€§ï¼‰ï¼š

```python
def _sort_by_score(self, scored_opportunities: List[Dict]) -> List[Dict]:
    """æŒ‰ final_score é™åºæ’åº"""
    return sorted(scored_opportunities, key=lambda x: x['final_score'], reverse=True)
```

### 4. è¯„åˆ†è¯¦æƒ…

æ¯ä¸ªæœºä¼šçš„è¯„åˆ†è¯¦æƒ…ï¼š
- `viability_score`: åŸå§‹å¯è¡Œæ€§è¯„åˆ†ï¼ˆ0-10ï¼‰
- `cluster_size_log`: log10(èšç±»è§„æ¨¡)ï¼Œç”¨äºå¹³æ»‘è§„æ¨¡å·®å¼‚
- `trust_level`: ä¿¡ä»»åº¦è¯„åˆ†ï¼ˆ0-1ï¼‰
- `cross_source_bonus`: è·¨æºéªŒè¯åŠ åˆ†ï¼ˆ0-2.0ï¼Œç”±éªŒè¯ç­‰çº§å†³å®šï¼‰
- `final_score`: æœ€ç»ˆåŠ æƒè¯„åˆ†ï¼ˆ0-10ï¼Œåº”ç”¨å¯¹æ•°ç¼©æ”¾å’Œé…ç½®æƒé‡ï¼‰

**ç¤ºä¾‹å¯¹æ¯”ï¼ˆçº¿æ€§ vs å¯¹æ•°ï¼‰**ï¼š

| cluster_size | çº¿æ€§ç¼©æ”¾ (/10) | å¯¹æ•°ç¼©æ”¾ (log10) | å·®å¼‚ |
|--------------|----------------|------------------|------|
| 10 | 1.0 | 1.0 | æ—  |
| 50 | 5.0 | 1.7 | å¯¹æ•°é™ä½ 66% |
| 100 | 10.0 | 2.0 | å¯¹æ•°é™ä½ 80% |
| 200 | 20.0ï¼ˆä¸Šé™ 10ï¼‰ | 2.3 | å¯¹æ•°é™ä½ 88.5% |

---

## ç¬¬å››éƒ¨åˆ†ï¼ˆç»­ï¼‰ï¼šå¤šæ ·æ€§ä¿è¯æœºåˆ¶ï¼ˆå¯é€‰é«˜çº§åŠŸèƒ½ï¼‰

### 1. é—®é¢˜èƒŒæ™¯

å¦‚æœè¯„åˆ†æœ€é«˜çš„ Top 5 ä¸ªæœºä¼šéƒ½æ¥è‡ªåŒä¸€ä¸ªé¢†åŸŸï¼ˆä¾‹å¦‚éƒ½å’Œ"Notion åŒæ­¥"ç›¸å…³ï¼‰ï¼Œshortlist ä¼šç¼ºä¹å¤šæ ·æ€§ï¼Œå¯èƒ½é”™è¿‡å…¶ä»–é¢†åŸŸçš„æœ‰è¶£æœºä¼šã€‚

### 2. å¤šæ ·æ€§æƒ©ç½šç­–ç•¥

**æ ¸å¿ƒæ€æƒ³**ï¼šåœ¨é€‰æ‹©åç»­æœºä¼šæ—¶ï¼Œå¯¹ä¸å·²é€‰æœºä¼šç›¸ä¼¼æˆ–åŒé¢†åŸŸçš„å€™é€‰æœºä¼šæ–½åŠ è½»å¾®çš„åˆ†æ•°æƒ©ç½šï¼ˆå¦‚ä¹˜ä»¥ 0.9ï¼‰ã€‚

```python
def _apply_diversity_penalty(
    self,
    candidate: Dict,
    selected_candidates: List[Dict]
) -> float:
    """è®¡ç®—å¤šæ ·æ€§æƒ©ç½šç³»æ•°"""

    penalty_factor = 1.0  # é»˜è®¤æ— æƒ©ç½š

    for selected in selected_candidates:
        # 1. æ£€æŸ¥æ˜¯å¦å±äºåŒä¸€ clusterï¼ˆç›´æ¥å…³è”ï¼‰
        if candidate.get('cluster_id') == selected.get('cluster_id'):
            penalty_factor *= 0.7  # ä¸¥é‡æƒ©ç½šï¼šåŒä¸€ cluster
            continue

        # 2. æ£€æŸ¥ pain_type ç›¸ä¼¼åº¦ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        candidate_pain_type = candidate.get('primary_pain_type', '')
        selected_pain_type = selected.get('primary_pain_type', '')

        if candidate_pain_type and candidate_pain_type == selected_pain_type:
            penalty_factor *= 0.85  # ä¸­ç­‰æƒ©ç½šï¼šåŒä¸€ pain_type

        # 3. æ£€æŸ¥å…³é”®è¯é‡å ï¼ˆå¯é€‰ï¼‰
        if self._check_keyword_overlap(candidate, selected):
            penalty_factor *= 0.90  # è½»å¾®æƒ©ç½šï¼šå…³é”®è¯é‡å 

    return penalty_factor


def _select_top_candidates_with_diversity(
    self,
    scored_opportunities: List[Dict]
) -> List[Dict]:
    """é€‰æ‹© Top 3-5 ä¸ªå€™é€‰æœºä¼šï¼ˆè€ƒè™‘å¤šæ ·æ€§ï¼‰"""

    # æŒ‰åˆ†æ•°é™åºæ’åº
    sorted_opportunities = sorted(
        scored_opportunities,
        key=lambda x: x['final_score'],
        reverse=True
    )

    selected = []
    remaining = sorted_opportunities.copy()

    max_candidates = self.config['decision_shortlist']['output']['max_candidates']
    min_candidates = self.config['decision_shortlist']['output']['min_candidates']

    for i in range(max_candidates):
        if not remaining:
            break

        # åº”ç”¨å¤šæ ·æ€§æƒ©ç½š
        for candidate in remaining:
            penalty = self._apply_diversity_penalty(candidate, selected)
            candidate['diversity_adjusted_score'] = candidate['final_score'] * penalty

        # é€‰æ‹©è°ƒæ•´ååˆ†æ•°æœ€é«˜çš„
        best_candidate = max(
            remaining,
            key=lambda x: x['diversity_adjusted_score']
        )

        selected.append(best_candidate)
        remaining.remove(best_candidate)

        # å¦‚æœå·²é€‰å¤Ÿ min_candidates ä¸ªï¼Œæ£€æŸ¥æ˜¯å¦éœ€è¦ç»§ç»­
        if len(selected) >= min_candidates:
            # æ£€æŸ¥ä¸‹ä¸€ä¸ªå€™é€‰çš„è°ƒæ•´ååˆ†æ•°æ˜¯å¦å¤ªä½
            if remaining:
                next_best = max(
                    remaining,
                    key=lambda x: x['diversity_adjusted_score']
                )
                # å¦‚æœå·®è·è¿‡å¤§ï¼Œåœæ­¢é€‰æ‹©
                if (best_candidate['diversity_adjusted_score'] -
                    next_best['diversity_adjusted_score']) > 2.0:
                    break

    return selected[:min(len(selected), max_candidates)]
```

### 3. è¾…åŠ©æ–¹æ³•

```python
def _check_keyword_overlap(self, candidate1: Dict, candidate2: Dict) -> bool:
    """æ£€æŸ¥ä¸¤ä¸ªæœºä¼šçš„å…³é”®è¯é‡å åº¦"""

    # æå–å…³é”®è¯ï¼ˆä» opportunity_name å’Œ descriptionï¼‰
    text1 = f"{candidate1.get('opportunity_name', '')} {candidate1.get('description', '')}"
    text2 = f"{candidate2.get('opportunity_name', '')} {candidate2.get('description', '')}"

    # ç®€å•çš„å…³é”®è¯æå–ï¼ˆå¯ä»¥ç”¨æ›´å¤æ‚çš„ NLP æ–¹æ³•ï¼‰
    keywords1 = set(text1.lower().split())
    keywords2 = set(text2.lower().split())

    # è®¡ç®—äº¤é›†
    overlap = keywords1 & keywords2

    # å¦‚æœé‡å å…³é”®è¯è¶…è¿‡ 3 ä¸ªï¼Œè®¤ä¸ºç›¸ä¼¼
    return len(overlap) >= 3
```

### 4. é…ç½®é€‰é¡¹

```yaml
decision_shortlist:
  diversity:
    enabled: true  # æ˜¯å¦å¯ç”¨å¤šæ ·æ€§æœºåˆ¶

    penalties:
      same_cluster: 0.7      # åŒä¸€ cluster çš„æƒ©ç½šç³»æ•°
      same_pain_type: 0.85   # åŒä¸€ pain_type çš„æƒ©ç½šç³»æ•°
      keyword_overlap: 0.90  # å…³é”®è¯é‡å çš„æƒ©ç½šç³»æ•°

    min_diversity_score_gap: 2.0  # æœ€ä½å¤šæ ·æ€§åˆ†æ•°å·®è·ï¼Œä½äºæ­¤å€¼åœæ­¢é€‰æ‹©
```

### 5. æ•ˆæœç¤ºä¾‹

**ä¸ä½¿ç”¨å¤šæ ·æ€§æœºåˆ¶**ï¼š
```
Top 5 æœºä¼šï¼š
1. Notion API åŒæ­¥å·¥å…· (score: 8.5)
2. Notion æ•°æ®åº“å¤‡ä»½å·¥å…· (score: 8.3)
3. Notion é¡µé¢æ¨¡æ¿ç”Ÿæˆå™¨ (score: 8.1)
4. Notion Webhook é›†æˆå·¥å…· (score: 7.9)
5. Notion æ‰¹é‡å¯¼å…¥å·¥å…· (score: 7.7)
â†’ å…¨éƒ¨éƒ½æ˜¯ Notion ç›¸å…³ï¼Œç¼ºä¹å¤šæ ·æ€§
```

**ä½¿ç”¨å¤šæ ·æ€§æœºåˆ¶**ï¼š
```
Top 5 æœºä¼šï¼š
1. Notion API åŒæ­¥å·¥å…· (score: 8.5, adjusted: 8.5)
2. API æ–‡æ¡£ç”Ÿæˆå·¥å…· (score: 8.2, adjusted: 8.2)
3. Slack æ¶ˆæ¯åˆ†æå·¥å…· (score: 7.8, adjusted: 7.8)
4. GitHub PR è‡ªåŠ¨å®¡æŸ¥å·¥å…· (score: 7.6, adjusted: 7.6)
5. Notion æ•°æ®åº“å¤‡ä»½å·¥å…· (score: 8.3, adjusted: 5.8, å› å¤šæ ·æ€§æƒ©ç½šè¢«é™çº§)
â†’ è¦†ç›–ä¸åŒé¢†åŸŸï¼Œæ›´ä¸°å¯Œ
```

---

## ç¬¬äº”éƒ¨åˆ†ï¼šLLM ç”Ÿæˆå¯è¯»å†…å®¹

### 1. Prompt æ¨¡æ¿

å­˜å‚¨åœ¨ `config/thresholds.yaml` çš„ `decision_shortlist.prompts.problem_mvp_whynow` ä¸­ï¼š

```yaml
decision_shortlist:
  prompts:
    problem_mvp_whynow: |
      You are a product expert specializing in identifying micro-SaaS opportunities for solo founders.

      Based on the following opportunity data, generate THREE concise, impactful sentences:

      **Opportunity Data:**
      - Name: {opportunity_name}
      - Description: {description}
      - Target Users: {target_users}
      - Missing Capability: {missing_capability}
      - Why Existing Tools Fail: {why_existing_fail}
      - Cluster Summary: {cluster_summary}
      - Pain Events: {cluster_size} unique pain points
      - Cross-Source Validation: {cross_source_info}

      **Output Requirements:**

      1. **Problem Statement** (one sentence, max 30 words)
         Format: "Users in [context/role] are struggling with [specific task] because [structural reason]."

      2. **MVP Cut** (one sentence, max 25 words)
         Format: "A minimal tool that helps them [do X faster/easier/safer] by replacing [current bad workaround]."

      3. **Why Now** (one sentence, max 20 words)
         Format: "This is urgent now because [specific signal/tool failure/multi-community validation]."

      **Constraints:**
      - Be specific and concrete (avoid generic fluff)
      - Focus on actionable insights (not vague observations)
      - Use present tense
      - Each sentence should stand alone (no dependencies)

      **Output Format:**
      Return ONLY a valid JSON object with these exact keys:
      {
        "problem": "one sentence problem statement",
        "mvp": "one sentence mvp description",
        "why_now": "one sentence urgency explanation"
      }

      No additional text, explanations, or markdown formatting.
```

### 2. è°ƒç”¨æ–¹å¼

```python
def _generate_readable_content(self, opportunity: Dict, cluster: Dict, cross_source_info: Dict) -> Dict[str, str]:
    """ä½¿ç”¨ LLMç”Ÿæˆ Problem/MVP/Why Now"""

    try:
        # å‡†å¤‡ prompt å‚æ•°
        prompt_params = {
            'opportunity_name': opportunity['opportunity_name'],
            'description': opportunity['description'],
            'target_users': opportunity.get('target_users', ''),
            'missing_capability': opportunity.get('missing_capability', ''),
            'why_existing_fail': opportunity.get('why_existing_fail', ''),
            'cluster_summary': cluster.get('centroid_summary', ''),
            'cluster_size': cluster['cluster_size'],
            'cross_source_info': cross_source_info['evidence']
        }

        # åŠ è½½ prompt æ¨¡æ¿
        prompt_template = self.config['decision_shortlist']['prompts']['problem_mvp_whynow']
        prompt = prompt_template.format(**prompt_params)

        # è°ƒç”¨ LLM
        response = self.llm_client.chat_completion(
            messages=[{"role": "user", "content": prompt}],
            model_type="main",
            temperature=0.3,
            max_tokens=500
        )

        # è§£æå“åº”
        content = response.get('content', response) if isinstance(response, dict) else response
        result = json.loads(content)

        return {
            'problem': result.get('problem', ''),
            'mvp': result.get('mvp', ''),
            'why_now': result.get('why_now', '')
        }

    except Exception as e:
        logger.error(f"LLM generation failed for {opportunity['opportunity_name']}: {e}")
        # é™çº§ï¼šä»ç°æœ‰å­—æ®µèšåˆæå–
        return self._fallback_readable_content(opportunity, cluster)
```

### 3. é™çº§ç­–ç•¥

```python
def _fallback_readable_content(self, opportunity: Dict, cluster: Dict) -> Dict[str, str]:
    """é™çº§ç­–ç•¥ï¼šä»ç°æœ‰å­—æ®µæå–"""

    # ä» opportunity.description æå–
    description = opportunity.get('description', '')
    target_users = opportunity.get('target_users', 'Users')
    missing_capability = opportunity.get('missing_capability', '')
    why_fail = opportunity.get('why_existing_fail', '')

    # Problem Statement
    problem = f"{target_users} are struggling with {description[:50]}... because {why_fail[:50]}..."

    # MVP Cut
    mvp = f"A minimal tool that addresses {missing_capability[:40]}... with a simple interface."

    # Why Now
    cluster_size = cluster.get('cluster_size', 0)
    why_now = f"Validated by {cluster_size} recent pain points from active communities."

    return {
        'problem': problem[:150],
        'mvp': mvp[:100],
        'why_now': why_now[:100]
    }
```

---

## ç¬¬å…­éƒ¨åˆ†ï¼šè¾“å‡ºæ ¼å¼

### 1. Markdown æŠ¥å‘Š

æ–‡ä»¶è·¯å¾„ï¼š`reports/shortlist_report_YYYYMMDD.md`

```markdown
# Decision Shortlist (2025-12-25)

**Generated**: 2025-12-25 18:30:00
**Pipeline Run**: pipeline_results_20251225_185806.json
**Total Opportunities Analyzed**: 50
**Filtered Candidates**: 5

---

## ğŸ¯ Candidate 1: AI-Powered API Documentation Generator

**Final Score**: 8.7/10
**Cross-Source Evidence**: âœ… YES (Level 1)
**Confidence**: HIGH

### Problem
Developers maintaining REST APIs are struggling to keep documentation in sync with code changes because manual updates are error-prone and time-consuming.

### MVP
A minimal CLI tool that auto-generates interactive API docs from OpenAPI specs by replacing manual Markdown maintenance with live preview.

### Why Now
This is urgent now because validated across Reddit r/programming and Hacker News with 15+ developers expressing frustration with Swagger UI.

---

**Supporting Data**:
- **Viability Score**: 8.2/10
- **Cluster Size**: 15 pain events
- **Trust Level**: 0.85 (high-quality sources)
- **Cross-Source**: Aligned from Reddit + HackerNews
- **Market Tier**: Medium (50K-100K addressable users)
- **Killer Risks**: Swagger UI is free and well-established

---

## ğŸ¯ Candidate 2: ...

(é‡å¤ä¸Šè¿°ç»“æ„)

---

## ğŸ“Š Summary Statistics

| Metric | Value |
|--------|-------|
| Total Opportunities Analyzed | 50 |
| Passed Hard Filters | 15 |
| Cross-Source Validated | 8 (Level 1: 3, Level 2: 3, Level 3: 2) |
| Final Selection | 5 |
| Avg Final Score | 7.8/10 |

### Distribution by Validation Level
- Level 1 (Strong): 3 candidates
- Level 2 (Medium): 1 candidate
- Level 3 (Weak): 1 candidate

---

## ğŸ“ Notes

- All candidates passed hard filters: viability >= 7.0, cluster_size >= 6, trust_level >= 0.7
- Cross-source validation adds +2.0 (Level 1), +1.0 (Level 2), or +0.5 (Level 3) to final score
- Review generated JSON for detailed scoring breakdown
```

### 2. JSON æ–‡ä»¶

æ–‡ä»¶è·¯å¾„ï¼š`data/decision_shortlist.json`

**æ­£å¸¸æƒ…å†µ**ï¼ˆæœ‰å€™é€‰æœºä¼šï¼‰ï¼š

```json
[
  {
    "id": "cluster_42",
    "opportunity_name": "AI-Powered API Documentation Generator",
    "problem": "Developers maintaining REST APIs are struggling to keep documentation in sync with code changes because manual updates are error-prone and time-consuming.",
    "mvp": "A minimal CLI tool that auto-generates interactive API docs from OpenAPI specs by replacing manual Markdown maintenance with live preview.",
    "why_now": "This is urgent now because validated across Reddit r/programming and Hacker News with 15+ developers expressing frustration with Swagger UI.",
    "final_score": 8.7,
    "viability_score": 8.2,
    "cluster_size_log": 1.18,
    "trust_level": 0.85,
    "cross_source_level": 1,
    "cross_source_bonus": 2.0,
    "validated_problem": true,
    "sources": ["reddit", "hackernews"],
    "cluster_size": 15,
    "subreddit_count": 4,
    "market_tier": "medium",
    "killer_risks": ["Swagger UI is free and well-established"],
    "recommendation": "pursue - Strong opportunity with high potential",
    "generated_at": "2025-12-25T18:30:00"
  }
]
```

**ç©ºåˆ—è¡¨æƒ…å†µ**ï¼ˆæ— å€™é€‰æœºä¼šï¼‰ï¼š

```json
{
  "empty": true,
  "message": "æœ¬æ¬¡è¿è¡Œæœªèƒ½å‘ç°æ»¡è¶³ Shortlist æ ‡å‡†çš„è¶³å¤Ÿå¼ºçš„æœºä¼šä¿¡å·ã€‚",
  "statistics": {
    "total_opportunities_analyzed": 50,
    "passed_viability_filter": 30,
    "passed_cluster_size_filter": 20,
    "passed_trust_level_filter": 15,
    "passed_all_filters": 0,
    "filter_reasons": {
      "viability_score_too_low": 20,
      "cluster_size_too_small": 10,
      "trust_level_too_low": 5
    },
    "highest_score": 6.8,
    "score_threshold": 7.0
  },
  "recommendations": [
    "è€ƒè™‘é™ä½è¿‡æ»¤é˜ˆå€¼ï¼ˆmin_viability_score, min_cluster_size, min_trust_levelï¼‰",
    "ç­‰å¾…æ›´å¤šæ•°æ®ç§¯ç´¯åé‡æ–°è¿è¡Œ",
    "æ£€æŸ¥æ•°æ®æºè´¨é‡å’Œè¦†ç›–èŒƒå›´"
  ],
  "generated_at": "2025-12-25T18:30:00"
}
```

### 3. ç©ºåˆ—è¡¨å¤„ç†

**å½“æ²¡æœ‰æœºä¼šæ»¡è¶³è¿‡æ»¤æ¡ä»¶æ—¶**ï¼Œç³»ç»Ÿåº”è¯¥æ˜ç¡®å¤„ç†è¿™ç§æƒ…å†µï¼š

```python
def _handle_empty_shortlist(self, filter_stats: Dict) -> Dict[str, Any]:
    """å¤„ç†ç©ºåˆ—è¡¨æƒ…å†µ"""

    # è·å–æœ€é«˜åˆ†ï¼ˆæœªé€šè¿‡è¿‡æ»¤çš„æœºä¼šï¼‰
    highest_score = self._get_highest_score_among_filtered()

    result = {
        "empty": True,
        "message": "æœ¬æ¬¡è¿è¡Œæœªèƒ½å‘ç°æ»¡è¶³ Shortlist æ ‡å‡†çš„è¶³å¤Ÿå¼ºçš„æœºä¼šä¿¡å·ã€‚",
        "statistics": {
            "total_opportunities_analyzed": filter_stats['total'],
            "passed_viability_filter": filter_stats['passed_viability'],
            "passed_cluster_size_filter": filter_stats['passed_size'],
            "passed_trust_level_filter": filter_stats['passed_trust'],
            "passed_all_filters": 0,
            "filter_reasons": filter_stats['reasons'],
            "highest_score": highest_score,
            "score_threshold": self.config['decision_shortlist']['min_viability_score']
        },
        "recommendations": self._generate_recommendations_for_empty_list(filter_stats),
        "generated_at": datetime.now().isoformat()
    }

    return result


def _export_empty_markdown_report(self, result: Dict) -> str:
    """ç”Ÿæˆç©ºåˆ—è¡¨çš„ Markdown æŠ¥å‘Š"""

    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    stats = result['statistics']

    content = f"""# Decision Shortlist ({datetime.now().strftime('%Y-%m-%d')})

âš ï¸ **æ³¨æ„ï¼šæœ¬æ¬¡è¿è¡Œæœªå‘ç°æ»¡è¶³æ ‡å‡†çš„å€™é€‰æœºä¼š**

**Generated**: {timestamp}
**Pipeline Run**: {self.pipeline_run_id}

---

## ğŸ“Š è¿‡æ»¤ç»Ÿè®¡

| æŒ‡æ ‡ | æ•°é‡ |
|------|------|
| æ€»æœºä¼šæ•° | {stats['total_opportunities_analyzed']} |
| é€šè¿‡å¯è¡Œæ€§è¯„åˆ†è¿‡æ»¤ (>= {stats['score_threshold']}) | {stats['passed_viability_filter']} |
| é€šè¿‡èšç±»è§„æ¨¡è¿‡æ»¤ (>= {self.config['decision_shortlist']['min_cluster_size']}) | {stats['passed_cluster_size_filter']} |
| é€šè¿‡ä¿¡ä»»åº¦è¿‡æ»¤ (>= {self.config['decision_shortlist']['min_trust_level']}) | {stats['passed_trust_level_filter']} |
| **é€šè¿‡æ‰€æœ‰è¿‡æ»¤** | **{stats['passed_all_filters']}** |

---

## ğŸš« è¿‡æ»¤åŸå› åˆ†å¸ƒ

"""

    for reason, count in stats['filter_reasons'].items():
        content += f"- **{reason}**: {count} ä¸ªæœºä¼š\n"

    content += f"""
---

## ğŸ“ˆ æœ€é«˜åˆ†æœºä¼š

**æœ€é«˜åˆ†**: {stats['highest_score']}/10 ï¼ˆä½äºé˜ˆå€¼ {stats['score_threshold']}/10ï¼‰

è¿™æ„å‘³ç€å³ä½¿æ˜¯æœ€å¼ºçš„æœºä¼šä¿¡å·ä¹Ÿæœªèƒ½è¾¾åˆ°æœ€ä½å¯è¡Œæ€§æ ‡å‡†ã€‚

---

## ğŸ’¡ å»ºè®®è¡ŒåŠ¨

"""

    for i, rec in enumerate(result['recommendations'], 1):
        content += f"{i}. {rec}\n"

    content += """
---

## ğŸ“ é…ç½®å‚è€ƒ

å½“å‰è¿‡æ»¤é˜ˆå€¼ï¼š
- `min_viability_score`: {viability_threshold}
- `min_cluster_size`: {size_threshold}
- `min_trust_level`: {trust_threshold}

å¦‚éœ€è°ƒæ•´ï¼Œè¯·ä¿®æ”¹ `config/thresholds.yaml` ä¸­çš„ `decision_shortlist` é…ç½®ã€‚
""".format(
        viability_threshold=self.config['decision_shortlist']['min_viability_score'],
        size_threshold=self.config['decision_shortlist']['min_cluster_size'],
        trust_threshold=self.config['decision_shortlist']['min_trust_level']
    )

    # å†™å…¥æ–‡ä»¶
    filename = f"shortlist_report_empty_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
    filepath = os.path.join(self.config['decision_shortlist']['output']['markdown_dir'], filename)

    os.makedirs(os.path.dirname(filepath), exist_ok=True)
    with open(filepath, 'w', encoding='utf-8') as f:
        f.write(content)

    return filepath
```

**å…³é”®ç‚¹**ï¼š
- ç©ºåˆ—è¡¨æœ¬èº«æ˜¯ä¸€ä¸ªæœ‰ä»·å€¼çš„æ´å¯Ÿï¼šè¡¨æ˜å½“å‰æ•°æ®ä¸­æ²¡æœ‰è¶³å¤Ÿå¼ºçš„æœºä¼šä¿¡å·
- æŠ¥å‘Šåº”è¯¥æ¸…æ™°è¯´æ˜è¿‡æ»¤åŸå› å’Œç»Ÿè®¡ä¿¡æ¯
- æä¾›å¯è¡Œçš„å»ºè®®ï¼ˆé™ä½é˜ˆå€¼ã€ç­‰å¾…æ›´å¤šæ•°æ®ã€æ£€æŸ¥æ•°æ®æºè´¨é‡ï¼‰
- ä½¿ç”¨ç‰¹æ®Šæ–‡ä»¶åï¼ˆ`shortlist_report_empty_*`ï¼‰é¿å…è¦†ç›–æ­£å¸¸æŠ¥å‘Š

---

## ç¬¬ä¸ƒéƒ¨åˆ†ï¼šPipeline é›†æˆ

### 1. åœ¨ run_pipeline.py ä¸­æ·»åŠ  Stage 9

```python
def run_stage_decision_shortlist(self) -> Dict[str, Any]:
    """é˜¶æ®µ9: å†³ç­–æ¸…å•ç”Ÿæˆ"""
    logger.info("=" * 50)
    logger.info("STAGE 9: Decision Shortlist Generation")
    logger.info("=" * 50)

    if self.enable_monitoring:
        performance_monitor.start_stage("decision_shortlist")

    try:
        from pipeline.decision_shortlist import DecisionShortlistGenerator

        generator = DecisionShortlistGenerator()
        result = generator.generate_shortlist()

        self.stats["stage_results"]["decision_shortlist"] = result
        self.stats["stages_completed"].append("decision_shortlist")

        logger.info(f"""
=== Decision Shortlist Generated ===
Total Candidates: {result['total_candidates']}
Selected: {result['shortlist_count']}
Report: {result['markdown_path']}
JSON: {result['json_path']}
""")

        return result

    except Exception as e:
        logger.error(f"Decision Shortlist failed: {e}")
        self.stats["stages_failed"].append("decision_shortlist")
        raise
    finally:
        if self.enable_monitoring:
            performance_monitor.end_stage("decision_shortlist")
```

### 2. ä¿®æ”¹ main() å‡½æ•°

```python
def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="Run Wise Collection Pipeline")

    # ... ç°æœ‰å‚æ•° ...

    # æ·»åŠ æ–° stage
    parser.add_argument(
        "--stage",
        choices=[
            "fetch", "filter", "extract", "embed", "cluster",
            "alignment", "map_opportunities", "score",
            "decision_shortlist",  # æ–°å¢
            "all"
        ],
        default="all",
        help="Pipeline stage to run"
    )

    args = parser.parse_args()

    # ... ç°æœ‰é€»è¾‘ ...

    if args.stage in ["decision_shortlist", "all"]:
        # Stage 9 ä»…åœ¨å‰é¢ stages éƒ½å®Œæˆåè¿è¡Œ
        if args.stage == "decision_shortlist" or "score" in pipeline.stats["stages_completed"]:
            pipeline.run_stage_decision_shortlist()
        elif args.stage == "all":
            logger.warning("Skipping decision_shortlist: prerequisite stages not completed")
```

### 3. å‘½ä»¤è¡Œä½¿ç”¨

```bash
# è¿è¡Œå®Œæ•´ pipelineï¼ˆåŒ…å« decision_shortlistï¼‰
python run_pipeline.py --stage all

# å•ç‹¬è¿è¡Œ decision_shortlist
python run_pipeline.py --stage decision_shortlist

# è¿è¡Œåˆ°æŸä¸ªé˜¶æ®µï¼ˆä¸åŒ…å« decision_shortlistï¼‰
python run_pipeline.py --stage score
```

---

## ç¬¬å…«éƒ¨åˆ†ï¼šé…ç½®æ–‡ä»¶ç»“æ„

åœ¨ `config/thresholds.yaml` ä¸­æ·»åŠ ï¼š

```yaml
# ... ç°æœ‰é…ç½® (filtering_rules, frequency_score_mapping, etc.) ...

# Decision Shortlist é…ç½®
decision_shortlist:
  # ========== ç¡¬æ€§è¿‡æ»¤é˜ˆå€¼ ==========
  min_viability_score: 7.0
  min_cluster_size: 6
  min_trust_level: 0.7
  ignored_clusters: []  # å¯é€‰ï¼šè¦å¿½ç•¥çš„ cluster åç§°åˆ—è¡¨ï¼Œå¦‚ ["test_cluster", "low_quality"]

  # ========== è·¨æºéªŒè¯åŠ åˆ† ==========
  cross_source_boosts:
    level_1: 2.0  # å¼ºä¿¡å·ï¼ˆaligned_problems æˆ– source_type='aligned'ï¼‰
    level_2: 1.0  # ä¸­ç­‰ä¿¡å·ï¼ˆcluster_size >= 10 AND >=3 subredditsï¼‰
    level_3: 0.5  # å¼±ä¿¡å·ï¼ˆcluster_size >= 8 AND >=2 subredditsï¼‰

  # ========== è·¨æºéªŒè¯æ¡ä»¶ ==========
  cross_source_validation:
    level_2:
      min_cluster_size: 10
      min_subreddits: 3
    level_3:
      min_cluster_size: 8
      min_subreddits: 2

  # ========== æœ€ç»ˆè¯„åˆ†æƒé‡ï¼ˆå¯¹æ•°ç¼©æ”¾æ¨¡å‹ï¼‰==========
  final_score_weights:
    # å¯¹æ•°ç¼©æ”¾å…¬å¼ï¼š
    # final_score = (
    #     viability_score * viability_score_weight +
    #     log10(cluster_size) * cluster_size_log_factor +
    #     trust_level * trust_level_weight
    # )
    # å¦‚æœ cross_source_validated: + cross_source_bonus * boost_score

    viability_score: 1.0           # å¯è¡Œæ€§è¯„åˆ†æƒé‡ï¼ˆ0-10 åˆ†ï¼‰
    cluster_size_log_factor: 2.5   # log10(cluster_size) çš„æƒé‡ç³»æ•°
    trust_level: 1.5               # ä¿¡ä»»åº¦æƒé‡ï¼ˆ0-1ï¼‰
    cross_source_bonus: 5.0        # è·¨æºéªŒè¯åŸºç¡€åŠ åˆ†

  # æƒé‡è°ƒæ•´æŒ‡å—ï¼š
  # - viability_score: æé«˜æ­¤æƒé‡ â†’ æ›´é‡è§† LLM è¯„ä¼°çš„å¯è¡Œæ€§
  # - cluster_size_log_factor: æé«˜æ­¤æƒé‡ â†’ æ›´é‡è§†æ•°æ®è§„æ¨¡ï¼ˆä½†å¯¹æ•°ç¼©æ”¾å‡å¼±æç«¯å½±å“ï¼‰
  # - trust_level: æé«˜æ­¤æƒé‡ â†’ æ›´é‡è§†æ•°æ®æºè´¨é‡
  # - cross_source_bonus: æé«˜æ­¤æƒé‡ â†’ æ›´é‡è§†è·¨æºéªŒè¯

  # ç¤ºä¾‹è®¡ç®—ï¼š
  # å‡è®¾æŸä¸ªæœºä¼šï¼š
  #   viability_score = 8.0
  #   cluster_size = 50
  #   trust_level = 0.8
  #   cross_source_level = 1 (boost = 2.0)
  #
  # è®¡ç®—ï¼š
  #   final_score = 8.0 * 1.0 + log10(50) * 2.5 + 0.8 * 1.5 + 2.0 * 5.0 * 0.2
  #              = 8.0 + 1.7 * 2.5 + 1.2 + 2.0
  #              = 8.0 + 4.25 + 1.2 + 2.0
  #              = 15.45 â†’ é™åˆ¶ä¸Šé™ä¸º 10.0

  # ========== å¤šæ ·æ€§æœºåˆ¶ï¼ˆå¯é€‰ï¼‰==========
  diversity:
    enabled: true  # æ˜¯å¦å¯ç”¨å¤šæ ·æ€§æœºåˆ¶

    penalties:
      same_cluster: 0.7      # åŒä¸€ cluster çš„æƒ©ç½šç³»æ•°
      same_pain_type: 0.85   # åŒä¸€ pain_type çš„æƒ©ç½šç³»æ•°
      keyword_overlap: 0.90  # å…³é”®è¯é‡å çš„æƒ©ç½šç³»æ•°

    min_diversity_score_gap: 2.0  # æœ€ä½å¤šæ ·æ€§åˆ†æ•°å·®è·ï¼Œä½äºæ­¤å€¼åœæ­¢é€‰æ‹©

  # ========== è¾“å‡ºè®¾ç½® ==========
  output:
    min_candidates: 3
    max_candidates: 5
    score_gap_threshold: 0.5  # ç”¨äºåŠ¨æ€è°ƒæ•´è¾“å‡ºæ•°é‡
    markdown_dir: "reports"
    json_dir: "data"

  # ========== LLM Prompts ==========
  prompts:
    problem_mvp_whynow: |
      You are a product expert specializing in identifying micro-SaaS opportunities for solo founders.

      Based on the following opportunity data, generate THREE concise, impactful sentences:

      **Opportunity Data:**
      - Name: {opportunity_name}
      - Description: {description}
      - Target Users: {target_users}
      - Missing Capability: {missing_capability}
      - Why Existing Tools Fail: {why_existing_fail}
      - Cluster Summary: {cluster_summary}
      - Pain Events: {cluster_size} unique pain points
      - Cross-Source Validation: {cross_source_info}

      **Output Requirements:**

      1. **Problem Statement** (one sentence, max 30 words)
         Format: "Users in [context/role] are struggling with [specific task] because [structural reason]."

      2. **MVP Cut** (one sentence, max 25 words)
         Format: "A minimal tool that helps them [do X faster/easier/safer] by replacing [current bad workaround]."

      3. **Why Now** (one sentence, max 20 words)
         Format: "This is urgent now because [specific signal/tool failure/multi-community validation]."

      **Constraints:**
      - Be specific and concrete (avoid generic fluff)
      - Focus on actionable insights (not vague observations)
      - Use present tense
      - Each sentence should stand alone (no dependencies)

      **Output Format:**
      Return ONLY a valid JSON object with these exact keys:
      {
        "problem": "one sentence problem statement",
        "mvp": "one sentence mvp description",
        "why_now": "one sentence urgency explanation"
      }

      No additional text, explanations, or markdown formatting.

  # ========== æ—¥å¿—è®¾ç½® ==========
  logging:
    log_filtering_details: true  # è®°å½•æ¯ä¸ªè¿‡æ»¤æ¡ä»¶çš„è¯¦ç»†ç»Ÿè®¡
    log_scoring_breakdown: true  # è®°å½•æ¯ä¸ªæœºä¼šçš„è¯¦ç»†è¯„åˆ†è®¡ç®—
    log_llm_calls: true  # è®°å½• LLM è°ƒç”¨æ¬¡æ•°å’Œæˆæœ¬ä¼°ç®—
    log_diversity_penalties: true  # è®°å½•å¤šæ ·æ€§æƒ©ç½šè¯¦æƒ…ï¼ˆå¦‚æœå¯ç”¨ï¼‰
```

### é…ç½®è¯´æ˜

#### 1. å¯¹æ•°ç¼©æ”¾æƒé‡ç³»ç»Ÿ

**ä¸ä¼ ç»Ÿçº¿æ€§æƒé‡çš„å¯¹æ¯”**ï¼š

ä¼ ç»Ÿçº¿æ€§æ¨¡å‹ï¼ˆå·²åºŸå¼ƒï¼‰ï¼š
```yaml
final_score_weights:
  viability_score: 0.4
  signal_strength: 0.25  # çº¿æ€§ï¼šcluster_size / 10
  trust_level: 0.2
  cross_source_bonus: 0.15
```

å¯¹æ•°ç¼©æ”¾æ¨¡å‹ï¼ˆæ¨èï¼‰ï¼š
```yaml
final_score_weights:
  viability_score: 1.0
  cluster_size_log_factor: 2.5  # å¯¹æ•°ï¼šlog10(cluster_size)
  trust_level: 1.5
  cross_source_bonus: 5.0
```

**è¿ç§»å»ºè®®**ï¼š
- æ–°éƒ¨ç½²ï¼šç›´æ¥ä½¿ç”¨å¯¹æ•°ç¼©æ”¾æ¨¡å‹
- ç°æœ‰éƒ¨ç½²ï¼šå¯ä»¥å…ˆç”¨æ—§æ¨¡å‹ï¼Œè§‚å¯Ÿç»“æœåé€æ­¥è¿ç§»åˆ°å¯¹æ•°æ¨¡å‹
- è°ƒä¼˜ï¼šæ ¹æ®å®é™…æ•ˆæœè°ƒæ•´æƒé‡ç³»æ•°

#### 2. å¤šæ ·æ€§æœºåˆ¶é…ç½®

**ä½•æ—¶å¯ç”¨**ï¼š
- æ•°æ®é‡å¤§ï¼ˆ50+ æœºä¼šï¼‰
- èšç±»é›†ä¸­åº¦é«˜ï¼ˆå¤šä¸ªæœºä¼šæ¥è‡ªåŒä¸€é¢†åŸŸï¼‰
- éœ€è¦æ¢ç´¢ä¸åŒé¢†åŸŸçš„æœºä¼š

**ä½•æ—¶ç¦ç”¨**ï¼š
- æ•°æ®é‡å°ï¼ˆ< 20 æœºä¼šï¼‰
- éœ€è¦ç»å¯¹ä¼˜å…ˆçº§æ’åº
- é¢†åŸŸå·²ç»å¤šæ ·åŒ–

**è°ƒæ•´æƒ©ç½šç³»æ•°**ï¼š
- æ›´ä¸¥æ ¼çš„å¤šæ ·æ€§ï¼šé™ä½æƒ©ç½šç³»æ•°ï¼ˆå¦‚ 0.6, 0.8, 0.85ï¼‰
- æ›´å®½æ¾çš„å¤šæ ·æ€§ï¼šæé«˜æƒ©ç½šç³»æ•°ï¼ˆå¦‚ 0.8, 0.9, 0.95ï¼‰

---

## ç¬¬ä¹éƒ¨åˆ†ï¼šé”™è¯¯å¤„ç†ä¸æ—¥å¿—

### 1. é”™è¯¯å¤„ç†ç­–ç•¥

| é”™è¯¯ç±»å‹ | å¤„ç†ç­–ç•¥ | é™çº§æ–¹æ¡ˆ |
|---------|---------|---------|
| æ•°æ®åº“æŸ¥è¯¢å¤±è´¥ | è®°å½•é”™è¯¯ï¼Œè¿”å›ç©ºåˆ—è¡¨ | æ£€æŸ¥æ•°æ®åº“è¿æ¥å’Œè¡¨ç»“æ„ |
| LLM è°ƒç”¨å¤±è´¥ | è®°å½•è­¦å‘Šï¼Œä½¿ç”¨é™çº§ç­–ç•¥ | ä»ç°æœ‰å­—æ®µèšåˆæå–å†…å®¹ |
| æ–‡ä»¶å†™å…¥å¤±è´¥ | å°è¯•å†™å…¥å¤‡ç”¨è·¯å¾„ï¼ˆ/tmp/ï¼‰ | è®°å½•é”™è¯¯ï¼Œè¿”å›ç»“æœï¼ˆä¸å«æ–‡ä»¶ï¼‰ |
| é…ç½®åŠ è½½å¤±è´¥ | ä½¿ç”¨ç¡¬ç¼–ç é»˜è®¤å€¼ | è®°å½•è­¦å‘Šï¼Œç»§ç»­æ‰§è¡Œ |
| JSON è§£æå¤±è´¥ | é‡è¯•ä¸€æ¬¡ï¼Œä½¿ç”¨æ›´å®½æ¾çš„è§£æ | ä½¿ç”¨é™çº§å†…å®¹ç”Ÿæˆ |

### 2. é™çº§ç­–ç•¥å®ç°

```python
def _generate_readable_content_with_retry(self, opportunity: Dict, cluster: Dict, cross_source_info: Dict) -> Dict[str, str]:
    """å¸¦é‡è¯•å’Œé™çº§çš„å†…å®¹ç”Ÿæˆ"""

    try:
        # é¦–æ¬¡å°è¯•ï¼šLLM ç”Ÿæˆ
        return self._generate_readable_content(opportunity, cluster, cross_source_info)
    except json.JSONDecodeError as e:
        # JSON è§£æå¤±è´¥ï¼šå°è¯•æå– JSON ç‰‡æ®µ
        logger.warning(f"JSON parsing failed for {opportunity['opportunity_name']}: {e}")
        return self._extract_json_from_response(opportunity, cluster, cross_source_info)
    except Exception as e:
        # å…¶ä»–é”™è¯¯ï¼šä½¿ç”¨é™çº§ç­–ç•¥
        logger.error(f"Content generation failed for {opportunity['opportunity_name']}: {e}")
        return self._fallback_readable_content(opportunity, cluster)
```

### 3. æ—¥å¿—è¾“å‡ºç¤ºä¾‹

```
INFO: === Decision Shortlist Generation Started ===
INFO: Loading configuration from config/thresholds.yaml
INFO: Configuration loaded successfully
INFO:
INFO: === Stage 1: Hard Filtering ===
INFO: Total opportunities in database: 50
INFO: Applied viability_score filter (>= 7.0): 50 â†’ 30 passed
INFO: Applied cluster_size filter (>= 6): 30 â†’ 20 passed
INFO: Applied trust_level filter (>= 0.7): 20 â†’ 15 passed
INFO: Hard filtering complete: 15/50 opportunities passed
INFO:
INFO: === Stage 2: Cross-Source Validation ===
INFO: Checking 15 opportunities for cross-source validation
INFO: Level 1 (strong signal): 3 opportunities found
INFO:   - cluster_42: Found in aligned_problems (AP_01)
INFO:   - cluster_17: source_type='aligned'
INFO:   - cluster_23: Found in aligned_problems (AP_03)
INFO: Level 2 (medium signal): 3 opportunities found
INFO:   - cluster_08: cluster_size=12, subreddits=4
INFO:   - cluster_15: cluster_size=11, subreddits=3
INFO:   - cluster_31: cluster_size=10, subreddits=3
INFO: Level 3 (weak signal): 2 opportunities found
INFO:   - cluster_05: cluster_size=9, subreddits=2
INFO:   - cluster_19: cluster_size=8, subreddits=2
INFO: Cross-source validation complete: 8/15 validated
INFO:
INFO: === Stage 3: Final Scoring ===
INFO: Calculating final scores for 15 opportunities
INFO: Score breakdown (top 5):
INFO:   1. cluster_42: viability=8.2, signal=10.0, trust=0.85, cross_bonus=2.0 â†’ final=8.7
INFO:   2. cluster_17: viability=7.8, signal=9.0, trust=0.80, cross_bonus=2.0 â†’ final=8.2
INFO:   3. cluster_08: viability=7.5, signal=10.0, trust=0.75, cross_bonus=1.0 â†’ final=7.8
INFO:   4. cluster_15: viability=7.2, signal=9.0, trust=0.70, cross_bonus=1.0 â†’ final=7.4
INFO:   5. cluster_23: viability=8.0, signal=8.0, trust=0.82, cross_bonus=2.0 â†’ final=7.1
INFO:
INFO: === Stage 4: LLM Content Generation ===
INFO: Generating Problem/MVP/Why Now for top 5 opportunities
INFO: [1/5] Generating for cluster_42... OK (3.2s)
INFO: [2/5] Generating for cluster_17... OK (2.8s)
INFO: [3/5] Generating for cluster_08... OK (3.5s)
INFO: [4/5] Generating for cluster_15... OK (2.9s)
INFO: [5/5] Generating for cluster_23... OK (3.1s)
INFO: LLM generation complete: 5/5 success (avg 3.1s each)
INFO: Estimated LLM cost: $0.05
INFO:
INFO: === Stage 5: Export Results ===
INFO: Writing Markdown report to: reports/shortlist_report_20251225.md
INFO: Writing JSON file to: data/decision_shortlist.json
INFO: Export complete
INFO:
INFO: === Decision Shortlist Complete ===
INFO: Total Candidates: 15
INFO: Selected: 5 (score range: 8.7 - 7.1)
INFO: Report: reports/shortlist_report_20251225.md
INFO: JSON: data/decision_shortlist.json
INFO: Total processing time: 45.3s
```

### 4. æ—¥å¿—çº§åˆ«

- **DEBUG**: è¯¦ç»†çš„è¯„åˆ†è®¡ç®—è¿‡ç¨‹
- **INFO**: å…³é”®æ­¥éª¤å’Œç»Ÿè®¡ä¿¡æ¯ï¼ˆé»˜è®¤ï¼‰
- **WARNING**: é™çº§ç­–ç•¥ä½¿ç”¨å’Œå¯æ¢å¤çš„é”™è¯¯
- **ERROR**: ä¸¥é‡é”™è¯¯å’Œå¤±è´¥æ“ä½œ

---

## ç¬¬åéƒ¨åˆ†ï¼šMilestone 1 éªŒæ”¶æµ‹è¯•

### 1. éªŒæ”¶æµ‹è¯•è„šæœ¬

æ–‡ä»¶ä½ç½®ï¼š`tests/test_decision_shortlist.py`

```python
#!/usr/bin/env python3
"""
Decision Shortlist Milestone 1 éªŒæ”¶æµ‹è¯•
éªŒè¯ç³»ç»Ÿæ˜¯å¦æ»¡è¶³ï¼šä» 50+ æœºä¼šä¸­ç­›é€‰å‡º Top 3-5 ä¸ªå¯æ‰§è¡Œæ¸…å•
"""

import os
import sys
import json
import logging
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)

from pipeline.decision_shortlist import DecisionShortlistGenerator
from utils.db import db

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def test_decision_shortlist_milestone1():
    """éªŒæ”¶æµ‹è¯•ï¼šMilestone 1 åŠŸèƒ½éªŒè¯"""

    print("\n" + "="*60)
    print("ğŸ§ª Decision Shortlist Milestone 1 éªŒæ”¶æµ‹è¯•")
    print("="*60 + "\n")

    # ========== æµ‹è¯• 1: è¿è¡Œ Decision Shortlist ==========
    print("ğŸ“‹ æµ‹è¯• 1: è¿è¡Œ Decision Shortlist...")
    generator = DecisionShortlistGenerator()
    result = generator.generate_shortlist()

    assert result is not None, "Result should not be None"
    print("âœ… Decision Shortlist æ‰§è¡ŒæˆåŠŸ\n")

    # ========== æµ‹è¯• 2: éªŒè¯è¾“å‡ºæ•°é‡ ==========
    print("ğŸ“‹ æµ‹è¯• 2: éªŒè¯è¾“å‡ºæ•°é‡...")
    shortlist_count = result['shortlist_count']
    assert 3 <= shortlist_count <= 5, f"Should output 3-5 candidates, got {shortlist_count}"
    print(f"âœ… è¾“å‡ºæ•°é‡æ­£ç¡®: {shortlist_count} ä¸ªå€™é€‰æœºä¼š\n")

    # ========== æµ‹è¯• 3: éªŒè¯æ¯ä¸ªå€™é€‰çš„å®Œæ•´æ€§ ==========
    print("ğŸ“‹ æµ‹è¯• 3: éªŒè¯æ¯ä¸ªå€™é€‰çš„å®Œæ•´æ€§...")
    for i, candidate in enumerate(result['shortlist'], 1):
        print(f"  æ£€æŸ¥ Candidate {i}...")

        # å¿…éœ€å­—æ®µ
        assert 'problem' in candidate, f"Candidate {i} missing problem statement"
        assert 'mvp' in candidate, f"Candidate {i} missing MVP cut"
        assert 'why_now' in candidate, f"Candidate {i} missing why now"
        assert 'final_score' in candidate, f"Candidate {i} missing final_score"
        assert 'opportunity_name' in candidate, f"Candidate {i} missing opportunity_name"

        # é•¿åº¦é™åˆ¶ï¼ˆç¡®ä¿ç®€æ´ï¼‰
        problem_len = len(candidate['problem'])
        mvp_len = len(candidate['mvp'])
        why_now_len = len(candidate['why_now'])

        assert problem_len <= 200, f"Problem too long: {problem_len} chars (max 200)"
        assert mvp_len <= 150, f"MVP too long: {mvp_len} chars (max 150)"
        assert why_now_len <= 150, f"Why now too long: {why_now_len} chars (max 150)"

        # éç©ºæ£€æŸ¥
        assert candidate['problem'].strip(), f"Candidate {i} problem is empty"
        assert candidate['mvp'].strip(), f"Candidate {i} MVP is empty"
        assert candidate['why_now'].strip(), f"Candidate {i} why_now is empty"

        # åˆ†æ•°èŒƒå›´
        assert 0 <= candidate['final_score'] <= 10, f"Invalid final_score: {candidate['final_score']}"

        print(f"    âœ… Candidate {i} å®Œæ•´ä¸”æ ¼å¼æ­£ç¡®")

    print(f"âœ… æ‰€æœ‰ {shortlist_count} ä¸ªå€™é€‰æœºä¼šéªŒè¯é€šè¿‡\n")

    # ========== æµ‹è¯• 4: éªŒè¯æ–‡ä»¶ç”Ÿæˆ ==========
    print("ğŸ“‹ æµ‹è¯• 4: éªŒè¯æ–‡ä»¶ç”Ÿæˆ...")
    markdown_path = result.get('markdown_path')
    json_path = result.get('json_path')

    assert markdown_path, "Missing markdown_path in result"
    assert json_path, "Missing json_path in result"
    assert os.path.exists(markdown_path), f"Markdown report not found: {markdown_path}"
    assert os.path.exists(json_path), f"JSON file not found: {json_path}"

    print(f"âœ… æ–‡ä»¶ç”ŸæˆæˆåŠŸ:")
    print(f"   - Markdown: {markdown_path}")
    print(f"   - JSON: {json_path}\n")

    # ========== æµ‹è¯• 5: éªŒè¯ JSON æ ¼å¼ ==========
    print("ğŸ“‹ æµ‹è¯• 5: éªŒè¯ JSON æ ¼å¼...")
    with open(json_path, 'r', encoding='utf-8') as f:
        json_data = json.load(f)

    assert isinstance(json_data, list), "JSON root should be a list"
    assert len(json_data) == shortlist_count, f"JSON count mismatch: {len(json_data)} vs {shortlist_count}"

    for i, item in enumerate(json_data, 1):
        assert 'problem' in item, f"JSON item {i} missing problem"
        assert 'mvp' in item, f"JSON item {i} missing mvp"
        assert 'why_now' in item, f"JSON item {i} missing why_now"

    print(f"âœ… JSON æ ¼å¼æ­£ç¡®ï¼ŒåŒ…å« {len(json_data)} ä¸ªæœºä¼š\n")

    # ========== æµ‹è¯• 6: éªŒè¯ç¡¬æ€§è¿‡æ»¤è§„åˆ™ ==========
    print("ğŸ“‹ æµ‹è¯• 6: éªŒè¯ç¡¬æ€§è¿‡æ»¤è§„åˆ™...")
    for candidate in result['shortlist']:
        assert candidate['viability_score'] >= 7.0, f"Viability score too low: {candidate['viability_score']}"
        assert candidate['cluster_size'] >= 6, f"Cluster size too small: {candidate['cluster_size']}"
        assert candidate['trust_level'] >= 0.7, f"Trust level too low: {candidate['trust_level']}"

    print("âœ… æ‰€æœ‰å€™é€‰æœºä¼šéƒ½é€šè¿‡ç¡¬æ€§è¿‡æ»¤\n")

    # ========== æµ‹è¯• 7: éªŒè¯è·¨æºéªŒè¯åŠ åˆ† ==========
    print("ğŸ“‹ æµ‹è¯• 7: éªŒè¯è·¨æºéªŒè¯åŠ åˆ†...")
    cross_source_validated = sum(1 for c in result['shortlist'] if c.get('validated_problem', False))
    print(f"   - è·¨æºéªŒè¯é€šè¿‡: {cross_source_validated}/{shortlist_count}")
    print(f"   - åŠ åˆ†åˆ†å¸ƒ:")
    for level in [1, 2, 3]:
        count = sum(1 for c in result['shortlist'] if c.get('cross_source_level') == level)
        print(f"     Level {level}: {count} ä¸ªæœºä¼š")

    print("âœ… è·¨æºéªŒè¯é€»è¾‘æ­£ç¡®\n")

    # ========== æµ‹è¯• 8: äººç±»å¯è¯»æ€§æ£€æŸ¥ ==========
    print("ğŸ“‹ æµ‹è¯• 8: äººç±»å¯è¯»æ€§æ£€æŸ¥ï¼ˆäººå·¥éªŒè¯ï¼‰...")
    print("\n" + "="*60)
    print("è¯·äººå·¥æ£€æŸ¥ä»¥ä¸‹è¾“å‡ºæ˜¯å¦ç¬¦åˆ 10 åˆ†é’Ÿå†³ç­–æ ‡å‡†ï¼š")
    print("="*60 + "\n")

    # æ‰“å° Markdown æŠ¥å‘Šçš„å‰éƒ¨åˆ†
    with open(markdown_path, 'r', encoding='utf-8') as f:
        lines = f.readlines()
        # æ‰“å°å‰ 80 è¡Œï¼ˆçº¦ 2-3 ä¸ªå€™é€‰æœºä¼šï¼‰
        print(''.join(lines[:80]))

    print("\n" + "="*60)
    print("âœ… è¯·äººå·¥éªŒè¯ï¼š")
    print("  1. æ¯ä¸ªæœºä¼šæ˜¯å¦èƒ½åœ¨ 1 åˆ†é’Ÿå†…ç†è§£ï¼Ÿ")
    print("  2. Problem / MVP / Why Now æ˜¯å¦ç®€æ´æœ‰åŠ›ï¼Ÿ")
    print("  3. èƒ½å¦æ ¹æ®è¿™äº›ä¿¡æ¯å¿«é€Ÿå†³ç­–åš or ä¸åšï¼Ÿ")
    print("="*60 + "\n")

    # ========== æµ‹è¯•æ€»ç»“ ==========
    print("\n" + "="*60)
    print("ğŸ‰ æ‰€æœ‰è‡ªåŠ¨åŒ–æµ‹è¯•é€šè¿‡ï¼")
    print("="*60)
    print(f"\nğŸ“Š æµ‹è¯•æ€»ç»“:")
    print(f"   - è¾“å…¥æœºä¼šæ€»æ•°: {result.get('total_candidates', 'N/A')}")
    print(f"   - é€šè¿‡ç¡¬æ€§è¿‡æ»¤: {result.get('passed_filters', 'N/A')}")
    print(f"   - è·¨æºéªŒè¯é€šè¿‡: {result.get('cross_source_validated', 'N/A')}")
    print(f"   - æœ€ç»ˆå…¥é€‰: {shortlist_count}")
    print(f"   - åˆ†æ•°èŒƒå›´: {result['shortlist'][0]['final_score']:.1f} - {result['shortlist'][-1]['final_score']:.1f}")
    print(f"\nğŸ“„ è¾“å‡ºæ–‡ä»¶:")
    print(f"   - Markdown: {markdown_path}")
    print(f"   - JSON: {json_path}")
    print("\nâœ… Milestone 1 éªŒæ”¶æµ‹è¯•é€šè¿‡ï¼\n")

    return True


if __name__ == "__main__":
    try:
        success = test_decision_shortlist_milestone1()
        sys.exit(0 if success else 1)
    except AssertionError as e:
        logger.error(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        sys.exit(1)
    except Exception as e:
        logger.error(f"âŒ æœªé¢„æœŸçš„é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
```

### 2. è¿è¡Œæµ‹è¯•

```bash
# è¿è¡ŒéªŒæ”¶æµ‹è¯•
python tests/test_decision_shortlist.py

# æˆ–é€šè¿‡ pytest
pytest tests/test_decision_shortlist.py -v
```

### 3. éªŒæ”¶æ ‡å‡†

âœ… **åŠŸèƒ½éªŒè¯**ï¼š
- [x] Pipeline è·‘å®Œåï¼Œç³»ç»Ÿè‡ªåŠ¨åªç»™ 3-5 ä¸ªå€™é€‰æœºä¼š
- [x] æ¯ä¸ªå€™é€‰åŒ…å« Problem / MVP / Why Now ä¸‰å¥è¯
- [x] ä¸ç”¨æ‰“å¼€ä»£ç ï¼Œä¸€çœ‹å°±èƒ½ç†è§£
- [x] ä¸ç”¨äºŒæ¬¡æ€è€ƒï¼Œä¸€è¯»å°±èƒ½è®¨è®º
- [x] èƒ½åœ¨ 10 åˆ†é’Ÿå†…å†³å®šåš or ä¸åš

âœ… **æŠ€æœ¯éªŒè¯**ï¼š
- [x] ä»æ•°æ®åº“è¯»å– opportunities
- [x] åº”ç”¨ç¡¬æ€§è¿‡æ»¤è§„åˆ™ï¼ˆviability >= 7.0, cluster_size >= 6, trust_level >= 0.7ï¼‰
- [x] ä¸‰å±‚è·¨æºéªŒè¯ï¼ˆLevel 1/2/3ï¼‰
- [x] æœ€ç»ˆè¯„åˆ†è®¡ç®—æ­£ç¡®
- [x] LLM ç”Ÿæˆå†…å®¹ç¬¦åˆæ ¼å¼è¦æ±‚
- [x] Markdown å’Œ JSON æ–‡ä»¶ç”ŸæˆæˆåŠŸ

---

## ç¬¬åä¸€éƒ¨åˆ†ï¼šå®æ–½è®¡åˆ’

### é˜¶æ®µ 1ï¼šæ ¸å¿ƒåŠŸèƒ½ï¼ˆ1-2 å¤©ï¼‰

1. åˆ›å»º `pipeline/decision_shortlist.py`
   - å®ç° `DecisionShortlistGenerator` ç±»éª¨æ¶
   - å®ç°ç¡¬æ€§è¿‡æ»¤é€»è¾‘
   - å®ç°è·¨æºéªŒè¯é€»è¾‘
   - å®ç°æœ€ç»ˆè¯„åˆ†è®¡ç®—

2. æ›´æ–°é…ç½®æ–‡ä»¶
   - åœ¨ `config/thresholds.yaml` æ·»åŠ  `decision_shortlist` é…ç½®

3. ç¼–å†™å•å…ƒæµ‹è¯•
   - æµ‹è¯•ç¡¬æ€§è¿‡æ»¤é€»è¾‘
   - æµ‹è¯•è·¨æºéªŒè¯é€»è¾‘
   - æµ‹è¯•è¯„åˆ†è®¡ç®—é€»è¾‘

### é˜¶æ®µ 2ï¼šLLM é›†æˆï¼ˆ1 å¤©ï¼‰

1. å®ç° LLM å†…å®¹ç”Ÿæˆ
   - è®¾è®¡å¹¶æµ‹è¯• prompt æ¨¡æ¿
   - å®ç° `_generate_readable_content()` æ–¹æ³•
   - å®ç°é™çº§ç­–ç•¥

2. æµ‹è¯• LLM è¾“å‡ºè´¨é‡
   - ç”Ÿæˆå¤šä¸ªç¤ºä¾‹ï¼Œäººå·¥æ£€æŸ¥è´¨é‡
   - ä¼˜åŒ– prompt ä»¥æé«˜è¾“å‡ºè´¨é‡

### é˜¶æ®µ 3ï¼šPipeline é›†æˆï¼ˆ0.5 å¤©ï¼‰

1. é›†æˆåˆ° `run_pipeline.py`
   - æ·»åŠ  Stage 9 å¤„ç†å‡½æ•°
   - æ›´æ–°å‘½ä»¤è¡Œå‚æ•°
   - æµ‹è¯•å®Œæ•´ pipeline è¿è¡Œ

2. æµ‹è¯•ç‹¬ç«‹è¿è¡Œ
   - æµ‹è¯• `--stage decision_shortlist` æ¨¡å¼
   - æµ‹è¯•ä¸å‰ç½® stages çš„ä¾èµ–å…³ç³»

### é˜¶æ®µ 4ï¼šè¾“å‡ºæ ¼å¼ä¸éªŒæ”¶ï¼ˆ0.5 å¤©ï¼‰

1. å®ç°è¾“å‡ºåŠŸèƒ½
   - å®ç° Markdown æŠ¥å‘Šç”Ÿæˆ
   - å®ç° JSON æ–‡ä»¶å¯¼å‡º
   - æ·»åŠ ç»Ÿè®¡ä¿¡æ¯è¾“å‡º

2. è¿è¡ŒéªŒæ”¶æµ‹è¯•
   - æ‰§è¡Œ `test_decision_shortlist.py`
   - äººå·¥æ£€æŸ¥è¾“å‡ºè´¨é‡
   - ç¡®è®¤æ»¡è¶³ Milestone 1 éªŒæ”¶æ ‡å‡†

### é˜¶æ®µ 5ï¼šæ–‡æ¡£ä¸ä¼˜åŒ–ï¼ˆ0.5 å¤©ï¼‰

1. ç¼–å†™æ–‡æ¡£
   - æ›´æ–° README.md
   - æ·»åŠ ä½¿ç”¨ç¤ºä¾‹
   - æ·»åŠ é…ç½®è¯´æ˜

2. æ€§èƒ½ä¼˜åŒ–
   - ä¼˜åŒ–æ•°æ®åº“æŸ¥è¯¢
   - æ·»åŠ ç¼“å­˜æœºåˆ¶
   - å‡å°‘ä¸å¿…è¦çš„ LLM è°ƒç”¨

**æ€»è®¡ä¼°è®¡æ—¶é—´ï¼š3-4 å¤©**

---

## ç¬¬åäºŒéƒ¨åˆ†ï¼šé£é™©ä¸ç¼“è§£

| é£é™© | å½±å“ | æ¦‚ç‡ | ç¼“è§£æªæ–½ |
|------|------|------|---------|
| LLM è¾“å‡ºè´¨é‡ä¸ç¨³å®š | é«˜ | ä¸­ | ä½¿ç”¨ç²¾å¿ƒè®¾è®¡çš„ prompt + é™çº§ç­–ç•¥ |
| è·¨æºéªŒè¯è¯¯åˆ¤ | ä¸­ | ä½ | ä¸‰å±‚ä¼˜å…ˆçº§è®¾è®¡ï¼Œå¼±ä¿¡å·ä»…ä½œä¸ºè¾…åŠ© |
| è¾“å‡ºæœºä¼šæ•°é‡è¿‡å°‘ | ä¸­ | ä¸­ | åŠ¨æ€è°ƒæ•´é˜ˆå€¼ï¼Œè®°å½•æ—¥å¿—ä¾›è°ƒè¯• |
| æ•°æ®åº“æŸ¥è¯¢æ€§èƒ½ | ä½ | ä½ | æ·»åŠ ç´¢å¼•ï¼Œä¼˜åŒ– SQL æŸ¥è¯¢ |
| é…ç½®è¿‡äºå¤æ‚ | ä½ | ä¸­ | æä¾›é»˜è®¤é…ç½®ï¼Œæ·»åŠ é…ç½®ç¤ºä¾‹ |

---

## é™„å½•ï¼šå¿«é€Ÿå‚è€ƒ

### å…³é”®æ–‡ä»¶ä½ç½®

```
reddit_pain_finder/
â”œâ”€â”€ pipeline/
â”‚   â””â”€â”€ decision_shortlist.py          # æ–°å¢ï¼šä¸»æ¨¡å—
â”œâ”€â”€ config/
â”‚   â””â”€â”€ thresholds.yaml                 # ä¿®æ”¹ï¼šæ·»åŠ  decision_shortlist é…ç½®
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_decision_shortlist.py     # æ–°å¢ï¼šéªŒæ”¶æµ‹è¯•
â”œâ”€â”€ reports/
â”‚   â””â”€â”€ shortlist_report_YYYYMMDD.md   # è‡ªåŠ¨ç”Ÿæˆï¼šMarkdown æŠ¥å‘Š
â”œâ”€â”€ data/
â”‚   â””â”€â”€ decision_shortlist.json        # è‡ªåŠ¨ç”Ÿæˆï¼šJSON è¾“å‡º
â””â”€â”€ run_pipeline.py                     # ä¿®æ”¹ï¼šé›†æˆ Stage 9
```

### å…³é”®é…ç½®å‚æ•°

```yaml
decision_shortlist:
  # ç¡¬æ€§è¿‡æ»¤é˜ˆå€¼
  min_viability_score: 7.0        # æœ€ä½å¯è¡Œæ€§è¯„åˆ†
  min_cluster_size: 6             # æœ€å°èšç±»è§„æ¨¡
  min_trust_level: 0.7            # æœ€ä½ä¿¡ä»»åº¦

  # è·¨æºéªŒè¯åŠ åˆ†
  cross_source_boosts:
    level_1: 2.0                  # å¼ºä¿¡å·åŠ åˆ†
    level_2: 1.0                  # ä¸­ç­‰ä¿¡å·åŠ åˆ†
    level_3: 0.5                  # å¼±ä¿¡å·åŠ åˆ†

  # å¯¹æ•°ç¼©æ”¾è¯„åˆ†æƒé‡ï¼ˆæ–°æ¨¡å‹ï¼‰
  final_score_weights:
    viability_score: 1.0           # å¯è¡Œæ€§è¯„åˆ†æƒé‡
    cluster_size_log_factor: 2.5   # log10(cluster_size) æƒé‡
    trust_level: 1.5               # ä¿¡ä»»åº¦æƒé‡
    cross_source_bonus: 5.0        # è·¨æºéªŒè¯åŸºç¡€åŠ åˆ†

  # å¤šæ ·æ€§æœºåˆ¶ï¼ˆå¯é€‰ï¼‰
  diversity:
    enabled: true                 # æ˜¯å¦å¯ç”¨å¤šæ ·æ€§æƒ©ç½š
    penalties:
      same_cluster: 0.7            # åŒ cluster æƒ©ç½š
      same_pain_type: 0.85         # åŒ pain_type æƒ©ç½š
      keyword_overlap: 0.90        # å…³é”®è¯é‡å æƒ©ç½š
```

### å‘½ä»¤è¡Œä½¿ç”¨

```bash
# å®Œæ•´ pipelineï¼ˆåŒ…å« decision_shortlistï¼‰
python run_pipeline.py --stage all

# å•ç‹¬è¿è¡Œ decision_shortlist
python run_pipeline.py --stage decision_shortlist

# è¿è¡ŒéªŒæ”¶æµ‹è¯•
python tests/test_decision_shortlist.py
```

---

**æ–‡æ¡£ç‰ˆæœ¬**: 1.1
**åˆ›å»ºæ—¥æœŸ**: 2025-12-26
**æœ€åæ›´æ–°**: 2025-12-26
**ä½œè€…**: Claude Code
**çŠ¶æ€**: å¾…å®¡é˜…

**å˜æ›´å†å²**ï¼š
- **v1.1** (2025-12-26):
  - âœ¨ æ–°å¢ï¼šå¯¹æ•°ç¼©æ”¾è¯„åˆ†æ¨¡å‹ï¼Œæ›¿ä»£çº¿æ€§ç¼©æ”¾
  - âœ¨ æ–°å¢ï¼šé…ç½®åŒ–æƒé‡ç³»ç»Ÿï¼Œæ‰€æœ‰æƒé‡ç³»æ•°å¯åœ¨ config ä¸­è°ƒæ•´
  - âœ¨ æ–°å¢ï¼šå¤šæ ·æ€§ä¿è¯æœºåˆ¶ï¼ˆå¯é€‰é«˜çº§åŠŸèƒ½ï¼‰
  - âœ¨ æ–°å¢ï¼šç©ºåˆ—è¡¨å¤„ç†é€»è¾‘å’ŒæŠ¥å‘Šæ ¼å¼
  - ğŸ“ æ›´æ–°ï¼šé…ç½®æ–‡ä»¶ç»“æ„ï¼Œæ·»åŠ å¯¹æ•°ç¼©æ”¾å’Œå¤šæ ·æ€§é…ç½®
- **v1.0** (2025-12-26):
  - ğŸ‰ åˆå§‹ç‰ˆæœ¬

---
