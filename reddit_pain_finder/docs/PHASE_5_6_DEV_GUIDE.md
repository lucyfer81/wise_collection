# Phase 5-6 å¼€å‘æŒ‡å— (Development Guide)

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0
**åˆ›å»ºæ—¥æœŸ**: 2026-01-13
**é€‚ç”¨åˆ†æ”¯**: `pipeline-upgrade`
**ç›®æ ‡è¯»è€…**: åç«¯å·¥ç¨‹å¸ˆ

---

## ğŸ“‹ ç›®å½•

1. [é¡¹ç›®èƒŒæ™¯](#é¡¹ç›®èƒŒæ™¯)
2. [Phase 5: Pipelineé›†æˆ](#phase-5-pipelineé›†æˆ)
3. [Phase 6: æ€§èƒ½ä¼˜åŒ–](#phase-6-æ€§èƒ½ä¼˜åŒ–)
4. [éªŒæ”¶æ ‡å‡†](#éªŒæ”¶æ ‡å‡†)
5. [æµ‹è¯•æŒ‡å—](#æµ‹è¯•æŒ‡å—)
6. [æ³¨æ„äº‹é¡¹](#æ³¨æ„äº‹é¡¹)
7. [å›æ»šè®¡åˆ’](#å›æ»šè®¡åˆ’)

---

## é¡¹ç›®èƒŒæ™¯

### å·²å®Œæˆå·¥ä½œ (Phase 1-4)

âœ… **Phase 1**: æ•°æ®åº“Schemaè¿ç§»
- æ·»åŠ lifecycleå­—æ®µåˆ°`pain_events`è¡¨
- æ”¯æŒ'active', 'orphan', 'archived'çŠ¶æ€

âœ… **Phase 2**: Chromaå‘é‡æ•°æ®åº“é›†æˆ
- 2242 embeddingsè¿ç§»å®Œæˆ
- æœ¬åœ°å­˜å‚¨: `data/chroma_db/` (9.8MB)

âœ… **Phase 3**: DynamicClusterUpdaterå®ç°
- å®æ—¶clusteræ›´æ–°é€»è¾‘
- è‡ªåŠ¨åˆå¹¶å’Œåˆ›å»ºclusters

âœ… **Phase 4**: Lifecycleæ¸…ç†ç³»ç»Ÿ
- 14å¤©è‡ªåŠ¨åˆ é™¤orphans
- 90å¤©å½’æ¡£inactive clusters

### æµ‹è¯•ç»“æœ

- âœ… Chromaæ€§èƒ½: **1.9ms/query, 514 qps**
- âœ… æ•°æ®ä¸€è‡´æ€§: **100%** (SQLite â†” Chroma)
- âœ… Dynamic clustering: **5/5** eventsæˆåŠŸclustered
- âœ… Retention rate: **65.5%** (è‡ªåŠ¨è¯†åˆ«æœ‰ä»·å€¼pattern)

---

## Phase 5: Pipelineé›†æˆ

### ç›®æ ‡

å°†Phase 1-4çš„ç»„ä»¶é›†æˆåˆ°ä¸»pipelineä¸­ï¼Œæ›¿æ¢æ—§çš„é™æ€èšç±»é€»è¾‘ã€‚

### å½“å‰Pipelineæ¶æ„

```
run_pipeline.py
â”œâ”€â”€ Stage 1: Fetch (fetch.py)         â† æ— éœ€ä¿®æ”¹
â”œâ”€â”€ Stage 2: Filter (filter_signal.py) â† æ— éœ€ä¿®æ”¹
â”œâ”€â”€ Stage 3: Extract (extract_pain.py) â† æ— éœ€ä¿®æ”¹
â”œâ”€â”€ Stage 4: Embed (embed.py)         â† âœ… å·²ä¿®æ”¹ä¸ºä½¿ç”¨Chroma
â”œâ”€â”€ Stage 5: Cluster (cluster.py)     â† ğŸ”´ éœ€è¦æ›¿æ¢ä¸ºDynamicClusterUpdater
â”œâ”€â”€ Stage 6: Map (map_opportunity.py)  â† æ— éœ€ä¿®æ”¹
â”œâ”€â”€ Stage 7: Score (score_viability.py) â† æ— éœ€ä¿®æ”¹
â”œâ”€â”€ Stage 8: Decision (decision_shortlist.py) â† æ— éœ€ä¿®æ”¹
â””â”€â”€ [NEW] Lifecycle Cleanup           â† ğŸ”´ éœ€è¦æ·»åŠ 
```

### ä»»åŠ¡æ¸…å•

#### 5.1 æ›´æ–°run_pipeline.py

**æ–‡ä»¶**: `run_pipeline.py`

**ç›®æ ‡**: æ›¿æ¢Stage 5çš„clusteré€»è¾‘

**å½“å‰ä»£ç ** (çº¦347-381è¡Œ):
```python
def run_stage_cluster(self, limit_events: Optional[int] = None, process_all: bool = False):
    """é˜¶æ®µ5: èšç±»"""
    logger.info("=" * 50)
    logger.info("STAGE 5: Clustering pain events")
    logger.info("=" * 50)

    if self.enable_monitoring:
        performance_monitor.start_stage("cluster")

    try:
        clusterer = PainEventClusterer()  # â† æ—§é€»è¾‘

        # å¦‚æœ process_all=True ä¸”æœªæŒ‡å®š limitï¼Œåˆ™å¤„ç†æ‰€æœ‰æ•°æ®ï¼ˆè®¾ç½®ä¸ºå¤§æ•°å€¼ï¼‰
        if process_all and limit_events is None:
            limit_events = 1000000  # å¤„ç†æ‰€æœ‰æ•°æ®
        elif limit_events is None:
            limit_events = 200

        result = clusterer.cluster_pain_events(limit=limit_events)  # â† æ—§æ–¹æ³•

        # ... å…¶ä½™ä»£ç 
```

**æ–°ä»£ç **:
```python
def run_stage_cluster(self, limit_events: Optional[int] = None, process_all: bool = False):
    """é˜¶æ®µ5: åŠ¨æ€èšç±»æ›´æ–° (Dynamic Clustering)"""
    logger.info("=" * 50)
    logger.info("STAGE 5: Dynamic Clustering (Real-time)")
    logger.info("=" * 50)

    if self.enable_monitoring:
        performance_monitor.start_stage("cluster")

    try:
        from pipeline.dynamic_cluster import DynamicClusterUpdater  # â† å¯¼å…¥æ–°çš„

        # åˆå§‹åŒ–åŠ¨æ€èšç±»å™¨
        clusterer = DynamicClusterUpdater()

        # è·å–éœ€è¦å¤„ç†çš„pain_events (æ–°å¢çš„æˆ–unclusteredçš„)
        with db.get_connection("pain") as conn:
            # ç­–ç•¥ï¼šå¤„ç†æ‰€æœ‰æœªclusteredçš„events + æœ€è¿‘çš„events (é‡æ–°è®¡ç®—)
            if process_all and limit_events is None:
                # å¤„ç†æ‰€æœ‰unclustered events
                cursor = conn.execute("""
                    SELECT pe.*, em.embedding_vector
                    FROM pain_events pe
                    JOIN pain_embeddings em ON pe.id = em.pain_event_id
                    WHERE pe.cluster_id IS NULL
                    ORDER BY pe.extracted_at DESC
                """)
                new_events = [dict(row) for row in cursor.fetchall()]
            else:
                # é™åˆ¶å¤„ç†æ•°é‡
                limit = limit_events if limit_events else 200
                cursor = conn.execute("""
                    SELECT pe.*, em.embedding_vector
                    FROM pain_events pe
                    JOIN pain_embeddings em ON pe.id = em.pain_event_id
                    WHERE pe.cluster_id IS NULL
                    ORDER BY pe.extracted_at DESC
                    LIMIT ?
                """, (limit,))
                new_events = [dict(row) for row in cursor.fetchall()]

        logger.info(f"Found {len(new_events)} events to process")

        if not new_events:
            logger.info("No new events to cluster")
            result = {
                'clusters_created': 0,
                'clusters_updated': 0,
                'events_processed': 0
            }
        else:
            # ä½¿ç”¨DynamicClusterUpdaterå¤„ç†
            stats = clusterer.process_new_pain_events(new_events)

            result = {
                'clusters_created': stats['new_clusters_created'],
                'clusters_updated': stats['existing_clusters_updated'],
                'events_processed': stats['total_events_processed']
            }

        self.stats["stages_completed"].append("cluster")
        self.stats["stage_results"]["cluster"] = result

        if self.enable_monitoring:
            # ä½¿ç”¨events_processedä½œä¸ºå¤„ç†æ•°é‡
            performance_monitor.end_stage("cluster", result.get('events_processed', 0))

        logger.info(f"âœ… Stage 5 completed:")
        logger.info(f"   Events processed: {result.get('events_processed', 0)}")
        logger.info(f"   New clusters: {result.get('clusters_created', 0)}")
        logger.info(f"   Updated clusters: {result.get('clusters_updated', 0)}")
        return result

    except Exception as e:
        logger.error(f"âŒ Stage 5 failed: {e}")
        self.stats["stages_failed"].append("cluster")
        if self.enable_monitoring:
            performance_monitor.end_stage("cluster", 0)
        raise
```

**å…³é”®å˜æ›´**:
1. å¯¼å…¥`DynamicClusterUpdater`è€Œé`PainEventClusterer`
2. æŸ¥è¯¢é€»è¾‘æ”¹ä¸ºåªè·å–`cluster_id IS NULL`çš„events
3. è°ƒç”¨`process_new_pain_events()`è€Œé`cluster_pain_events()`
4. è¿”å›å€¼é€‚é…æ–°çš„ç»Ÿè®¡æ ¼å¼

---

#### 5.2 æ·»åŠ Lifecycle Cleanup Stage

**æ–‡ä»¶**: `run_pipeline.py`

**ç›®æ ‡**: åœ¨pipelineæœ€åæ·»åŠ lifecycle cleanupé˜¶æ®µ

**ä½ç½®**: åœ¨`run_stage_decision_shortlist`ä¹‹åæ·»åŠ æ–°æ–¹æ³•

**æ–°ä»£ç **:
```python
def run_stage_lifecycle_cleanup(self, orphan_age_days: int = 14, cluster_inactivity_days: int = 90) -> Dict[str, Any]:
    """é˜¶æ®µ9: ç”Ÿå‘½å‘¨æœŸæ¸…ç†

    Args:
        orphan_age_days: åˆ é™¤å¤šå°‘å¤©å‰çš„orphans (é»˜è®¤14å¤©)
        cluster_inactivity_days: å½’æ¡£å¤šå°‘å¤©æ— æ´»åŠ¨çš„clusters (é»˜è®¤90å¤©)
    """
    logger.info("=" * 50)
    logger.info("STAGE 9: Lifecycle Cleanup")
    logger.info("=" * 50)

    if self.enable_monitoring:
        performance_monitor.start_stage("lifecycle_cleanup")

    try:
        # å¯¼å…¥cleanupå‡½æ•°
        from scripts.lifecycle_cleanup import (
            mark_orphan_events,
            cleanup_old_orphans,
            get_lifecycle_statistics
        )

        # Step 1: æ ‡è®°orphans
        logger.info("Step 1: Marking orphan events...")
        marked_count = mark_orphan_events()
        logger.info(f"   Marked {marked_count} events as orphans")

        # Step 2: æ¸…ç†æ—§orphans
        logger.info(f"Step 2: Cleaning up orphans older than {orphan_age_days} days...")
        deleted_count = cleanup_old_orphans(
            db_path="data/wise_collection.db",
            orphan_age_days=orphan_age_days
        )
        logger.info(f"   Deleted {deleted_count} old orphans")

        # Step 3: è·å–ç»Ÿè®¡ä¿¡æ¯
        stats = get_lifecycle_statistics()

        result = {
            'orphans_marked': marked_count,
            'orphans_deleted': deleted_count,
            'final_stats': stats
        }

        self.stats["stages_completed"].append("lifecycle_cleanup")
        self.stats["stage_results"]["lifecycle_cleanup"] = result

        if self.enable_monitoring:
            performance_monitor.end_stage("lifecycle_cleanup", deleted_count)

        logger.info("âœ… Stage 9 completed:")
        logger.info(f"   Active events: {stats['active_events']}")
        logger.info(f"   Orphan events: {stats['orphan_events']}")
        logger.info(f"   Retention rate: {stats['retention_rate']:.1f}%")

        return result

    except Exception as e:
        logger.error(f"âŒ Stage 9 failed: {e}")
        self.stats["stages_failed"].append("lifecycle_cleanup")
        if self.enable_monitoring:
            performance_monitor.end_stage("lifecycle_cleanup", 0)
        raise
```

**ä¿®æ”¹`run_full_pipeline`æ–¹æ³•**:

åœ¨`stages`åˆ—è¡¨ä¸­æ·»åŠ æ–°é˜¶æ®µ (çº¦621-630è¡Œ):

```python
stages = [
    ("fetch", lambda: self.run_stage_fetch(limit_sources, fetch_sources)),
    ("filter", lambda: self.run_stage_filter(limit_posts, process_all)),
    ("extract", lambda: self.run_stage_extract(limit_posts, process_all)),
    ("embed", lambda: self.run_stage_embed(limit_events, process_all)),
    ("cluster", lambda: self.run_stage_cluster(limit_events, process_all)),
    ("map_opportunities", lambda: self.run_stage_map_opportunities(limit_clusters, process_all)),
    ("score", lambda: self.run_stage_score(limit_opportunities, process_all)),
    ("shortlist", lambda: self.run_stage_decision_shortlist()),
    ("lifecycle_cleanup", lambda: self.run_stage_lifecycle_cleanup())  # â† æ–°å¢
]
```

**ä¿®æ”¹`run_single_stage`æ–¹æ³•**:

åœ¨`stage_map`å­—å…¸ä¸­æ·»åŠ æ–°stage (çº¦655-670è¡Œ):

```python
stage_map = {
    "fetch": lambda: self.run_stage_fetch(kwargs.get("limit_sources"), kwargs.get("sources")),
    "filter": lambda: self.run_stage_filter(
        kwargs.get("limit_posts"),
        process_all
    ),
    "extract": lambda: self.run_stage_extract(
        kwargs.get("limit_posts"),
        process_all
    ),
    "embed": lambda: self.run_stage_embed(kwargs.get("limit_events"), process_all),
    "cluster": lambda: self.run_stage_cluster(kwargs.get("limit_events"), process_all),
    "map": lambda: self.run_stage_map_opportunities(kwargs.get("limit_clusters"), process_all),
    "score": lambda: self.run_stage_score(kwargs.get("limit_opportunities"), process_all),
    "shortlist": lambda: self.run_stage_decision_shortlist(),
    "lifecycle_cleanup": lambda: self.run_stage_lifecycle_cleanup()  # â† æ–°å¢
}
```

**ä¿®æ”¹`main`å‡½æ•°çš„argument parser** (çº¦916-917è¡Œ):

```python
parser.add_argument("--stage", choices=[
    "fetch", "filter", "extract", "embed", "cluster",
    "map", "score", "shortlist", "lifecycle_cleanup", "all"  # â† æ·»åŠ lifecycle_cleanup
], default="all", help="Which stage to run (default: all)")
```

---

#### 5.3 æ›´æ–°embed.py (å·²å®ŒæˆéªŒè¯)

**çŠ¶æ€**: âœ… å·²åœ¨Phase 2å®Œæˆ

**ç¡®è®¤äº‹é¡¹**:
- [x] `embed.py`å·²ä¿®æ”¹ä¸ºä½¿ç”¨Chromaå­˜å‚¨
- [x] `save_embedding()`æ–¹æ³•å·²æ›´æ–°
- [x] `process_missing_embeddings()`å·²æ›´æ–°ä¸ºæŸ¥è¯¢Chroma

**æ— éœ€é¢å¤–ä¿®æ”¹**

---

### éªŒæ”¶æ ‡å‡† Phase 5

#### åŠŸèƒ½éªŒæ”¶

- [ ] `python run_pipeline.py --stage cluster` æˆåŠŸè¿è¡Œ
- [ ] `python run_pipeline.py --stage lifecycle_cleanup` æˆåŠŸè¿è¡Œ
- [ ] `python run_pipeline.py --stage all` å®Œæ•´pipelineæˆåŠŸ
- [ ] æ–°pain_eventsè¢«æ­£ç¡®clustered
- [ ] Orphan eventsè¢«æ­£ç¡®æ ‡è®°å’Œæ¸…ç†

#### æ—¥å¿—éªŒè¯

è¿è¡Œpipelineåæ£€æŸ¥æ—¥å¿—è¾“å‡º:

```bash
# åº”è¯¥çœ‹åˆ°:
STAGE 5: Dynamic Clustering (Real-time)
Found X events to process
âœ… Stage 5 completed:
   Events processed: X
   New clusters: Y
   Updated clusters: Z

STAGE 9: Lifecycle Cleanup
Step 1: Marking orphan events...
   Marked X events as orphans
Step 2: Cleaning up orphans older than 14 days...
   Deleted Y old orphans
âœ… Stage 9 completed:
   Active events: X
   Orphan events: Y
   Retention rate: Z%
```

#### æ•°æ®åº“éªŒè¯

```sql
-- æ£€æŸ¥clustersè¢«æ›´æ–°
SELECT COUNT(*) FROM clusters;  -- æ•°é‡åº”è¯¥å¢åŠ æˆ–ä¿æŒ

-- æ£€æŸ¥lifecycleçŠ¶æ€æ­£ç¡®
SELECT
    COUNT(*) FILTER (WHERE lifecycle_stage = 'active') as active,
    COUNT(*) FILTER (WHERE lifecycle_stage = 'orphan') as orphan
FROM pain_events;

-- æ£€æŸ¥æœ€æ–°çš„eventsè¢«å¤„ç†
SELECT COUNT(*) FROM pain_events
WHERE extracted_at > datetime('now', '-1 day')
AND lifecycle_stage = 'active';
```

---

## Phase 6: æ€§èƒ½ä¼˜åŒ–

### ç›®æ ‡

é€šè¿‡å¹¶è¡ŒåŒ–å’Œå¢é‡å¤„ç†ï¼Œå°†pipelineæ€»è¿è¡Œæ—¶é—´æ§åˆ¶åœ¨2å°æ—¶ä»¥å†…ã€‚

### æ€§èƒ½åŸºå‡†

#### å½“å‰æ€§èƒ½ (Phase 4æµ‹è¯•)

| Stage | å½“å‰è€—æ—¶ | ç›®æ ‡è€—æ—¶ |
|-------|----------|----------|
| Fetch | 10 min | 10 min |
| Filter | 10 min | 10 min |
| Extract | 40 min | **8 min** âš¡ |
| Embed | 5 min | 5 min |
| Cluster | 20 min | **10 min** âš¡ |
| Map | 15 min | 15 min |
| Score | 15 min | 15 min |
| Decision | 5 min | 5 min |
| Cleanup | 1 min | 1 min |
| **æ€»è®¡** | **121 min** | **79 min** âœ… |

**ä¼˜åŒ–æ½œåŠ›**: Extractå’ŒClusteré˜¶æ®µæœ‰æœ€å¤§ä¼˜åŒ–ç©ºé—´

---

### ä»»åŠ¡æ¸…å•

#### 6.1 å¹¶è¡ŒåŒ–LLMè°ƒç”¨ (Extracté˜¶æ®µ)

**æ–‡ä»¶**: `pipeline/extract_pain.py`

**å½“å‰æ€§èƒ½ç“¶é¢ˆ**:
- ä¸²è¡Œå¤„ç†posts: æ¯ä¸ª~2ç§’
- 100 posts = 200ç§’ â‰ˆ 3.3åˆ†é’Ÿ

**ä¼˜åŒ–æ–¹æ¡ˆ**: ä½¿ç”¨`ThreadPoolExecutor`å¹¶è¡Œè°ƒç”¨

**å½“å‰ä»£ç ** (çº¦100-150è¡Œ):
```python
def extract_pain_from_posts_batch(
    self,
    posts: List[Dict[str, Any]],
    batch_size: int = 20
) -> int:
    """æ‰¹é‡ä»å¸–å­ä¸­æå–ç—›ç‚¹"""
    logger.info(f"Extracting pain from {len(posts)} posts")

    extracted_count = 0

    for i, post in enumerate(posts):
        if i % 10 == 0:
            logger.info(f"Processing {i}/{len(posts)} posts")

        # æå–å•ä¸ªå¸–å­
        pain_events = self.extract_pain_from_post(post)

        if pain_events:
            extracted_count += len(pain_events)

        # æ·»åŠ å»¶è¿Ÿé¿å…APIé™åˆ¶
        if i % batch_size == 0 and i > 0:
            time.sleep(1)

    return extracted_count
```

**ä¼˜åŒ–åä»£ç **:
```python
from concurrent.futures import ThreadPoolExecutor, as_completed
import time

def extract_pain_from_posts_batch(
    self,
    posts: List[Dict[str, Any]],
    batch_size: int = 20,
    max_workers: int = 5  # â† å¹¶å‘æ•°
) -> int:
    """æ‰¹é‡ä»å¸–å­ä¸­æå–ç—›ç‚¹ (å¹¶è¡ŒåŒ–ç‰ˆæœ¬)"""
    logger.info(f"Extracting pain from {len(posts)} posts (parallel, max_workers={max_workers})")

    extracted_count = 0
    failed_count = 0
    start_time = time.time()

    # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œå¤„ç†
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # æäº¤æ‰€æœ‰ä»»åŠ¡
        future_to_post = {
            executor.submit(self.extract_pain_from_post, post): post
            for post in posts
        }

        # æ”¶é›†ç»“æœ
        for i, future in enumerate(as_completed(future_to_post), 1):
            post = future_to_post[future]

            try:
                # è·å–ç»“æœ
                pain_events = future.result()

                if pain_events:
                    extracted_count += len(pain_events)

                # è¿›åº¦æ—¥å¿—
                if i % 10 == 0:
                    logger.info(f"Processed {i}/{len(posts)} posts, extracted: {extracted_count}")

            except Exception as e:
                logger.error(f"Failed to extract pain from post {post.get('id')}: {e}")
                failed_count += 1

    elapsed = time.time() - start_time
    logger.info(f"Extraction complete: {extracted_count} events from {len(posts)} posts")
    logger.info(f"Failed: {failed_count}, Time: {elapsed:.1f}s ({elapsed/len(posts):.1f}s per post)")

    return extracted_count
```

**æ³¨æ„äº‹é¡¹**:
1. **max_workersè®¾ç½®**: å»ºè®®ä»5å¼€å§‹æµ‹è¯•ï¼Œé€æ­¥å¢åŠ 
2. **APIé™æµ**: è§‚å¯ŸLLM APIçš„rate limité”™è¯¯
3. **å†…å­˜ä½¿ç”¨**: å¹¶å‘ä¼šå¢åŠ å†…å­˜å ç”¨
4. **é”™è¯¯å¤„ç†**: ç¡®ä¿å•ä¸ªå¤±è´¥ä¸å½±å“æ•´ä½“

**æµ‹è¯•æ–¹æ³•**:
```bash
# æµ‹è¯•ä¸åŒå¹¶å‘æ•°
for workers in 3 5 8 10; do
    echo "Testing with max_workers=$workers"
    time python run_pipeline.py --stage extract --limit-posts 100
done
```

**é¢„æœŸæ•ˆæœ**:
- ä¸²è¡Œ: 200ç§’ (100 posts Ã— 2s)
- å¹¶å‘(5 workers): 40ç§’ (200/5)
- **æå‡**: 5x

---

#### 6.2 å¢é‡å¤„ç† (æ‰€æœ‰Stage)

**ç›®æ ‡**: åªå¤„ç†è‡ªä¸Šæ¬¡è¿è¡Œä»¥æ¥çš„æ–°æ•°æ®ï¼Œè€Œä¸æ˜¯æ¯æ¬¡éƒ½å¤„ç†å…¨éƒ¨ã€‚

**å®ç°ç­–ç•¥**: ä½¿ç”¨æ—¶é—´æˆ³è¿‡æ»¤

##### 6.2.1 æ·»åŠ æœ€åè¿è¡Œæ—¶é—´è¿½è¸ª

**æ–‡ä»¶**: `utils/db.py` æˆ–æ–°å»º `utils/pipeline_state.py`

**æ–°å¢ä»£ç **:
```python
class PipelineState:
    """PipelineçŠ¶æ€è¿½è¸ª"""

    def __init__(self, db_path: str = "data/wise_collection.db"):
        self.db_path = db_path
        self._init_state_table()

    def _init_state_table(self):
        """åˆå§‹åŒ–stateè¡¨"""
        import sqlite3
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        cursor.execute("""
            CREATE TABLE IF NOT EXISTS pipeline_state (
                stage TEXT PRIMARY KEY,
                last_run_at TIMESTAMP,
                last_processed_count INTEGER,
                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        """)

        conn.commit()
        conn.close()

    def get_last_run_time(self, stage: str) -> Optional[str]:
        """è·å–æŸstageæœ€åè¿è¡Œæ—¶é—´"""
        import sqlite3
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        cursor.execute("""
            SELECT last_run_at FROM pipeline_state
            WHERE stage = ?
        """, (stage,))

        row = cursor.fetchone()
        conn.close()

        return row[0] if row else None

    def update_stage_run(
        self,
        stage: str,
        processed_count: int = 0
    ):
        """æ›´æ–°stageè¿è¡Œæ—¶é—´"""
        import sqlite3
        from datetime import datetime
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        now = datetime.now().isoformat()

        cursor.execute("""
            INSERT OR REPLACE INTO pipeline_state (stage, last_run_at, last_processed_count, updated_at)
            VALUES (?, ?, ?, ?)
        """, (stage, now, processed_count, now))

        conn.commit()
        conn.close()


# å•ä¾‹å®ä¾‹
_pipeline_state = None

def get_pipeline_state() -> PipelineState:
    global _pipeline_state
    if _pipeline_state is None:
        _pipeline_state = PipelineState()
    return _pipeline_state
```

##### 6.2.2 æ›´æ–°Filteré˜¶æ®µ

**æ–‡ä»¶**: `run_pipeline.py`

**ä¿®æ”¹**:
```python
def run_stage_filter(self, limit_posts: Optional[int] = None, process_all: bool = False):
    """é˜¶æ®µ2: ä¿¡å·è¿‡æ»¤ (Posts) - å¢é‡å¤„ç†ç‰ˆæœ¬"""
    logger.info("=" * 50)
    logger.info("STAGE 2: Filtering pain signals (Incremental)")
    logger.info("=" * 50)

    if self.enable_monitoring:
        performance_monitor.start_stage("filter")

    try:
        from utils.pipeline_state import get_pipeline_state

        # è·å–ä¸Šæ¬¡è¿è¡Œæ—¶é—´
        last_run = get_pipeline_state().get_last_run_time("filter")
        if last_run:
            logger.info(f"Last run: {last_run}")
            logger.info("Processing posts collected since then...")

        filter = PainSignalFilter()

        # è·å–æœªè¿‡æ»¤çš„å¸–å­ (å¢é‡)
        if process_all and limit_posts is None:
            limit_posts = 1000000
        elif limit_posts is None:
            limit_posts = 1000

        unfiltered_posts = db.get_unprocessed_posts(
            limit=limit_posts,
            since=last_run  # â† æ–°å¢å‚æ•°ï¼šåªè·å–æ–°æ•°æ®
        )

        # ... å¤„ç†é€»è¾‘ (ä¿æŒä¸å˜)

        # æ›´æ–°state
        saved_count = post_result['filtered']
        get_pipeline_state().update_stage_run("filter", saved_count)

        # ... å…¶ä½™ä»£ç ä¿æŒä¸å˜
```

**ä¿®æ”¹** `utils/db.py`:
```python
def get_unprocessed_posts(
    self,
    limit: int = 1000,
    since: Optional[str] = None  # â† æ–°å¢å‚æ•°
) -> List[Dict]:
    """è·å–æœªå¤„ç†çš„å¸–å­

    Args:
        limit: é™åˆ¶æ•°é‡
        since: ISOæ ¼å¼æ—¶é—´æˆ³ï¼Œåªè·å–æ­¤æ—¶é—´ä¹‹åçš„posts
    """
    try:
        with self.get_connection("raw") as conn:
            if since:
                # å¢é‡æ¨¡å¼
                cursor = conn.execute("""
                    SELECT * FROM posts
                    WHERE collected_at > ?
                    AND id NOT IN (SELECT id FROM filtered_posts)
                    ORDER BY collected_at DESC
                    LIMIT ?
                """, (since, limit))
            else:
                # å…¨é‡æ¨¡å¼
                cursor = conn.execute("""
                    SELECT * FROM posts
                    WHERE id NOT IN (SELECT id FROM filtered_posts)
                    ORDER BY collected_at DESC
                    LIMIT ?
                """, (limit,))

            return [dict(row) for row in cursor.fetchall()]

    except Exception as e:
        logger.error(f"Failed to get unprocessed posts: {e}")
        return []
```

##### 6.2.3 æ›´æ–°å…¶ä»–Stage

**Applyç›¸åŒæ¨¡å¼åˆ°**:
- `run_stage_extract`: åªå¤„ç†è‡ªlast_runä»¥æ¥çš„filtered_posts
- `run_stage_embed`: åªå¤„ç†è‡ªlast_runä»¥æ¥çš„pain_events
- `run_stage_cluster`: åªå¤„ç†è‡ªlast_runä»¥æ¥çš„unclustered_events

**ç¤ºä¾‹ (Extracté˜¶æ®µ)**:
```python
def run_stage_extract(self, limit_posts: Optional[int] = None, process_all: bool = False):
    """é˜¶æ®µ3: ç—›ç‚¹æŠ½å– - å¢é‡å¤„ç†ç‰ˆæœ¬"""
    # ...

    last_run = get_pipeline_state().get_last_run_time("extract")

    # è·å–æœªæå–çš„posts
    unextracted_posts = db.get_unextracted_posts(
        limit=limit_posts,
        since=last_run  # â† æ–°å¢
    )

    # ... å¤„ç†é€»è¾‘

    # æ›´æ–°state
    get_pipeline_state().update_stage_run("extract", post_result['pain_events_saved'])
```

---

### éªŒæ”¶æ ‡å‡† Phase 6

#### æ€§èƒ½éªŒæ”¶

è¿è¡Œå®Œæ•´pipelineå¹¶è®¡æ—¶:

```bash
# è®°å½•å¼€å§‹æ—¶é—´
start_time=$(date +%s)

# è¿è¡Œpipeline (å…¨é‡å¤„ç†ç¬¬ä¸€å¤©ï¼Œå¢é‡å¤„ç†åç»­)
python run_pipeline.py --stage all --process-all

# è®¡ç®—è€—æ—¶
end_time=$(date +%s)
duration=$((end_time - start_time))
minutes=$((duration / 60))

echo "Pipeline completed in ${minutes} minutes"
```

**é€šè¿‡æ ‡å‡†**:
- âœ… **é¦–æ¬¡è¿è¡Œ** (process-all): < 150åˆ†é’Ÿ
- âœ… **åç»­è¿è¡Œ** (å¢é‡): < 90åˆ†é’Ÿ
- âœ… **Extracté˜¶æ®µ**: < 10åˆ†é’Ÿ
- âœ… **Clusteré˜¶æ®µ**: < 15åˆ†é’Ÿ

#### å¹¶å‘æµ‹è¯•

```bash
# æµ‹è¯•ä¸åŒå¹¶å‘é…ç½®
for workers in 3 5 8; do
    echo "=== Testing max_workers=$workers ==="
    time python run_pipeline.py --stage extract --limit-posts 100
    echo ""
done
```

é€‰æ‹©æ€§èƒ½æœ€å¥½ä¸”ç¨³å®šçš„å¹¶å‘æ•°ã€‚

#### å†…å­˜ç›‘æ§

```bash
# ç›‘æ§å†…å­˜ä½¿ç”¨
/usr/bin/time -v python run_pipeline.py --stage all --process-all

# æŸ¥çœ‹Maximum resident set size
# åº”è¯¥ < 4GB
```

---

## æµ‹è¯•æŒ‡å—

### å•å…ƒæµ‹è¯•

#### Test 1: StageåŠŸèƒ½æµ‹è¯•

```bash
# æµ‹è¯•å•ä¸ªstage
python run_pipeline.py --stage fetch --limit-sources 5
python run_pipeline.py --stage filter --limit-posts 10
python run_pipeline.py --stage extract --limit-posts 10
python run_pipeline.py --stage embed --limit-events 10
python run_pipeline.py --stage cluster
python run_pipeline.py --stage map --limit-clusters 5
python run_pipeline.py --stage score --limit-opportunities 10
python run_pipeline.py --stage shortlist
python run_pipeline.py --stage lifecycle_cleanup
```

#### Test 2: å¢é‡å¤„ç†æµ‹è¯•

```bash
# ç¬¬ä¸€æ¬¡è¿è¡Œ (å…¨é‡)
python run_pipeline.py --stage all --process-all

# ç¬¬äºŒæ¬¡è¿è¡Œ (å¢é‡ï¼Œåº”è¯¥å¾ˆå¿«)
python run_pipeline.py --stage all

# æ£€æŸ¥æ˜¯å¦è·³è¿‡å·²å¤„ç†æ•°æ®
# æ—¥å¿—åº”è¯¥æ˜¾ç¤º "Processing posts collected since [last_run]"
```

#### Test 3: å¹¶å‘æ€§èƒ½æµ‹è¯•

```bash
# æµ‹è¯•ä¸åŒå¹¶å‘æ•°
for workers in 3 5 8 10; do
    echo "=== max_workers=$workers ==="
    time python run_pipeline.py --stage extract --limit-posts 50
done
```

### é›†æˆæµ‹è¯•

#### Test 4: å®Œæ•´Pipelineæµ‹è¯•

```bash
# å®Œæ•´è¿è¡Œ (è®°å½•æ—¶é—´)
time python run_pipeline.py --stage all --process-all

# æ£€æŸ¥ç»“æœ
sqlite3 data/wise_collection.db <<EOF
SELECT
    (SELECT COUNT(*) FROM posts) as raw_posts,
    (SELECT COUNT(*) FROM filtered_posts) as filtered,
    (SELECT COUNT(*) FROM pain_events) as pain_events,
    (SELECT COUNT(*) FROM clusters) as clusters,
    (SELECT COUNT(*) FROM opportunities) as opportunities;
EOF
```

#### Test 5: æ•°æ®ä¸€è‡´æ€§éªŒè¯

```bash
# è¿è¡Œæµ‹è¯•å¥—ä»¶
python tests/test_01_chroma_client.py
python tests/test_02_chroma_similarity.py
python tests/test_05_data_consistency.py

# åº”è¯¥å…¨éƒ¨é€šè¿‡
```

### æ€§èƒ½æµ‹è¯•

#### Test 6: 2å°æ—¶ç›®æ ‡éªŒè¯

```bash
# ç›‘æ§èµ„æºä½¿ç”¨
/usr/bin/time -v python run_pipeline.py --stage all --process-all

# æ£€æŸ¥è¾“å‡º:
# - Elapsed (wall clock) time: åº”è¯¥ < 7200 seconds (2å°æ—¶)
# - Maximum resident set size: åº”è¯¥ < 4GB
# - Percent of CPU this job got: åº”è¯¥ > 400% (å¤šæ ¸åˆ©ç”¨)
```

---

## æ³¨æ„äº‹é¡¹

### 1. LLM APIé™æµ

**é—®é¢˜**: å¹¶å‘è°ƒç”¨å¯èƒ½è§¦å‘rate limit

**è§£å†³æ–¹æ¡ˆ**:
```python
# åœ¨extract_pain.pyä¸­æ·»åŠ é‡è¯•é€»è¾‘
import time
from functools import wraps

def retry_on_rate_limit(max_retries=3, delay=5):
    """é‡è¯•è£…é¥°å™¨"""
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            for attempt in range(max_retries):
                try:
                    return func(*args, **kwargs)
                except RateLimitError as e:
                    if attempt < max_retries - 1:
                        wait_time = delay * (2 ** attempt)  # æŒ‡æ•°é€€é¿
                        logger.warning(f"Rate limit hit, waiting {wait_time}s...")
                        time.sleep(wait_time)
                    else:
                        raise
            return wrapper
        return decorator

# ä½¿ç”¨
@retry_on_rate_limit(max_retries=3, delay=5)
def extract_pain_from_post(self, post):
    # ... original code
```

### 2. å†…å­˜ç®¡ç†

**é—®é¢˜**: å¹¶å‘å¤„ç†å¯èƒ½å¯¼è‡´å†…å­˜æº¢å‡º

**è§£å†³æ–¹æ¡ˆ**:
```python
# æ‰¹é‡å¤„ç†ï¼Œé¿å…ä¸€æ¬¡æ€§åŠ è½½å¤ªå¤šæ•°æ®
def process_in_batches(items, batch_size=50):
    """åˆ†æ‰¹å¤„ç†"""
    for i in range(0, len(items), batch_size):
        batch = items[i:i+batch_size]
        yield batch

# ä½¿ç”¨
for batch in process_in_batches(all_posts, batch_size=50):
    results = extract_pain_from_posts_batch(batch, max_workers=5)
    # é‡Šæ”¾å†…å­˜
    del batch
    import gc
    gc.collect()
```

### 3. æ•°æ®åº“è¿æ¥ç®¡ç†

**é—®é¢˜**: å¹¶å‘ç¯å¢ƒä¸‹å¯èƒ½å¯¼è‡´è¿æ¥æ³„æ¼

**è§£å†³æ–¹æ¡ˆ**:
```python
# ç¡®ä¿ä½¿ç”¨context manager
with db.get_connection("pain") as conn:
    # æ“ä½œ
    cursor = conn.execute(...)
    results = cursor.fetchall()
# è¿æ¥è‡ªåŠ¨å…³é—­
```

### 4. Chromaå¹¶å‘å†™å…¥

**é—®é¢˜**: å¤šçº¿ç¨‹åŒæ—¶å†™å…¥Chromaå¯èƒ½å†²çª

**è§£å†³æ–¹æ¡ˆ**:
```python
# åœ¨embed.pyä¸­æ·»åŠ çº¿ç¨‹é”
from threading import Lock

class PainEventEmbedder:
    def __init__(self):
        self.chroma_lock = Lock()  # â† ä¿æŠ¤Chromaå†™å…¥

    def save_embedding(self, pain_event_id, embedding, pain_event_data=None):
        with self.chroma_lock:  # â† åŠ é”
            chroma.add_embeddings(...)
```

### 5. é”™è¯¯æ¢å¤

**é—®é¢˜**: Pipelineä¸­æ–­åå¦‚ä½•æ¢å¤ï¼Ÿ

**è§£å†³æ–¹æ¡ˆ**:
- âœ… å¢é‡å¤„ç†è‡ªåŠ¨æ”¯æŒæ¢å¤
- âœ… ä½¿ç”¨`--stage`å‚æ•°å•ç‹¬è¿è¡Œå¤±è´¥çš„stage
- âœ… æ£€æŸ¥`pipeline_state`è¡¨ç¡®å®šæœ€åæˆåŠŸç‚¹

```bash
# æŸ¥çœ‹å„stageæœ€åè¿è¡Œæ—¶é—´
sqlite3 data/wise_collection.db "SELECT * FROM pipeline_state"

# ä»å¤±è´¥ç‚¹ç»§ç»­
python run_pipeline.py --stage <failed_stage>
```

---

## å›æ»šè®¡åˆ’

### å¦‚æœPhase 5å¤±è´¥

**ç—‡çŠ¶**: æ–°pipelineæ— æ³•æ­£å¸¸è¿è¡Œ

**å›æ»šæ­¥éª¤**:
```bash
# 1. åˆ‡æ¢å›æ—§ä»£ç 
git checkout main
git pull origin main

# 2. æ¢å¤æ•°æ®åº“ (å¦‚æœschemaè¢«ä¿®æ”¹)
cp data/wise_collection.db.backup data/wise_collection.db

# 3. åˆ é™¤Chromaæ•°æ® (å¯é€‰)
rm -rf data/chroma_db/

# 4. è¿è¡Œæ—§pipelineéªŒè¯
python run_pipeline.py --stage all
```

### å¦‚æœPhase 6æ€§èƒ½ä¸è¾¾æ ‡

**ç—‡çŠ¶**: Pipelineè¿è¡Œæ—¶é—´è¶…è¿‡2å°æ—¶

**å›é€€æ­¥éª¤**:
```bash
# 1. å‡å°‘å¹¶å‘æ•°
# ä¿®æ”¹ extract_pain.py: max_workers = 3

# 2. ç¦ç”¨å¢é‡å¤„ç† (ä½¿ç”¨å…¨é‡)
python run_pipeline.py --stage all --process-all

# 3. åˆ†é˜¶æ®µè¿è¡Œ
python run_pipeline.py --stage fetch
python run_pipeline.py --stage filter
python run_pipeline.py --stage extract
python run_pipeline.py --stage embed
python run_pipeline.py --stage cluster
# ... etc
```

### æ•°æ®å›æ»š

**å¦‚æœéœ€è¦å›æ»šæ•°æ®ä¿®æ”¹**:

```bash
# 1. åœæ­¢pipeline
pkill -f run_pipeline.py

# 2. å¤‡ä»½å½“å‰æ•°æ®åº“
cp data/wise_collection.db data/wise_collection.db.before_rollback

# 3. æ¢å¤åˆ°ä¹‹å‰ç‰ˆæœ¬
cp data/wise_collection.db.backup_YYYYMMDD data/wise_collection.db

# 4. æ¢å¤Chroma (å¦‚æœ‰å¤‡ä»½)
tar -xzf chroma_backup_YYYYMMDD.tar.gz -C data/
```

---

## é™„å½•

### A. ç¯å¢ƒè¦æ±‚

```
Python: >= 3.10
Dependencies:
  - chromadb >= 0.4.0
  - openai (æˆ–å…¼å®¹çš„LLM API)
  - sqlite3 (built-in)
```

### B. é…ç½®æ–‡ä»¶

æ£€æŸ¥ä»¥ä¸‹é…ç½®æ–‡ä»¶å­˜åœ¨ä¸”æ­£ç¡®:

- `config/llm.yaml`: LLM APIé…ç½®
- `config/thresholds.yaml`: èšç±»é˜ˆå€¼é…ç½®

### C. æ•…éšœæ’æŸ¥

| é—®é¢˜ | å¯èƒ½åŸå›  | è§£å†³æ–¹æ¡ˆ |
|------|----------|----------|
| ImportError: No module named 'chromadb' | chromadbæœªå®‰è£… | `pip install chromadb` |
| "Collection expecting dimension..." | Embeddingç»´åº¦ä¸åŒ¹é… | æ£€æŸ¥embedding_modelé…ç½® |
| Database is locked | å¹¶å‘å†™å…¥å†²çª | ä½¿ç”¨WALæ¨¡å¼: `PRAGMA journal_mode=WAL` |
| Rate limit exceeded | LLM APIè°ƒç”¨è¿‡å¿« | å‡å°‘max_workers |
| Out of memory | å¹¶å‘æ•°å¤ªé«˜ | å‡å°‘batch_sizeæˆ–max_workers |

### D. ç›‘æ§å‘½ä»¤

```bash
# å®æ—¶æŸ¥çœ‹æ—¥å¿—
tail -f logs/pipeline.log

# ç›‘æ§æ•°æ®åº“å¤§å°
watch -n 60 'du -sh data/wise_collection.db data/chroma_db/'

# ç›‘æ§è¿›ç¨‹
ps aux | grep run_pipeline

# ç³»ç»Ÿèµ„æº
htop
```

---

## æ€»ç»“

### Phase 5 æ ¸å¿ƒä»»åŠ¡
1. âœ… æ›´æ–°`run_pipeline.py`çš„`run_stage_cluster`æ–¹æ³•
2. âœ… æ·»åŠ `run_stage_lifecycle_cleanup`æ–¹æ³•
3. âœ… æ›´æ–°`run_full_pipeline`å’Œ`run_single_stage`
4. âœ… æ›´æ–°argument parser

### Phase 6 æ ¸å¿ƒä»»åŠ¡
1. âœ… å¹¶è¡ŒåŒ–LLMè°ƒç”¨ (extract_pain.py)
2. âœ… æ·»åŠ PipelineStateè¿½è¸ª
3. âœ… å®ç°å¢é‡å¤„ç† (æ‰€æœ‰stage)
4. âœ… æ€§èƒ½æµ‹è¯•å’Œè°ƒä¼˜

### äº¤ä»˜ç‰©
- [ ] æ›´æ–°çš„`run_pipeline.py`
- [ ] æ›´æ–°çš„`extract_pain.py`
- [ ] æ–°å¢çš„`utils/pipeline_state.py`
- [ ] æ›´æ–°çš„`utils/db.py` (æ–°å¢sinceå‚æ•°)
- [ ] æµ‹è¯•æŠ¥å‘Š
- [ ] æ€§èƒ½å¯¹æ¯”æŠ¥å‘Š

---

**æ–‡æ¡£ä½œè€…**: Claude Sonnet 4.5
**æœ€åæ›´æ–°**: 2026-01-13
**ç‰ˆæœ¬**: v1.0
