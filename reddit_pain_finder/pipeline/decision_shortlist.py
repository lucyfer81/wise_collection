# pipeline/decision_shortlist.py
"""
Decision Shortlist Generator
ä»æ‰€æœ‰è¯„åˆ†æœºä¼šä¸­ç­›é€‰å‡º Top 3-5 ä¸ªæœ€å€¼å¾—æ‰§è¡Œçš„äº§å“æœºä¼š
"""
import json
import logging
import math
import os
from datetime import datetime
from typing import Dict, Any, List, Optional

import yaml

from utils.llm_client import llm_client
from utils.db import db

logger = logging.getLogger(__name__)


class DecisionShortlistGenerator:
    """å†³ç­–æ¸…å•ç”Ÿæˆå™¨"""

    def __init__(self, config_path: str = "config/thresholds.yaml"):
        """åˆå§‹åŒ–ç”Ÿæˆå™¨"""
        self.config = self._load_config(config_path)
        self.pipeline_run_id = f"pipeline_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        logger.info("DecisionShortlistGenerator initialized")

    def _load_config(self, config_path: str) -> Dict[str, Any]:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
            return config.get('decision_shortlist', {})
        except Exception as e:
            logger.error(f"Failed to load config: {e}")
            return self._get_default_config()

    def _get_default_config(self) -> Dict[str, Any]:
        """è¿”å›é»˜è®¤é…ç½®"""
        return {
            'min_viability_score': 7.0,
            'min_cluster_size': 6,
            'min_trust_level': 0.7,
            'ignored_clusters': [],
            'final_score_weights': {
                'viability_score': 1.0,
                'cluster_size_log_factor': 2.5,
                'trust_level': 1.5,
                'cross_source_bonus': 5.0
            },
            'output': {
                'min_candidates': 3,
                'max_candidates': 5,
                'markdown_dir': 'reports',
                'json_dir': 'data'
            }
        }

    def _apply_hard_filters(self) -> List[Dict[str, Any]]:
        """åº”ç”¨ç¡¬æ€§è¿‡æ»¤è§„åˆ™

        Returns:
            é€šè¿‡æ‰€æœ‰è¿‡æ»¤çš„æœºä¼šåˆ—è¡¨
        """
        config = self.config

        min_viability = config['min_viability_score']
        min_cluster_size = config['min_cluster_size']
        min_trust = config['min_trust_level']
        ignored_clusters = set(config.get('ignored_clusters', []))

        logger.info(f"Applying hard filters: viability>={min_viability}, "
                    f"cluster_size>={min_cluster_size}, trust>={min_trust}")

        try:
            with db.get_connection("clusters") as conn:
                cursor = conn.execute("""
                    SELECT
                        o.id as opportunity_id,
                        o.opportunity_name,
                        o.description,
                        o.raw_total_score as viability_score,
                        o.trust_level as trust_level,
                        o.target_users,
                        o.missing_capability,
                        o.why_existing_fail,
                        c.id as cluster_id,
                        c.cluster_name,
                        c.cluster_size,
                        c.source_type,
                        c.pain_event_ids,
                        c.centroid_summary as cluster_summary
                    FROM opportunities o
                    JOIN clusters c ON o.cluster_id = c.id
                    WHERE o.raw_total_score >= ?
                      AND c.cluster_size >= ?
                      AND o.trust_level >= ?
                      AND c.cluster_name NOT IN (
                        SELECT value FROM json_each(?)
                        WHERE json_valid(?) AND json_each.value IS NOT NULL
                      )
                    ORDER BY o.raw_total_score DESC
                """, (min_viability, min_cluster_size, min_trust,
                      json.dumps(list(ignored_clusters)),
                      json.dumps(list(ignored_clusters))))

                opportunities = [dict(row) for row in cursor.fetchall()]

                # è§£æ pain_event_ids JSON
                for opp in opportunities:
                    if opp.get('pain_event_ids'):
                        try:
                            opp['pain_event_ids'] = json.loads(opp['pain_event_ids'])
                        except:
                            opp['pain_event_ids'] = []

                logger.info(f"Hard filters: {len(opportunities)} opportunities passed")
                return opportunities

        except Exception as e:
            logger.error(f"Failed to apply hard filters: {e}")
            return []

    def _check_cross_source_validation(self, opportunity: Dict) -> Dict[str, Any]:
        """æ£€æŸ¥è·¨æºéªŒè¯ï¼Œè¿”å›éªŒè¯ä¿¡æ¯å’ŒåŠ åˆ†

        ä¸‰å±‚ä¼˜å…ˆçº§ï¼š
        - Level 1 (å¼ºä¿¡å·): source_type='aligned' æˆ–åœ¨ aligned_problems è¡¨ä¸­
        - Level 2 (ä¸­ç­‰ä¿¡å·): cluster_size >= 10 AND è·¨ >=3 subreddits
        - Level 3 (å¼±ä¿¡å·): cluster_size >= 8 AND è·¨ >=2 subreddits
        """
        cluster = opportunity

        # Level 1: æ£€æŸ¥ source_type
        if cluster.get('source_type') == 'aligned':
            return {
                "has_cross_source": True,
                "validation_level": 1,
                "boost_score": 2.0,
                "validated_problem": True,
                "evidence": "source_type='aligned'"
            }

        # Level 1: æ£€æŸ¥ aligned_problems è¡¨
        aligned_problem = self._check_aligned_problems_table(cluster['cluster_name'])
        if aligned_problem:
            return {
                "has_cross_source": True,
                "validation_level": 1,
                "boost_score": 2.0,
                "validated_problem": True,
                "evidence": f"Found in aligned_problems: {aligned_problem['aligned_problem_id']}"
            }

        # Level 2 & 3: æ£€æŸ¥ cluster_size + è·¨ subreddit
        pain_event_ids = cluster.get('pain_event_ids', [])
        if not pain_event_ids:
            return {
                "has_cross_source": False,
                "validation_level": 0,
                "boost_score": 0.0,
                "validated_problem": False,
                "evidence": "No pain events"
            }

        subreddit_count = self._count_subreddits(pain_event_ids)
        cluster_size = cluster['cluster_size']

        # Level 2
        if cluster_size >= 10 and subreddit_count >= 3:
            return {
                "has_cross_source": True,
                "validation_level": 2,
                "boost_score": 1.0,
                "validated_problem": True,
                "evidence": f"Large cluster ({cluster_size}) across {subreddit_count} subreddits"
            }

        # Level 3
        if cluster_size >= 8 and subreddit_count >= 2:
            return {
                "has_cross_source": True,
                "validation_level": 3,
                "boost_score": 0.5,
                "validated_problem": False,
                "evidence": f"Medium cluster ({cluster_size}) across {subreddit_count} subreddits"
            }

        # æ— è·¨æºéªŒè¯
        return {
            "has_cross_source": False,
            "validation_level": 0,
            "boost_score": 0.0,
            "validated_problem": False,
            "evidence": "No cross-source validation"
        }

    def _check_aligned_problems_table(self, cluster_name: str) -> Optional[Dict]:
        """æ£€æŸ¥ cluster æ˜¯å¦åœ¨ aligned_problems è¡¨ä¸­"""
        try:
            with db.get_connection("clusters") as conn:
                cursor = conn.execute("""
                    SELECT aligned_problem_id, sources, alignment_score
                    FROM aligned_problems
                    WHERE cluster_ids LIKE ?
                """, (f'%{cluster_name}%',))
                result = cursor.fetchone()
                return dict(result) if result else None
        except Exception as e:
            logger.error(f"Failed to check aligned_problems: {e}")
            return None

    def _count_subreddits(self, pain_event_ids: List[int]) -> int:
        """è®¡ç®—æ¶‰åŠçš„ä¸åŒ subreddit æ•°é‡"""
        try:
            with db.get_connection("pain") as conn:
                placeholders = ','.join('?' for _ in pain_event_ids)
                cursor = conn.execute(f"""
                    SELECT COUNT(DISTINCT fp.subreddit) as count
                    FROM pain_events pe
                    JOIN filtered_posts fp ON pe.post_id = fp.id
                    WHERE pe.id IN ({placeholders})
                """, pain_event_ids)
                return cursor.fetchone()['count']
        except Exception as e:
            logger.error(f"Failed to count subreddits: {e}")
            return 1  # é»˜è®¤ä¸º 1ï¼Œé¿å… 0

    def _calculate_final_score(self, opportunity: Dict, cross_source_info: Dict) -> float:
        """è®¡ç®—æœ€ç»ˆè¯„åˆ†ï¼ˆä½¿ç”¨å¯¹æ•°å°ºåº¦ï¼‰

        Args:
            opportunity: æœºä¼šå­—å…¸ï¼ŒåŒ…å« viability_score, cluster_size, trust_level
            cross_source_info: è·¨æºéªŒè¯ä¿¡æ¯

        Returns:
            æœ€ç»ˆè¯„åˆ† (0-10)
        """
        weights = self.config['final_score_weights']

        viability_score = opportunity['viability_score']
        trust_level = opportunity['trust_level']
        cluster_size = opportunity['cluster_size']

        # ä½¿ç”¨å¯¹æ•°å°ºåº¦ï¼Œé¿å…å¤§clusterä¸»å¯¼è¯„åˆ†
        cluster_size_log = math.log10(max(cluster_size, 1))

        # è®¡ç®—åŸºç¡€åˆ†æ•°
        final_score = (
            viability_score * weights['viability_score'] +
            cluster_size_log * weights['cluster_size_log_factor'] +
            trust_level * weights['trust_level']
        )

        # å¦‚æœæœ‰è·¨æºéªŒè¯ï¼ŒåŠ åˆ†
        if cross_source_info['has_cross_source']:
            boost = cross_source_info['boost_score']
            final_score += weights['cross_source_bonus'] * boost * 0.1

        # é™åˆ¶åœ¨ 0-10 èŒƒå›´å†…
        return min(max(final_score, 0), 10.0)

    def _get_default_prompt(self) -> str:
        """è·å–é»˜è®¤çš„ LLM prompt"""
        return """ä½ æ˜¯ä¸€ä¸ªäº§å“ç»ç†ä¸“å®¶ã€‚è¯·åŸºäºä»¥ä¸‹ç—›ç‚¹èšç±»å’Œæœºä¼šä¿¡æ¯ï¼Œç”Ÿæˆç®€æ´æ˜äº†çš„äº§å“æè¿°ï¼š

**æœºä¼šåç§°**: {opportunity_name}

**é—®é¢˜æè¿°**:
{cluster_summary}

**ç›®æ ‡ç”¨æˆ·**: {target_users}

**ç¼ºå¤±èƒ½åŠ›**: {missing_capability}

**ç°æœ‰æ–¹æ¡ˆä¸è¶³**: {why_existing_fail}

è¯·ä»¥ JSON æ ¼å¼è¿”å›ä»¥ä¸‹å­—æ®µï¼ˆä¸è¦åŒ…å« markdown æ ‡è®°ï¼‰ï¼š
{{
  "problem": "ç”¨1-2å¥è¯æ¸…æ™°æè¿°æ ¸å¿ƒç—›ç‚¹é—®é¢˜",
  "mvp": "æè¿°æœ€å°å¯è¡Œäº§å“çš„æ ¸å¿ƒåŠŸèƒ½å’Œè§£å†³æ–¹æ¡ˆ",
  "why_now": "è§£é‡Šä¸ºä»€ä¹ˆç°åœ¨æ˜¯åˆ‡å…¥è¿™ä¸ªå¸‚åœºçš„æœ€ä½³æ—¶æœºï¼ˆæŠ€æœ¯æˆç†Ÿåº¦ã€å¸‚åœºå˜åŒ–ã€ç”¨æˆ·éœ€æ±‚ç­‰ï¼‰"
}}

è¦æ±‚ï¼š
1. é—®é¢˜æè¿°è¦å…·ä½“ä¸”å‡»ä¸­ç”¨æˆ·ç—›ç‚¹
2. MVP è¦ç®€æ´å¯è¡Œï¼Œé€‚åˆ solo developer
3. Why Now è¦æœ‰è¯´æœåŠ›ï¼Œä½“ç°å¸‚åœºæœºä¼š
4. æ¯ä¸ªå­—æ®µæ§åˆ¶åœ¨50å­—ä»¥å†…
5. åªè¿”å› JSONï¼Œä¸è¦æœ‰å…¶ä»–å†…å®¹
"""

    def _generate_readable_content(self, opportunity: Dict, cluster: Dict, cross_source_info: Dict) -> Dict[str, str]:
        """ç”Ÿæˆå¯è¯»æ€§å†…å®¹ï¼ˆProblem, MVP, Why Nowï¼‰

        Args:
            opportunity: æœºä¼šä¿¡æ¯
            cluster: èšç±»ä¿¡æ¯
            cross_source_info: è·¨æºéªŒè¯ä¿¡æ¯

        Returns:
            åŒ…å« problem, mvp, why_now çš„å­—å…¸
        """
        try:
            prompt = self._get_default_prompt().format(
                opportunity_name=opportunity.get('opportunity_name', ''),
                cluster_summary=cluster.get('cluster_summary', opportunity.get('description', '')),
                target_users=opportunity.get('target_users', 'Unknown'),
                missing_capability=opportunity.get('missing_capability', 'Unknown'),
                why_existing_fail=opportunity.get('why_existing_fail', 'Unknown')
            )

            # è°ƒç”¨ LLM
            response = llm_client.generate(
                prompt=prompt,
                model="gpt-4o-mini",  # ä½¿ç”¨æ›´ç»æµçš„æ¨¡å‹
                temperature=0.7,
                max_tokens=500
            )

            # è§£æ JSON å“åº”
            import json
            import re

            # å°è¯•æå– JSONï¼ˆå»é™¤å¯èƒ½çš„ markdown ä»£ç å—æ ‡è®°ï¼‰
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                json_str = json_match.group(0)
                content = json.loads(json_str)

                # éªŒè¯å¿…éœ€å­—æ®µ
                required_fields = ['problem', 'mvp', 'why_now']
                if all(field in content for field in required_fields):
                    logger.info(f"âœ… LLM content generated for {opportunity['opportunity_name']}")
                    return {
                        'problem': content['problem'],
                        'mvp': content['mvp'],
                        'why_now': content['why_now']
                    }

            # å¦‚æœè§£æå¤±è´¥ï¼Œä½¿ç”¨ fallback
            logger.warning(f"Failed to parse LLM response, using fallback")
            return self._fallback_readable_content(opportunity, cluster)

        except Exception as e:
            logger.error(f"Error generating readable content: {e}")
            return self._fallback_readable_content(opportunity, cluster)

    def _fallback_readable_content(self, opportunity: Dict, cluster: Dict) -> Dict[str, str]:
        """ç”Ÿæˆå¯è¯»æ€§å†…å®¹çš„ fallback æ–¹æ¡ˆ

        Args:
            opportunity: æœºä¼šä¿¡æ¯
            cluster: èšç±»ä¿¡æ¯

        Returns:
            åŒ…å« problem, mvp, why_now çš„å­—å…¸
        """
        cluster_name = cluster.get('cluster_name', 'Unknown')
        description = opportunity.get('description', '')
        target_users = opportunity.get('target_users', 'users')
        missing_capability = opportunity.get('missing_capability', 'capability')

        return {
            'problem': f"Users in {cluster_name} are struggling with {description[:100]}",
            'mvp': f"Build a tool that provides {missing_capability} for {target_users}",
            'why_now': f"High demand from {cluster.get('cluster_size', 0)} users indicates immediate market need"
        }

    def _sort_priority_key(self, candidate: Dict) -> tuple:
        """ç”Ÿæˆæ’åºé”®ï¼Œç¡®ä¿è·¨æºéªŒè¯çš„æœºä¼šæ’åœ¨å‰é¢

        æ’åºä¼˜å…ˆçº§ï¼š
        1. è·¨æºéªŒè¯ç­‰çº§ï¼ˆLevel 1 > Level 2 > Level 3 > No validationï¼‰
        2. æœ€ç»ˆè¯„åˆ†ï¼ˆé™åºï¼‰
        3. èšç±»è§„æ¨¡ï¼ˆé™åºï¼‰

        Args:
            candidate: å€™é€‰æœºä¼šå­—å…¸

        Returns:
            æ’åºé”®å…ƒç»„
        """
        cross_source = candidate.get('cross_source_validation', {})
        validation_level = cross_source.get('validation_level', 0)

        # éªŒè¯ç­‰çº§ï¼šLevel 1 æœ€ä¼˜å…ˆï¼ŒLevel 0ï¼ˆæ— éªŒè¯ï¼‰æœ€ä½
        # ä½¿ç”¨åå‘æ˜ å°„ï¼š1 -> 3, 2 -> 2, 3 -> 1, 0 -> 0
        if validation_level == 1:
            priority_score = 3
        elif validation_level == 2:
            priority_score = 2
        elif validation_level == 3:
            priority_score = 1
        else:
            priority_score = 0

        # ç”¨è´Ÿæ•°å®ç°é™åºï¼šä¼˜å…ˆçº§é«˜çš„æ’åœ¨å‰é¢
        priority_score = -priority_score

        # æœ€ç»ˆè¯„åˆ†é™åº
        final_score = -candidate.get('final_score', 0)

        # èšç±»è§„æ¨¡é™åº
        cluster_size = -candidate.get('cluster_size', 0)

        return (priority_score, final_score, cluster_size)

    def generate_shortlist(self) -> Dict[str, Any]:
        """ç”Ÿæˆå†³ç­–æ¸…å•ï¼ˆä¸»æ–¹æ³•ï¼‰

        æµç¨‹ï¼š
        1. åº”ç”¨ç¡¬æ€§è¿‡æ»¤
        2. å¯¹æ¯ä¸ªæœºä¼šè¿›è¡Œè·¨æºéªŒè¯å’Œè¯„åˆ†
        3. æŒ‰æœ€ç»ˆè¯„åˆ†æ’åº
        4. é€‰æ‹© Top 3-5 å€™é€‰
        5. ç”Ÿæˆå¯è¯»æ€§å†…å®¹
        6. å¯¼å‡º markdown å’Œ JSON æŠ¥å‘Š

        Returns:
            åŒ…å« shortlist çš„ç»“æœå­—å…¸
        """
        logger.info("=== Decision Shortlist Generation Started ===")

        # æ­¥éª¤ 1: åº”ç”¨ç¡¬æ€§è¿‡æ»¤
        logger.info("Step 1: Applying hard filters...")
        opportunities = self._apply_hard_filters()

        if not opportunities:
            logger.warning("No opportunities passed hard filters")
            return self._handle_empty_shortlist()

        logger.info(f"âœ… {len(opportunities)} opportunities passed hard filters")

        # æ­¥éª¤ 2-3: å¯¹æ¯ä¸ªæœºä¼šè¿›è¡Œè·¨æºéªŒè¯å’Œè¯„åˆ†
        logger.info("Step 2-3: Calculating final scores with cross-source validation...")
        scored_opportunities = []

        for opp in opportunities:
            # è·¨æºéªŒè¯
            cross_source_info = self._check_cross_source_validation(opp)

            # è®¡ç®—æœ€ç»ˆè¯„åˆ†
            final_score = self._calculate_final_score(opp, cross_source_info)

            # æ·»åŠ è¯„åˆ†ä¿¡æ¯
            opp_with_score = {
                **opp,
                'final_score': final_score,
                'cross_source_validation': cross_source_info
            }

            scored_opportunities.append(opp_with_score)

        logger.info(f"âœ… Scored {len(scored_opportunities)} opportunities")

        # æ­¥éª¤ 4: æŒ‰ç…§ä¼˜å…ˆçº§æ’åºå¹¶é€‰æ‹© Top å€™é€‰
        logger.info("Step 4: Selecting top candidates...")
        # æŒ‰ç…§ä¼˜å…ˆçº§æ’åºï¼šè·¨æºéªŒè¯ > æœ€ç»ˆè¯„åˆ† > èšç±»è§„æ¨¡
        scored_opportunities.sort(key=self._sort_priority_key)

        top_candidates = self._select_top_candidates_with_diversity(scored_opportunities)
        logger.info(f"âœ… Selected {len(top_candidates)} top candidates")

        if not top_candidates:
            logger.warning("No candidates selected")
            return self._handle_empty_shortlist()

        # æ­¥éª¤ 5: ç”Ÿæˆå¯è¯»æ€§å†…å®¹
        logger.info("Step 5: Generating readable content...")
        for candidate in top_candidates:
            readable_content = self._generate_readable_content(
                candidate,
                candidate,
                candidate['cross_source_validation']
            )
            candidate['readable_content'] = readable_content
            logger.info(f"  - {candidate['opportunity_name']}: {readable_content['problem'][:50]}...")

        # æ­¥éª¤ 6: å¯¼å‡ºæŠ¥å‘Š
        logger.info("Step 6: Exporting reports...")
        markdown_path = self._export_markdown_report(top_candidates)
        json_path = self._export_json_report(top_candidates)

        result = {
            'shortlist_count': len(top_candidates),
            'shortlist': top_candidates,
            'markdown_report': markdown_path,
            'json_report': json_path,
            'generated_at': datetime.now().isoformat()
        }

        logger.info("=== Decision Shortlist Generation Complete ===")
        logger.info(f"ğŸ“ Markdown report: {markdown_path}")
        logger.info(f"ğŸ“Š JSON report: {json_path}")

        return result

    def _select_top_candidates_with_diversity(self, scored_opportunities: List[Dict]) -> List[Dict]:
        """é€‰æ‹© Top å€™é€‰ï¼Œè€ƒè™‘å¤šæ ·æ€§

        Args:
            scored_opportunities: å·²è¯„åˆ†çš„æœºä¼šåˆ—è¡¨

        Returns:
            é€‰ä¸­çš„å€™é€‰åˆ—è¡¨
        """
        config = self.config['output']
        min_candidates = config['min_candidates']
        max_candidates = config['max_candidates']

        # ç®€å•ç­–ç•¥ï¼šå–å‰ N ä¸ª
        # TODO: æœªæ¥å¯ä»¥åŠ å…¥å¤šæ ·æ€§è€ƒè™‘ï¼ˆä¸åŒçš„ cluster, ä¸åŒçš„é—®é¢˜ç±»å‹ç­‰ï¼‰
        selected_count = min(max_candidates, len(scored_opportunities))

        # ç¡®ä¿è‡³å°‘æœ‰ min_candidates ä¸ª
        if len(scored_opportunities) < min_candidates:
            logger.warning(f"Only {len(scored_opportunities)} candidates available, less than min {min_candidates}")
            selected_count = len(scored_opportunities)

        return scored_opportunities[:selected_count]

    def _get_cross_source_badge(self, cross_source: Dict) -> str:
        """ç”Ÿæˆè·¨æºéªŒè¯çš„å¾½ç« æ ‡è¯†

        Args:
            cross_source: è·¨æºéªŒè¯ä¿¡æ¯å­—å…¸

        Returns:
            å¾½ç« å­—ç¬¦ä¸²ï¼ˆMarkdownæ ¼å¼ï¼‰
        """
        if not cross_source.get('has_cross_source'):
            return ""

        validation_level = cross_source.get('validation_level', 0)

        if validation_level == 1:
            # Level 1: æœ€å¼ºä¿¡å· - å¤šå¹³å°ç‹¬ç«‹éªŒè¯
            return """
<div align="center">

### ğŸ¯ INDEPENDENT VALIDATION ACROSS REDDIT + HACKER NEWS

**This pain point has been independently validated across multiple communities**

</div>
"""
        elif validation_level == 2:
            # Level 2: ä¸­ç­‰ä¿¡å· - å¤š subreddit éªŒè¯
            return """
### âœ“ Multi-Subreddit Validation
*Validated across 3+ subreddits with strong cluster size*
"""
        elif validation_level == 3:
            # Level 3: å¼±ä¿¡å·
            return """
### â— Weak Cross-Source Signal
*Initial cross-community detection signal*
"""
        else:
            return ""

    def _get_cross_source_badge_text(self, cross_source: Dict) -> str:
        """è·å–è·¨æºéªŒè¯å¾½ç« çš„çº¯æ–‡æœ¬ç‰ˆæœ¬

        Args:
            cross_source: è·¨æºéªŒè¯ä¿¡æ¯å­—å…¸

        Returns:
            å¾½ç« æ–‡æœ¬
        """
        if not cross_source.get('has_cross_source'):
            return ""

        validation_level = cross_source.get('validation_level', 0)

        badge_texts = {
            1: "ğŸ¯ INDEPENDENT VALIDATION ACROSS REDDIT + HACKER NEWS",
            2: "âœ“ Multi-Subreddit Validation",
            3: "â— Weak Cross-Source Signal"
        }

        return badge_texts.get(validation_level, "")

    def _export_markdown_report(self, shortlist: List[Dict]) -> str:
        """å¯¼å‡º Markdown æ ¼å¼çš„æŠ¥å‘Š

        Args:
            shortlist: å†³ç­–æ¸…å•åˆ—è¡¨

        Returns:
            æŠ¥å‘Šæ–‡ä»¶è·¯å¾„
        """
        config = self.config['output']
        output_dir = config['markdown_dir']

        os.makedirs(output_dir, exist_ok=True)

        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f'decision_shortlist_{timestamp}.md'
        filepath = os.path.join(output_dir, filename)

        # ç”ŸæˆæŠ¥å‘Šå†…å®¹
        report_lines = [
            "# Decision Shortlist Report",
            f"\n**Generated**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            f"**Total Candidates**: {len(shortlist)}",
            "\n---\n"
        ]

        for idx, candidate in enumerate(shortlist, 1):
            content = candidate.get('readable_content', {})
            cross_source = candidate.get('cross_source_validation', {})

            report_lines.extend([
                f"## {idx}. {candidate['opportunity_name']}"
            ])

            # æ·»åŠ è·¨æºéªŒè¯å¾½ç« ï¼ˆåœ¨æœ€å‰é¢ï¼Œæœ€é†’ç›®ï¼‰
            badge = self._get_cross_source_badge(cross_source)
            if badge:
                report_lines.extend([
                    f"\n{badge}",
                    f"**Validation Level**: {cross_source.get('validation_level', 0)}  ",
                    f"**Boost Applied**: +{cross_source.get('boost_score', 0.0):.1f} to final score",
                    ""
                ])

            report_lines.extend([
                f"**Final Score**: {candidate['final_score']:.2f}/10.0  ",
                f"**Viability Score**: {candidate['viability_score']:.1f}  ",
                f"**Cluster Size**: {candidate['cluster_size']}  ",
                f"**Trust Level**: {candidate['trust_level']:.2f}  ",
                f"**Validated Problem**: {'âœ… Yes' if cross_source.get('validated_problem') else 'âŒ No'}"
            ])

            report_lines.extend([
                "\n### Problem",
                f"\n{content.get('problem', 'N/A')}",
                "\n### MVP Solution",
                f"\n{content.get('mvp', 'N/A')}",
                "\n### Why Now",
                f"\n{content.get('why_now', 'N/A')}",
                "\n### Additional Details",
                f"\n- **Target Users**: {candidate.get('target_users', 'N/A')}",
                f"- **Missing Capability**: {candidate.get('missing_capability', 'N/A')}",
                f"- **Why Existing Solutions Fail**: {candidate.get('why_existing_fail', 'N/A')}",
                "\n---\n"
            ])

        # å†™å…¥æ–‡ä»¶
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write('\n'.join(report_lines))

        logger.info(f"âœ… Markdown report exported: {filepath}")
        return filepath

    def _export_json_report(self, shortlist: List[Dict]) -> str:
        """å¯¼å‡º JSON æ ¼å¼çš„æŠ¥å‘Š

        Args:
            shortlist: å†³ç­–æ¸…å•åˆ—è¡¨

        Returns:
            æŠ¥å‘Šæ–‡ä»¶è·¯å¾„
        """
        config = self.config['output']
        output_dir = config['json_dir']

        os.makedirs(output_dir, exist_ok=True)

        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f'decision_shortlist_{timestamp}.json'
        filepath = os.path.join(output_dir, filename)

        # å‡†å¤‡å¯¼å‡ºæ•°æ®
        export_data = {
            'generated_at': datetime.now().isoformat(),
            'total_candidates': len(shortlist),
            'candidates': []
        }

        for candidate in shortlist:
            cross_source = candidate.get('cross_source_validation', {})

            export_candidate = {
                'opportunity_name': candidate.get('opportunity_name'),
                'final_score': candidate.get('final_score'),
                'viability_score': candidate.get('viability_score'),
                'cluster_size': candidate.get('cluster_size'),
                'trust_level': candidate.get('trust_level'),
                'target_users': candidate.get('target_users'),
                'missing_capability': candidate.get('missing_capability'),
                'why_existing_fail': candidate.get('why_existing_fail'),
                'readable_content': candidate.get('readable_content', {}),
                'cross_source_validation': {
                    'has_cross_source': cross_source.get('has_cross_source', False),
                    'validation_level': cross_source.get('validation_level', 0),
                    'validated_problem': cross_source.get('validated_problem', False),
                    'boost_score': cross_source.get('boost_score', 0.0),
                    'evidence': cross_source.get('evidence', ''),
                    'badge_text': self._get_cross_source_badge_text(cross_source)
                }
            }
            export_data['candidates'].append(export_candidate)

        # å†™å…¥æ–‡ä»¶
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(export_data, f, indent=2, ensure_ascii=False)

        logger.info(f"âœ… JSON report exported: {filepath}")
        return filepath

    def _handle_empty_shortlist(self) -> Dict[str, Any]:
        """å¤„ç†ç©ºæ¸…å•çš„æƒ…å†µ

        Returns:
            ç©ºç»“æœå­—å…¸
        """
        logger.warning("=== Empty Shortlist ===")

        result = {
            'shortlist_count': 0,
            'shortlist': [],
            'generated_at': datetime.now().isoformat(),
            'warning': 'No opportunities met the criteria'
        }

        return result
