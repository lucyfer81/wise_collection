# Reddit Pain Finder - ä»£ç æ±‡æ€»

ç”Ÿæˆæ—¶é—´: 2025-12-23 09:16:00

æœ¬æ–‡æ¡£åŒ…å« reddit_pain_finder é¡¹ç›®çš„æ ¸å¿ƒä»£ç æ–‡ä»¶ï¼š
- Pipelineå¤„ç†æ¨¡å— (pipeline/)
- å·¥å…·æ¨¡å— (utils/)
- ä¸»è¦æ‰§è¡Œè„šæœ¬



================================================================================
æ–‡ä»¶: run_pipeline.py
================================================================================

```python
#!/usr/bin/env python3
"""
Wise Collection - Main Pipeline Runner
ä¸»è¦çš„pipelineæ‰§è¡Œè„šæœ¬ - ä¸€é”®è¿è¡Œæ•´ä¸ªæ•°æ®æ”¶é›†æµç¨‹
"""
import os
import sys
import argparse
import logging
import json
import time
from datetime import datetime
from typing import Dict, Any, Optional, List

# è®¾ç½®é¡¹ç›®æ ¹ç›®å½•
project_root = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, project_root)

# å¯¼å…¥pipelineæ¨¡å—
from pipeline.fetch import RedditSourceFetcher
from pipeline.filter_signal import PainSignalFilter
from pipeline.extract_pain import PainPointExtractor
from pipeline.embed import PainEventEmbedder
from pipeline.cluster import PainEventClusterer
from pipeline.score_viability import ViabilityScorer
from pipeline.map_opportunity import OpportunityMapper
from pipeline.align_cross_sources import CrossSourceAligner

# å¯¼å…¥å·¥å…·æ¨¡å—
from utils.db import db
from utils.llm_client import LLMClient

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/pipeline.log'),
        logging.StreamHandler(sys.stdout)
    ]
)

logger = logging.getLogger(__name__)

class WiseCollectionPipeline:
    """Wise Collectionæ•°æ®æ”¶é›†Pipeline"""

    def __init__(self):
        """åˆå§‹åŒ–pipeline"""
        self.pipeline_start_time = datetime.now()
        self.stats = {
            "start_time": self.pipeline_start_time.isoformat(),
            "stages_completed": [],
            "stages_failed": [],
            "stage_results": {},
            "total_runtime_seconds": 0,
            "final_summary": {}
        }

        # ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨
        os.makedirs("logs", exist_ok=True)

    def _load_config(self, config_path: str = "config/llm.yaml") -> Dict[str, Any]:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            import yaml
            with open(config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
            return config
        except Exception as e:
            logger.warning(f"Failed to load config from {config_path}: {e}")
            # è¿”å›é»˜è®¤é…ç½®
            return {
                'database': {'path': 'data/wise_collection.db'},
                'llm': {
                    'models': {
                        'main': 'gpt-4',
                        'medium': 'gpt-3.5-turbo',
                        'small': 'gpt-3.5-turbo'
                    }
                }
            }

    def run_stage_fetch(self, limit_sources: Optional[int] = None,
                       sources: Optional[List[str]] = None) -> Dict[str, Any]:
        """é˜¶æ®µ1: æ•°æ®æŠ“å–ï¼ˆæ”¯æŒå¤šæ•°æ®æºï¼‰"""
        logger.info("=" * 50)
        logger.info("STAGE 1: Multi-Source Posts Fetcher")
        logger.info("=" * 50)

        try:
            from pipeline.fetch import MultiSourceFetcher

            # ä½¿ç”¨æŒ‡å®šçš„æ•°æ®æºï¼Œé»˜è®¤ä¸º reddit + hackernews
            fetch_sources = sources or ['reddit', 'hackernews']
            fetcher = MultiSourceFetcher(sources=fetch_sources)
            result = fetcher.fetch_all(limit_sources=limit_sources)

            self.stats["stages_completed"].append("fetch")
            self.stats["stage_results"]["fetch"] = result

            logger.info(f"âœ… Stage 1 completed: Found {result['total_saved']} posts from {len(result['sources_processed'])} sources")

            # è¾“å‡ºå„æ•°æ®æºç»Ÿè®¡
            for source, stats in result.get("source_stats", {}).items():
                if "error" not in stats:
                    logger.info(f"   - {source}: {stats.get('total_saved', 0)} posts")
                else:
                    logger.error(f"   - {source}: ERROR - {stats['error']}")

            return result

        except Exception as e:
            logger.error(f"âŒ Stage 1 failed: {e}")
            self.stats["stages_failed"].append("fetch")
            raise

    def run_stage_filter(self, limit_posts: Optional[int] = None) -> Dict[str, Any]:
        """é˜¶æ®µ2: ä¿¡å·è¿‡æ»¤"""
        logger.info("=" * 50)
        logger.info("STAGE 2: Filtering pain signals")
        logger.info("=" * 50)

        try:
            filter = PainSignalFilter()

            # è·å–æœªè¿‡æ»¤çš„å¸–å­
            unfiltered_posts = db.get_unprocessed_posts(limit=limit_posts or 1000)

            if not unfiltered_posts:
                logger.info("No posts to filter")
                result = {"processed": 0, "filtered": 0}
            else:
                logger.info(f"Filtering {len(unfiltered_posts)} posts")
                filtered_posts = filter.filter_posts_batch(unfiltered_posts)

                # ä¿å­˜è¿‡æ»¤ç»“æœ
                saved_count = 0
                for post in filtered_posts:
                    if db.insert_filtered_post(post):
                        saved_count += 1

                result = {
                    "processed": len(unfiltered_posts),
                    "filtered": len(filtered_posts),
                    "saved": saved_count,
                    "filter_stats": filter.get_statistics()
                }

            self.stats["stages_completed"].append("filter")
            self.stats["stage_results"]["filter"] = result

            logger.info(f"âœ… Stage 2 completed: Filtered {result['saved']} posts")
            return result

        except Exception as e:
            logger.error(f"âŒ Stage 2 failed: {e}")
            self.stats["stages_failed"].append("filter")
            raise

    def run_stage_extract(self, limit_posts: Optional[int] = None) -> Dict[str, Any]:
        """é˜¶æ®µ3: ç—›ç‚¹æŠ½å–"""
        logger.info("=" * 50)
        logger.info("STAGE 3: Extracting pain points")
        logger.info("=" * 50)

        try:
            extractor = PainPointExtractor()
            result = extractor.process_unextracted_posts(limit=limit_posts or 100)

            self.stats["stages_completed"].append("extract")
            self.stats["stage_results"]["extract"] = result

            logger.info(f"âœ… Stage 3 completed: Extracted {result['pain_events_saved']} pain events")
            return result

        except Exception as e:
            logger.error(f"âŒ Stage 3 failed: {e}")
            self.stats["stages_failed"].append("extract")
            raise

    def run_stage_embed(self, limit_events: Optional[int] = None) -> Dict[str, Any]:
        """é˜¶æ®µ4: å‘é‡åŒ–"""
        logger.info("=" * 50)
        logger.info("STAGE 4: Creating embeddings")
        logger.info("=" * 50)

        try:
            embedder = PainEventEmbedder()
            result = embedder.process_missing_embeddings(limit=limit_events or 200)

            self.stats["stages_completed"].append("embed")
            self.stats["stage_results"]["embed"] = result

            logger.info(f"âœ… Stage 4 completed: Created {result['embeddings_created']} embeddings")
            return result

        except Exception as e:
            logger.error(f"âŒ Stage 4 failed: {e}")
            self.stats["stages_failed"].append("embed")
            raise

    def run_stage_cluster(self, limit_events: Optional[int] = None) -> Dict[str, Any]:
        """é˜¶æ®µ5: èšç±»"""
        logger.info("=" * 50)
        logger.info("STAGE 5: Clustering pain events")
        logger.info("=" * 50)

        try:
            clusterer = PainEventClusterer()
            result = clusterer.cluster_pain_events(limit=limit_events or 200)

            self.stats["stages_completed"].append("cluster")
            self.stats["stage_results"]["cluster"] = result

            logger.info(f"âœ… Stage 5 completed: Created {result['clusters_created']} clusters")
            return result

        except Exception as e:
            logger.error(f"âŒ Stage 5 failed: {e}")
            self.stats["stages_failed"].append("cluster")
            raise

    def run_stage_cross_source_alignment(self) -> Dict[str, Any]:
        """é˜¶æ®µ5.5: è·¨æºå¯¹é½"""
        logger.info("=" * 50)
        logger.info("STAGE 5.5: Cross-Source Alignment")
        logger.info("=" * 50)

        try:
            # åˆå§‹åŒ–å¯¹é½å™¨
            llm_client = LLMClient()  # Uses default config path
            aligner = CrossSourceAligner(db, llm_client)

            # æ‰§è¡Œè·¨æºå¯¹é½
            logger.info("Processing cross-source alignment...")
            aligner.process_alignments()

            # è·å–å¯¹é½ç»“æœ
            aligned_problems = db.get_aligned_problems()

            result = {
                "aligned_problems_count": len(aligned_problems),
                "aligned_problems": aligned_problems
            }

            self.stats["stages_completed"].append("alignment")
            self.stats["stage_results"]["alignment"] = result

            logger.info(f"âœ… Stage 5.5 completed: Found {len(aligned_problems)} aligned problems")

            # æ˜¾ç¤ºå¯¹é½æ‘˜è¦
            if aligned_problems:
                logger.info("\nAlignment Summary:")
                logger.info(f"- Total aligned problems: {len(aligned_problems)}")
                for problem in aligned_problems[:3]:  # æ˜¾ç¤ºå‰3ä¸ª
                    logger.info(f"  {problem['aligned_problem_id']}: {problem['core_problem'][:100]}...")
                    logger.info(f"  Sources: {', '.join(problem['sources'])}")
            else:
                logger.info("No cross-source alignments found in this run")

            return result

        except Exception as e:
            logger.error(f"âŒ Stage 5.5 failed: {e}")
            self.stats["stages_failed"].append("alignment")
            raise

    def run_stage_map_opportunities(self, limit_clusters: Optional[int] = None) -> Dict[str, Any]:
        """é˜¶æ®µ6: æœºä¼šæ˜ å°„"""
        logger.info("=" * 50)
        logger.info("STAGE 6: Mapping opportunities")
        logger.info("=" * 50)

        try:
            mapper = OpportunityMapper()
            result = mapper.map_opportunities_for_clusters(limit=limit_clusters or 50)

            self.stats["stages_completed"].append("map_opportunities")
            self.stats["stage_results"]["map_opportunities"] = result

            logger.info(f"âœ… Stage 6 completed: Mapped {result['opportunities_created']} opportunities")
            return result

        except Exception as e:
            logger.error(f"âŒ Stage 6 failed: {e}")
            self.stats["stages_failed"].append("map_opportunities")
            raise

    def run_stage_score(self, limit_opportunities: Optional[int] = None) -> Dict[str, Any]:
        """é˜¶æ®µ7: å¯è¡Œæ€§è¯„åˆ†"""
        logger.info("=" * 50)
        logger.info("STAGE 7: Scoring viability")
        logger.info("=" * 50)

        try:
            scorer = ViabilityScorer()
            result = scorer.score_opportunities(limit=limit_opportunities or 100)

            self.stats["stages_completed"].append("score")
            self.stats["stage_results"]["score"] = result

            logger.info(f"âœ… Stage 7 completed: Scored {result['opportunities_scored']} opportunities")
            return result

        except Exception as e:
            logger.error(f"âŒ Stage 7 failed: {e}")
            self.stats["stages_failed"].append("score")
            raise

    def generate_final_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š"""
        logger.info("=" * 50)
        logger.info("GENERATING FINAL REPORT")
        logger.info("=" * 50)

        try:
            # è·å–æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯
            db_stats = db.get_statistics()

            # è·å–æœ€é«˜åˆ†çš„æœºä¼š
            top_opportunities = []
            try:
                with db.get_connection("clusters") as conn:
                    cursor = conn.execute("""
                        SELECT o.opportunity_name, o.total_score, o.recommendation, c.cluster_name
                        FROM opportunities o
                        JOIN clusters c ON o.cluster_id = c.id
                        WHERE o.total_score > 0
                        ORDER BY o.total_score DESC
                        LIMIT 10
                    """)
                    top_opportunities = [dict(row) for row in cursor.fetchall()]
            except Exception as e:
                logger.warning(f"Failed to get top opportunities: {e}")

            # è®¡ç®—è¿è¡Œæ—¶é—´
            end_time = datetime.now()
            total_runtime = (end_time - self.pipeline_start_time).total_seconds()
            self.stats["total_runtime_seconds"] = total_runtime

            # ç”Ÿæˆæœ€ç»ˆæ‘˜è¦
            final_summary = {
                "pipeline_completed": len(self.stats["stages_failed"]) == 0,
                "stages_completed": len(self.stats["stages_completed"]),
                "stages_failed": len(self.stats["stages_failed"]),
                "total_runtime_minutes": round(total_runtime / 60, 2),
                "database_statistics": db_stats,
                "top_opportunities": top_opportunities,
                "pipeline_efficiency": {
                    "posts_per_minute": self.stats["stage_results"].get("fetch", {}).get("total_saved", 0) / max(total_runtime / 60, 1),
                    "pain_events_per_hour": self.stats["stage_results"].get("extract", {}).get("pain_events_saved", 0) / max(total_runtime / 3600, 1),
                    "opportunities_per_hour": self.stats["stage_results"].get("map_opportunities", {}).get("opportunities_created", 0) / max(total_runtime / 3600, 1)
                }
            }

            self.stats["final_summary"] = final_summary

            logger.info("ğŸ‰ PIPELINE COMPLETED SUCCESSFULLY!")
            logger.info(f"ğŸ“Š Final Summary:")
            logger.info(f"   â€¢ Runtime: {final_summary['total_runtime_minutes']} minutes")
            logger.info(f"   â€¢ Stages completed: {final_summary['stages_completed']}/7")
            logger.info(f"   â€¢ Raw posts collected: {db_stats.get('raw_posts_count', 0)}")
            logger.info(f"   â€¢ Pain events extracted: {db_stats.get('pain_events_count', 0)}")
            logger.info(f"   â€¢ Clusters created: {db_stats.get('clusters_count', 0)}")
            logger.info(f"   â€¢ Opportunities identified: {db_stats.get('opportunities_count', 0)}")

            if top_opportunities:
                logger.info(f"   â€¢ Top opportunity: {top_opportunities[0]['opportunity_name']} (Score: {top_opportunities[0]['total_score']:.1f})")

            return final_summary

        except Exception as e:
            logger.error(f"Failed to generate final report: {e}")
            return {"error": str(e)}

    def run_full_pipeline(
        self,
        limit_sources: Optional[int] = None,
        limit_posts: Optional[int] = None,
        limit_events: Optional[int] = None,
        limit_clusters: Optional[int] = None,
        limit_opportunities: Optional[int] = None,
        sources: Optional[List[str]] = None,
        stop_on_error: bool = False
    ) -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´pipeline"""
        logger.info("ğŸš€ Starting Wise Collection Multi-Source Pipeline")
        logger.info(f"â° Started at: {self.pipeline_start_time}")

        # ä½¿ç”¨æŒ‡å®šçš„æ•°æ®æºï¼Œé»˜è®¤ä¸º reddit + hackernews
        fetch_sources = sources or ['reddit', 'hackernews']
        logger.info(f"ğŸ“¡ Data sources: {', '.join(fetch_sources)}")

        stages = [
            ("fetch", lambda: self.run_stage_fetch(limit_sources, fetch_sources)),
            ("filter", lambda: self.run_stage_filter(limit_posts)),
            ("extract", lambda: self.run_stage_extract(limit_posts)),
            ("embed", lambda: self.run_stage_embed(limit_events)),
            ("cluster", lambda: self.run_stage_cluster(limit_events)),
            ("alignment", lambda: self.run_stage_cross_source_alignment()),
            ("map_opportunities", lambda: self.run_stage_map_opportunities(limit_clusters)),
            ("score", lambda: self.run_stage_score(limit_opportunities))
        ]

        for stage_name, stage_func in stages:
            try:
                stage_func()
            except Exception as e:
                logger.error(f"Stage '{stage_name}' failed: {e}")
                if stop_on_error:
                    logger.error("Stopping pipeline due to error")
                    break
                else:
                    logger.warning(f"Continuing pipeline despite '{stage_name}' failure")

        # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
        final_report = self.generate_final_report()

        return final_report

    def run_single_stage(self, stage_name: str, **kwargs) -> Dict[str, Any]:
        """è¿è¡Œå•ä¸ªé˜¶æ®µ"""
        stage_map = {
            "fetch": lambda: self.run_stage_fetch(kwargs.get("limit_sources"), kwargs.get("sources")),
            "filter": lambda: self.run_stage_filter(kwargs.get("limit_posts")),
            "extract": lambda: self.run_stage_extract(kwargs.get("limit_posts")),
            "embed": lambda: self.run_stage_embed(kwargs.get("limit_events")),
            "cluster": lambda: self.run_stage_cluster(kwargs.get("limit_events")),
            "alignment": lambda: self.run_stage_cross_source_alignment(),
            "map": lambda: self.run_stage_map_opportunities(kwargs.get("limit_clusters")),
            "score": lambda: self.run_stage_score(kwargs.get("limit_opportunities"))
        }

        if stage_name not in stage_map:
            raise ValueError(f"Unknown stage: {stage_name}")

        return stage_map[stage_name]()

    def save_results(self, filename: Optional[str] = None):
        """ä¿å­˜pipelineç»“æœ"""
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"pipeline_results_{timestamp}.json"

        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(self.stats, f, indent=2, default=str)

            logger.info(f"ğŸ“ Results saved to: {filename}")
            return filename

        except Exception as e:
            logger.error(f"Failed to save results: {e}")
            return None

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="Wise Collection Multi-Source Pipeline")

    # è¿è¡Œæ¨¡å¼
    parser.add_argument("--stage", choices=["fetch", "filter", "extract", "embed", "cluster", "alignment", "map", "score", "all"],
                       default="all", help="Which stage to run (default: all)")

    # æ•°æ®æºé€‰æ‹©
    parser.add_argument("--sources", nargs="+", choices=["reddit", "hackernews"],
                       default=["reddit", "hackernews"], help="Data sources to fetch (default: reddit hackernews)")

    # é™åˆ¶å‚æ•°
    parser.add_argument("--limit-sources", type=int, help="Limit number of sources to fetch")
    parser.add_argument("--limit-posts", type=int, help="Limit number of posts to process")
    parser.add_argument("--limit-events", type=int, help="Limit number of pain events to process")
    parser.add_argument("--limit-clusters", type=int, help="Limit number of clusters to process")
    parser.add_argument("--limit-opportunities", type=int, help="Limit number of opportunities to score")

    # å…¶ä»–é€‰é¡¹
    parser.add_argument("--stop-on-error", action="store_true", help="Stop pipeline on first error")
    parser.add_argument("--save-results", action="store_true", help="Save results to file")
    parser.add_argument("--results-file", help="Custom results filename")

    args = parser.parse_args()

    try:
        # åˆå§‹åŒ–pipeline
        pipeline = WiseCollectionPipeline()

        if args.stage == "all":
            # è¿è¡Œå®Œæ•´pipeline
            result = pipeline.run_full_pipeline(
                limit_sources=args.limit_sources,
                limit_posts=args.limit_posts,
                limit_events=args.limit_events,
                limit_clusters=args.limit_clusters,
                limit_opportunities=args.limit_opportunities,
                sources=args.sources,
                stop_on_error=args.stop_on_error
            )
        else:
            # è¿è¡Œå•ä¸ªé˜¶æ®µ
            stage_kwargs = {
                "limit_sources": args.limit_sources,
                "limit_posts": args.limit_posts,
                "limit_events": args.limit_events,
                "limit_clusters": args.limit_clusters,
                "limit_opportunities": args.limit_opportunities,
                "sources": args.sources
            }

            # åªä¼ é€’ç›¸å…³çš„å‚æ•°
            relevant_kwargs = {k: v for k, v in stage_kwargs.items() if v is not None}
            result = pipeline.run_single_stage(args.stage, **relevant_kwargs)

        # ä¿å­˜ç»“æœ
        if args.save_results:
            pipeline.save_results(args.results_file)

        # è¾“å‡ºç»“æœ
        print("\n" + "=" * 60)
        print("PIPELINE RESULTS")
        print("=" * 60)
        print(json.dumps(result, indent=2, default=str))

    except KeyboardInterrupt:
        logger.info("Pipeline interrupted by user")
        sys.exit(1)
    except Exception as e:
        logger.error(f"Pipeline failed: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()
```


================================================================================
æ–‡ä»¶: pain_point_analyzer.py
================================================================================

```python
#!/usr/bin/env python3
"""
ç—›ç‚¹åº”ç”¨åˆ†æå™¨

é’ˆå¯¹æ¯ä¸ªç—›ç‚¹èšç±»ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Šï¼ŒåŒ…å«ï¼š
1. ç—›ç‚¹åˆ†æ
2. åº”ç”¨è®¾è®¡æ–¹æ¡ˆ
3. å¯æ‰§è¡Œæœºä¼šæ¸…å•

æ¯ä¸ªèšç±»ç”Ÿæˆä¸€ä¸ªç‹¬ç«‹çš„markdownæ–‡ä»¶
"""

import os
import sqlite3
import json
import requests
from datetime import datetime
from typing import Dict, List, Any, Optional
import re
from pathlib import Path
import logging
import sys

# å¯¼å…¥ç»Ÿä¸€æ•°æ®åº“ç®¡ç†å™¨
try:
    from utils.db import WiseCollectionDB
except ImportError as e:
    print(f"âŒ æ— æ³•å¯¼å…¥æ•°æ®åº“ç®¡ç†å™¨: {e}")
    sys.exit(1)

# åŠ è½½.envæ–‡ä»¶
def load_env():
    """åŠ è½½.envæ–‡ä»¶"""
    env_path = Path(__file__).parent / '.env'
    if env_path.exists():
        with open(env_path, 'r') as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith('#'):
                    key, value = line.split('=', 1)
                    os.environ[key.strip()] = value.strip()

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),  # è¾“å‡ºåˆ°æ§åˆ¶å°
        logging.FileHandler('pain_point_analyzer.log', encoding='utf-8')  # åŒæ—¶è¾“å‡ºåˆ°æ–‡ä»¶
    ]
)

logger = logging.getLogger(__name__)

# åŠ è½½ç¯å¢ƒå˜é‡
logger.info("å¼€å§‹åŠ è½½ç¯å¢ƒå˜é‡...")
load_env()
logger.info("ç¯å¢ƒå˜é‡åŠ è½½å®Œæˆ")


class PainPointAnalyzer:
    def __init__(self, unified_db: bool = True):
        """åˆå§‹åŒ–åˆ†æå™¨"""
        logger.info("åˆå§‹åŒ– PainPointAnalyzer...")

        # åˆå§‹åŒ–ç»Ÿä¸€æ•°æ®åº“ç®¡ç†å™¨
        logger.info("åˆå§‹åŒ–æ•°æ®åº“ç®¡ç†å™¨...")
        self.db = WiseCollectionDB(unified=unified_db)
        self.unified_db = unified_db

        if unified_db:
            logger.info(f"ä½¿ç”¨ç»Ÿä¸€æ•°æ®åº“æ¨¡å¼: {self.db.get_database_path()}")
        else:
            logger.info("ä½¿ç”¨å¤šæ•°æ®åº“æ¨¡å¼")

        self.base_url = os.getenv('Siliconflow_Base_URL', 'https://api.siliconflow.cn/v1')
        self.api_key = os.getenv('Siliconflow_KEY')
        self.model = os.getenv('Siliconflow_AI_Model_Default', 'deepseek-ai/DeepSeek-V3.2')

        logger.info(f"é…ç½®ä¿¡æ¯: base_url={self.base_url}, model={self.model}")
        logger.info(f"API key {'å·²è®¾ç½®' if self.api_key else 'æœªè®¾ç½®'}")

        if not self.api_key:
            logger.error("SiliconFlow API key not found in environment variables")
            raise ValueError("SiliconFlow API key not found in environment variables")

        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir = "pain_analysis_reports"
        os.makedirs(self.output_dir, exist_ok=True)
        logger.info(f"è¾“å‡ºç›®å½•å·²åˆ›å»º: {self.output_dir}")

        print(f"ğŸ”§ åˆå§‹åŒ–åˆ†æå™¨")
        print(f"   â€¢ æ•°æ®åº“æ¨¡å¼: {'ç»Ÿä¸€æ•°æ®åº“' if unified_db else 'å¤šæ•°æ®åº“æ–‡ä»¶'}")
        if unified_db:
            print(f"   â€¢ æ•°æ®åº“è·¯å¾„: {self.db.get_database_path()}")
        print(f"   â€¢ APIæ¨¡å‹: {self.model}")
        print(f"   â€¢ è¾“å‡ºç›®å½•: {self.output_dir}")

    def get_db_connection(self, db_type: str = "clusters"):
        """è·å–æ•°æ®åº“è¿æ¥ - ä½¿ç”¨ç»Ÿä¸€æ•°æ®åº“ç®¡ç†å™¨"""
        logger.debug(f"è·å–æ•°æ®åº“è¿æ¥ï¼Œç±»å‹: {db_type}")
        return self.db.get_connection(db_type)

    def call_llm(self, prompt: str, temperature: float = 0.3, max_retries: int = 3) -> str:
        """è°ƒç”¨LLM"""
        logger.info(f"å¼€å§‹è°ƒç”¨LLM: model={self.model}, temperature={temperature}, max_retries={max_retries}")
        logger.debug(f"prompté•¿åº¦: {len(prompt)} å­—ç¬¦")

        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }

        data = {
            "model": self.model,
            "messages": [
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä½èµ„æ·±çš„äº§å“åˆ†æå¸ˆå’ŒæŠ€æœ¯é¡¾é—®ï¼Œä¸“é—¨åˆ†æç”¨æˆ·ç—›ç‚¹å¹¶è®¾è®¡åˆ›æ–°çš„è§£å†³æ–¹æ¡ˆã€‚"},
                {"role": "user", "content": prompt}
            ],
            "temperature": temperature,
            "max_tokens": 4000
        }

        for attempt in range(max_retries):
            try:
                print(f"  ğŸ¤– è°ƒç”¨LLM (å°è¯• {attempt + 1}/{max_retries})...")
                logger.info(f"å°è¯•ç¬¬ {attempt + 1}/{max_retries} æ¬¡LLMè°ƒç”¨")

                url = f"{self.base_url}/chat/completions"
                logger.debug(f"è¯·æ±‚URL: {url}")

                response = requests.post(
                    url,
                    headers=headers,
                    json=data,
                    timeout=180  # å¢åŠ åˆ°3åˆ†é’Ÿ
                )

                logger.debug(f"å“åº”çŠ¶æ€ç : {response.status_code}")

                response.raise_for_status()
                result = response.json()

                if 'choices' not in result or len(result['choices']) == 0:
                    logger.error("LLMå“åº”ä¸­æ²¡æœ‰choiceså­—æ®µ")
                    return "LLMå“åº”æ ¼å¼é”™è¯¯: æ²¡æœ‰choices"

                content = result['choices'][0]['message']['content'].strip()
                logger.debug(f"LLMå“åº”é•¿åº¦: {len(content)} å­—ç¬¦")
                print(f"  âœ… LLMå“åº”æˆåŠŸ")
                logger.info("LLMè°ƒç”¨æˆåŠŸ")
                return content

            except requests.exceptions.Timeout:
                error_msg = f"LLMè°ƒç”¨è¶…æ—¶ (å°è¯• {attempt + 1}/{max_retries})"
                logger.warning(error_msg)
                print(f"  âš ï¸ {error_msg}")
                if attempt < max_retries - 1:
                    continue
                logger.error(f"LLMè°ƒç”¨è¶…æ—¶ï¼Œå·²é‡è¯•{max_retries}æ¬¡")
                return f"LLMè°ƒç”¨è¶…æ—¶: å·²é‡è¯•{max_retries}æ¬¡"

            except requests.exceptions.HTTPError as e:
                error_msg = f"HTTPé”™è¯¯: {e}"
                logger.error(error_msg)
                logger.error(f"å“åº”å†…å®¹: {response.text if 'response' in locals() else 'N/A'}")
                print(f"  âŒ {error_msg}")
                if attempt < max_retries - 1:
                    print(f"  ğŸ”„ æ­£åœ¨é‡è¯•...")
                    continue
                return f"LLM HTTPé”™è¯¯: {str(e)}"

            except Exception as e:
                error_msg = f"LLMè°ƒç”¨å¤±è´¥: {e}"
                logger.error(error_msg)
                import traceback
                logger.error(traceback.format_exc())
                print(f"  âŒ {error_msg}")
                if attempt < max_retries - 1:
                    print(f"  ğŸ”„ æ­£åœ¨é‡è¯•...")
                    continue
                return f"LLMè°ƒç”¨å¤±è´¥: {str(e)}"

    def get_top_clusters(self, min_score: float = 0.8, limit: int = 10) -> List[Dict]:
        """è·å–é«˜åˆ†èšç±» - ä½¿ç”¨ç»Ÿä¸€æ•°æ®åº“"""
        logger.info(f"è·å–é«˜åˆ†èšç±»: min_score={min_score}, limit={limit}")
        clusters = []

        try:
            with self.get_db_connection("clusters") as conn:
                cursor = conn.cursor()

                logger.debug("æ‰§è¡Œèšç±»æŸ¥è¯¢SQL...")

                cursor.execute("""
                    SELECT c.id, c.cluster_name, c.cluster_description, c.avg_pain_score,
                           c.cluster_size, c.pain_event_ids,
                           COUNT(o.id) as opportunity_count,
                           MAX(o.total_score) as max_opportunity_score,
                           GROUP_CONCAT(o.opportunity_name, ' | ') as opportunity_names
                    FROM clusters c
                    LEFT JOIN opportunities o ON c.id = o.cluster_id
                    GROUP BY c.id
                    HAVING opportunity_count > 0 AND max_opportunity_score >= ?
                    ORDER BY max_opportunity_score DESC, c.avg_pain_score DESC
                    LIMIT ?
                """, (min_score, limit))

                logger.debug(f"æŸ¥è¯¢æ‰§è¡Œå®Œæˆï¼Œå¼€å§‹å¤„ç†ç»“æœ...")
                rows = cursor.fetchall()
                logger.info(f"æŸ¥è¯¢åˆ° {len(rows)} ä¸ªèšç±»")

                for i, row in enumerate(rows, 1):
                    logger.debug(f"å¤„ç†ç¬¬ {i}/{len(rows)} ä¸ªèšç±»: {row['cluster_name'][:50]}...")
                    # è·å–è¯¥èšç±»çš„æ‰€æœ‰æœºä¼š
                    logger.debug(f"è·å–èšç±» {row['id']} çš„æœºä¼šæ•°æ®...")
                    cursor.execute("""
                        SELECT opportunity_name, description, total_score, recommendation,
                               current_tools, missing_capability, why_existing_fail,
                               target_users, killer_risks
                        FROM opportunities
                        WHERE cluster_id = ?
                        ORDER BY total_score DESC
                    """, (row['id'],))

                    opportunities = []
                    opp_rows = cursor.fetchall()
                    logger.debug(f"èšç±» {row['id']} æœ‰ {len(opp_rows)} ä¸ªæœºä¼š")

                    for opp_row in opp_rows:
                        opportunities.append({
                            'name': opp_row['opportunity_name'],
                            'description': opp_row['description'],
                            'score': opp_row['total_score'],
                            'recommendation': opp_row['recommendation'],
                            'current_tools': opp_row['current_tools'],
                            'missing_capability': opp_row['missing_capability'],
                            'why_existing_fail': opp_row['why_existing_fail'],
                            'target_users': opp_row['target_users'],
                            'killer_risks': json.loads(opp_row['killer_risks']) if opp_row['killer_risks'] else []
                        })

                    # è·å–ç—›ç‚¹äº‹ä»¶æ ·æœ¬
                    try:
                        pain_event_ids = json.loads(row['pain_event_ids'])
                        logger.debug(f"èšç±» {row['id']} ç—›ç‚¹äº‹ä»¶IDs: {len(pain_event_ids)} ä¸ª")
                        sample_pains = self.get_sample_pain_events(pain_event_ids[:5])
                    except json.JSONDecodeError as e:
                        logger.warning(f"èšç±» {row['id']} pain_event_ids JSONè§£æå¤±è´¥: {e}")
                        sample_pains = []

                    clusters.append({
                        'id': row['id'],
                        'name': row['cluster_name'],
                        'description': row['cluster_description'],
                        'avg_pain_score': row['avg_pain_score'],
                        'cluster_size': row['cluster_size'],
                        'opportunity_count': row['opportunity_count'],
                        'max_opportunity_score': row['max_opportunity_score'],
                        'opportunities': opportunities,
                        'sample_pains': sample_pains
                    })

            logger.info(f"æˆåŠŸè·å– {len(clusters)} ä¸ªèšç±»æ•°æ®")
            return clusters

        except Exception as e:
            logger.error(f"è·å–èšç±»æ•°æ®å¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return []

    def get_sample_pain_events(self, pain_event_ids: List[int]) -> List[Dict]:
        """è·å–ç—›ç‚¹äº‹ä»¶æ ·æœ¬ - ä½¿ç”¨ç»Ÿä¸€æ•°æ®åº“"""
        logger.debug(f"è·å– {len(pain_event_ids)} ä¸ªç—›ç‚¹äº‹ä»¶æ ·æœ¬: {pain_event_ids}")
        pains = []

        if not pain_event_ids:
            logger.warning("pain_event_ids ä¸ºç©ºï¼Œè¿”å›ç©ºåˆ—è¡¨")
            return []

        try:
            with self.get_db_connection("pain") as conn:
                cursor = conn.cursor()

                placeholders = ','.join(['?' for _ in pain_event_ids])
                logger.debug(f"æ‰§è¡Œç—›ç‚¹äº‹ä»¶æŸ¥è¯¢ï¼ŒIDs: {pain_event_ids}")

                cursor.execute(f"""
                    SELECT problem, current_workaround, frequency, emotional_signal, mentioned_tools
                    FROM pain_events
                    WHERE id IN ({placeholders})
                """, pain_event_ids)

                rows = cursor.fetchall()
                logger.debug(f"æŸ¥è¯¢åˆ° {len(rows)} ä¸ªç—›ç‚¹äº‹ä»¶")

                for row in rows:
                    pains.append({
                        'problem': row['problem'],
                        'workaround': row['current_workaround'],
                        'frequency': row['frequency'],
                        'emotion': row['emotional_signal'],
                        'tools': row['mentioned_tools']
                    })

            logger.debug(f"æˆåŠŸè·å– {len(pains)} ä¸ªç—›ç‚¹äº‹ä»¶")
            return pains

        except Exception as e:
            logger.error(f"è·å–ç—›ç‚¹äº‹ä»¶å¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return []

    def generate_basic_analysis(self, cluster: Dict) -> str:
        """ç”ŸæˆåŸºç¡€åˆ†æï¼ˆå½“LLMè°ƒç”¨å¤±è´¥æ—¶ï¼‰"""
        pain_context = "\n".join([
            f"â€¢ {pain['problem']}" + chr(10) + f"  å½“å‰è§£å†³æ–¹æ¡ˆ: {pain['workaround']}" + chr(10) + f"  å‘ç”Ÿé¢‘ç‡: {pain['frequency']}" + chr(10) + f"  æƒ…ç»ªä¿¡å·: {pain['emotion']}"
            for pain in cluster['sample_pains']
        ])

        opp_analysis = ""
        for opp in cluster['opportunities'][:3]:
            opp_analysis += f"""
### {opp['name']} (è¯„åˆ†: {opp['score']:.2f})

**é—®é¢˜æè¿°**: {opp['description']}

**å…³é”®æœºä¼šåˆ†æ**:
- å¸‚åœºéœ€æ±‚: é€šè¿‡{cluster['cluster_size']}ä¸ªç›¸å…³å¸–å­éªŒè¯äº†å¼ºçƒˆéœ€æ±‚
- ç›®æ ‡ç”¨æˆ·: {opp['target_users'] or 'ä¸­å°ä¼ä¸šã€ä¸ªäººå¼€å‘è€…ã€è‡ªç”±èŒä¸šè€…'}
- ç«äº‰ä¼˜åŠ¿: {opp['missing_capability'] or 'å¡«è¡¥ç°æœ‰å·¥å…·çš„åŠŸèƒ½ç©ºç™½'}

**MVPåŠŸèƒ½å»ºè®®**:
1. æ ¸å¿ƒåŠŸèƒ½å®ç°{opp['current_tools'] and f"ï¼Œæ•´åˆ{opp['current_tools']}çš„å·¥ä½œæµ"}
2. ç®€åŒ–ç”¨æˆ·ç•Œé¢ï¼Œé™ä½å­¦ä¹ æˆæœ¬
3. å¿«é€Ÿéƒ¨ç½²å’Œé›†æˆèƒ½åŠ›

**å•†ä¸šåŒ–å»ºè®®**:
- å…è´¹åŸºç¡€ç‰ˆå¸å¼•åˆå§‹ç”¨æˆ·
- Proç‰ˆæœ¬æœˆè´¹$10-20
- ä¼ä¸šå®šåˆ¶ç‰ˆæ”¯æŒ
"""

        return f"""## ç—›ç‚¹æ·±åº¦åˆ†æ

### æ ¸å¿ƒé—®é¢˜
{cluster['description']}

### å½±å“èŒƒå›´
- å—å½±å“ç”¨æˆ·ç¾¤ä½“: {cluster['cluster_size']}ä¸ªçœŸå®ç”¨æˆ·åé¦ˆ
- ç—›ç‚¹å¼ºåº¦: {cluster['avg_pain_score']:.2f}/1.0

### å…¸å‹ç—›ç‚¹äº‹ä»¶
{pain_context}

## å¸‚åœºæœºä¼šè¯„ä¼°

### å¸‚åœºè§„æ¨¡
åŸºäºRedditè®¨è®ºçƒ­åº¦ï¼Œè¯¥é—®é¢˜å½±å“äº†å¤§é‡ç”¨æˆ·ï¼Œå…·æœ‰æ˜ç¡®çš„ä»˜è´¹æ„æ„¿ã€‚

### æœºä¼šæ•°é‡
å·²è¯†åˆ«{cluster['opportunity_count']}ä¸ªå…·ä½“æœºä¼šï¼Œæœ€é«˜è¯„åˆ†{cluster['max_opportunity_score']:.2f}

## äº§å“è®¾è®¡æ–¹æ¡ˆ

{opp_analysis}

## å¯æ‰§è¡Œè¡ŒåŠ¨è®¡åˆ’

### ç«‹å³è¡ŒåŠ¨ï¼ˆ1ä¸ªæœˆå†…ï¼‰
1. éªŒè¯ç›®æ ‡ç”¨æˆ·éœ€æ±‚ï¼Œè¿›è¡Œæ·±åº¦ç”¨æˆ·è®¿è°ˆ
2. å¼€å‘æœ€å°å¯è¡Œäº§å“(MVP)åŸå‹
3. å»ºç«‹ç”¨æˆ·åé¦ˆæ¸ é“

### çŸ­æœŸç›®æ ‡ï¼ˆ3ä¸ªæœˆå†…ï¼‰
1. å‘å¸ƒMVPç‰ˆæœ¬å¹¶è·å–é¦–æ‰¹100ä¸ªç”¨æˆ·
2. åŸºäºåé¦ˆè¿­ä»£äº§å“åŠŸèƒ½
3. æ¢ç´¢ç›ˆåˆ©æ¨¡å¼

### æˆåŠŸæŒ‡æ ‡
- ç”¨æˆ·ç•™å­˜ç‡ > 60%
- æœˆæ´»è·ƒç”¨æˆ·å¢é•¿ > 20%
- NPSå¾—åˆ† > 40
"""

    def analyze_cluster(self, cluster: Dict) -> str:
        """åˆ†æå•ä¸ªèšç±»å¹¶ç”Ÿæˆå®Œæ•´æŠ¥å‘Š"""

        # æ„å»ºåˆ†æprompt
        pain_context = "\n".join([
            f"â€¢ {pain['problem']} (å½“å‰è§£å†³æ–¹æ¡ˆ: {pain['workaround']}, é¢‘ç‡: {pain['frequency']}, æƒ…ç»ª: {pain['emotion']})"
            for pain in cluster['sample_pains']
        ])

        opportunities_context = "\n".join([
            f"â€¢ {opp['name']} (è¯„åˆ†: {opp['score']:.2f})"
            f"  æè¿°: {opp['description'][:100]}..."
            for opp in cluster['opportunities'][:3]
        ])

        prompt = f"""
è¯·åˆ†æä»¥ä¸‹ç—›ç‚¹èšç±»å¹¶ç”Ÿæˆç»¼åˆæŠ¥å‘Šï¼š

## èšç±»ä¿¡æ¯
- **èšç±»åç§°**: {cluster['name']}
- **èšç±»æè¿°**: {cluster['description']}
- **ç—›ç‚¹æ•°é‡**: {cluster['cluster_size']}
- **å¹³å‡ç—›ç‚¹å¼ºåº¦**: {cluster['avg_pain_score']:.2f}
- **æœºä¼šæ•°é‡**: {cluster['opportunity_count']}

## å…¸å‹ç—›ç‚¹æ ·æœ¬
{pain_context}

## å·²è¯†åˆ«çš„æœºä¼š
{opportunities_context}

## åˆ†æè¦æ±‚
è¯·æŒ‰ç…§ä»¥ä¸‹ç»“æ„ç”Ÿæˆè¯¦ç»†åˆ†ææŠ¥å‘Šï¼š

### 1. ç—›ç‚¹æ·±åº¦åˆ†æ
- æ ¸å¿ƒé—®é¢˜æœ¬è´¨
- å½±å“èŒƒå›´å’Œä¸¥é‡ç¨‹åº¦
- ç”¨æˆ·ç‰¹å¾å’Œä½¿ç”¨åœºæ™¯
- ç°æœ‰è§£å†³æ–¹æ¡ˆçš„ä¸è¶³

### 2. å¸‚åœºæœºä¼šè¯„ä¼°
- å¸‚åœºè§„æ¨¡ä¼°ç®—
- ç”¨æˆ·ä»˜è´¹æ„æ„¿
- ç«äº‰æ ¼å±€åˆ†æ
- è¿›å…¥å£å’è¯„ä¼°

### 3. äº§å“è®¾è®¡æ–¹æ¡ˆ
- MVPåŠŸèƒ½å®šä¹‰
- æŠ€æœ¯æ¶æ„å»ºè®®
- ç”¨æˆ·ä½“éªŒè®¾è®¡è¦ç‚¹
- å·®å¼‚åŒ–ç«äº‰ç­–ç•¥

### 4. å•†ä¸šåŒ–è·¯å¾„
- ç›ˆåˆ©æ¨¡å¼è®¾è®¡
- è·å®¢ç­–ç•¥
- å®šä»·ç­–ç•¥
- å‘å±•è·¯çº¿å›¾

### 5. å¯æ‰§è¡Œè¡ŒåŠ¨è®¡åˆ’
- è¿‘æœŸè¡ŒåŠ¨é¡¹ï¼ˆ1-3ä¸ªæœˆï¼‰
- ä¸­æœŸç›®æ ‡ï¼ˆ3-6ä¸ªæœˆï¼‰
- å…³é”®æˆåŠŸæŒ‡æ ‡
- é£é™©åº”å¯¹æªæ–½

è¯·ç¡®ä¿åˆ†ææ·±å…¥ã€å…·ä½“ä¸”å¯æ“ä½œã€‚ä½¿ç”¨markdownæ ¼å¼è¾“å‡ºã€‚
"""

        print(f"ğŸ¤– æ­£åœ¨åˆ†æèšç±»: {cluster['name'][:50]}...")

        # å°è¯•è°ƒç”¨LLM
        analysis = self.call_llm(prompt, temperature=0.4)

        # å¦‚æœLLMè°ƒç”¨å¤±è´¥ï¼Œä½¿ç”¨åŸºç¡€åˆ†æ
        if "LLMè°ƒç”¨" in analysis:
            print(f"  âš ï¸ ä½¿ç”¨åŸºç¡€åˆ†ææ›¿ä»£")
            analysis = self.generate_basic_analysis(cluster)

        return analysis

    def generate_cluster_report(self, cluster: Dict, analysis: str) -> str:
        """ç”Ÿæˆèšç±»æŠ¥å‘Šæ–‡ä»¶"""
        logger.info(f"ç”Ÿæˆèšç±»æŠ¥å‘Š: {cluster['name'][:50]}...")

        # æ¸…ç†æ–‡ä»¶å
        safe_name = re.sub(r'[^\w\s-]', '', cluster['name']).strip()
        safe_name = re.sub(r'[-\s]+', '_', safe_name)
        filename = f"{safe_name}_opportunity_analysis.md"
        filepath = os.path.join(self.output_dir, filename)

        logger.debug(f"æŠ¥å‘Šæ–‡ä»¶è·¯å¾„: {filepath}")

        # æ„å»ºå®Œæ•´æŠ¥å‘Š
        report_content = f"""# {cluster['name']} - æœºä¼šåˆ†ææŠ¥å‘Š

> **ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
> **èšç±»ID**: {cluster['id']}
> **ç—›ç‚¹æ•°é‡**: {cluster['cluster_size']}
> **å¹³å‡ç—›ç‚¹å¼ºåº¦**: {cluster['avg_pain_score']:.2f}
> **æœºä¼šæ•°é‡**: {cluster['opportunity_count']}

---

## ğŸ“Š èšç±»æ¦‚è§ˆ

**èšç±»æè¿°**: {cluster['description']}

### ğŸ¯ é¡¶çº§æœºä¼š
{chr(10).join([f"- **{opp['name']}** (è¯„åˆ†: {opp['score']:.2f})" for opp in cluster['opportunities'][:5]])}

---

## ğŸ” æ·±åº¦åˆ†æ

{analysis}

---

## ğŸ“‹ åŸå§‹æ•°æ®

### å…¸å‹ç—›ç‚¹äº‹ä»¶
{chr(10).join([f"**é—®é¢˜**: {pain['problem']}" + chr(10) + f"- å½“å‰æ–¹æ¡ˆ: {pain['workaround']}" + chr(10) + f"- å‘ç”Ÿé¢‘ç‡: {pain['frequency']}" + chr(10) + f"- æƒ…ç»ªä¿¡å·: {pain['emotion']}" + chr(10) for pain in cluster['sample_pains']])}

### å·²è¯†åˆ«æœºä¼šè¯¦æƒ…
{chr(10).join([f"**{opp['name']}** (è¯„åˆ†: {opp['score']:.2f})" + chr(10) + f"- æè¿°: {opp['description']}" + chr(10) + f"- æ¨èå»ºè®®: {opp['recommendation']}" + chr(10) + (f"- ç›®æ ‡ç”¨æˆ·: {opp['target_users']}" if opp['target_users'] else "") + chr(10) for opp in cluster['opportunities']])}

---

*æœ¬æŠ¥å‘Šç”± Reddit Pain Point Finder è‡ªåŠ¨ç”Ÿæˆ*
"""

        # å†™å…¥æ–‡ä»¶
        try:
            logger.debug(f"å¼€å§‹å†™å…¥æŠ¥å‘Šæ–‡ä»¶: {filepath}")
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(report_content)
            logger.info(f"æŠ¥å‘Šç”ŸæˆæˆåŠŸ: {filepath}")
            print(f"âœ… æŠ¥å‘Šå·²ç”Ÿæˆ: {filepath}")
            return filepath
        except Exception as e:
            logger.error(f"æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {filepath}, é”™è¯¯: {e}")
            import traceback
            logger.error(traceback.format_exc())
            print(f"âŒ æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")
            return None

    def generate_summary_index(self, report_files: List[str]) -> str:
        """ç”Ÿæˆæ€»ç»“ç´¢å¼•æ–‡ä»¶"""
        logger.info(f"ç”Ÿæˆæ€»ç»“ç´¢å¼•ï¼ŒåŒ…å« {len(report_files)} ä¸ªæŠ¥å‘Š")

        index_content = f"""# ç—›ç‚¹æœºä¼šåˆ†ææŠ¥å‘Šç´¢å¼•

> **ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
> **åˆ†ææ•°é‡**: {len(report_files)}

---

## ğŸ“ˆ åˆ†ææ¦‚è§ˆ

æœ¬æ¬¡å…±åˆ†æäº† {len(report_files)} ä¸ªé«˜ä»·å€¼ç—›ç‚¹èšç±»ï¼Œæ¯ä¸ªèšç±»éƒ½åŒ…å«è¯¦ç»†çš„ç—›ç‚¹åˆ†æã€åº”ç”¨è®¾è®¡æ–¹æ¡ˆå’Œå¯æ‰§è¡Œè¡ŒåŠ¨è®¡åˆ’ã€‚

---

## ğŸ“‹ æŠ¥å‘Šåˆ—è¡¨

{chr(10).join([f"- [{os.path.basename(f)}]({f})" for f in report_files])}

---

## ğŸ¯ ä¸‹ä¸€æ­¥è¡ŒåŠ¨å»ºè®®

1. **ä¼˜å…ˆçº§æ’åº**: æ ¹æ®æœºä¼šè¯„åˆ†å’Œå¸‚åœºè§„æ¨¡ç¡®å®šäº§å“å¼€å‘ä¼˜å…ˆçº§
2. **ç”¨æˆ·éªŒè¯**: é’ˆå¯¹Top 3æœºä¼šè¿›è¡Œç”¨æˆ·è®¿è°ˆå’Œéœ€æ±‚éªŒè¯
3. **MVPå¼€å‘**: é€‰æ‹©æœ€é«˜ä»·å€¼çš„æœºä¼šå¯åŠ¨MVPå¼€å‘
4. **æŒç»­ç›‘æ§**: å®šæœŸæ›´æ–°Redditæ•°æ®ï¼Œè·Ÿè¸ªæ–°çš„ç—›ç‚¹è¶‹åŠ¿

---

*ä½¿ç”¨æ–¹æ³•: ç‚¹å‡»ä¸Šæ–¹é“¾æ¥æŸ¥çœ‹å…·ä½“çš„æœºä¼šåˆ†ææŠ¥å‘Š*
"""

        index_path = os.path.join(self.output_dir, "README.md")
        try:
            logger.debug(f"å¼€å§‹å†™å…¥ç´¢å¼•æ–‡ä»¶: {index_path}")
            with open(index_path, 'w', encoding='utf-8') as f:
                f.write(index_content)
            logger.info(f"ç´¢å¼•æ–‡ä»¶ç”ŸæˆæˆåŠŸ: {index_path}")
            print(f"ğŸ“‘ ç´¢å¼•æ–‡ä»¶å·²ç”Ÿæˆ: {index_path}")
            return index_path
        except Exception as e:
            logger.error(f"ç´¢å¼•æ–‡ä»¶ç”Ÿæˆå¤±è´¥: {index_path}, é”™è¯¯: {e}")
            import traceback
            logger.error(traceback.format_exc())
            print(f"âŒ ç´¢å¼•æ–‡ä»¶ç”Ÿæˆå¤±è´¥: {e}")
            return None

    def run_analysis(self, min_score: float = 0.8, limit: int = 10):
        """è¿è¡Œå®Œæ•´åˆ†æ"""
        logger.info(f"å¼€å§‹è¿è¡Œå®Œæ•´åˆ†æ: min_score={min_score}, limit={limit}")
        print(f"\nğŸš€ å¼€å§‹ç—›ç‚¹æœºä¼šåˆ†æ...")
        print(f"   â€¢ æœ€ä½æœºä¼šè¯„åˆ†: {min_score}")
        print(f"   â€¢ æœ€å¤§åˆ†ææ•°é‡: {limit}")
        print("="*60)

        # è·å–èšç±»æ•°æ®
        logger.info("å¼€å§‹è·å–èšç±»æ•°æ®...")
        clusters = self.get_top_clusters(min_score, limit)
        if not clusters:
            logger.warning("æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„èšç±»æ•°æ®")
            print("âŒ æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„èšç±»æ•°æ®")
            return

        logger.info(f"æˆåŠŸè·å– {len(clusters)} ä¸ªèšç±»")
        print(f"ğŸ“Š æ‰¾åˆ° {len(clusters)} ä¸ªé«˜ä»·å€¼èšç±»")

        # åˆ†ææ¯ä¸ªèšç±»
        report_files = []
        for i, cluster in enumerate(clusters, 1):
            logger.info(f"å¼€å§‹åˆ†æç¬¬ {i}/{len(clusters)} ä¸ªèšç±»: {cluster['name'][:50]}...")
            print(f"\n[{i}/{len(clusters)}] åˆ†æèšç±»: {cluster['name'][:50]}...")

            # æ‰§è¡Œåˆ†æ
            logger.debug("æ‰§è¡Œèšç±»åˆ†æ...")
            analysis = self.analyze_cluster(cluster)

            # ç”ŸæˆæŠ¥å‘Š
            logger.debug("ç”Ÿæˆèšç±»æŠ¥å‘Š...")
            report_path = self.generate_cluster_report(cluster, analysis)
            if report_path:
                report_files.append(report_path)
                logger.info(f"æŠ¥å‘Šå·²æ·»åŠ åˆ°åˆ—è¡¨: {report_path}")

            logger.info(f"èšç±» {i} åˆ†æå®Œæˆ")
            print(f"âœ… å®Œæˆ: {cluster['name'][:50]}")

        # ç”Ÿæˆç´¢å¼•æ–‡ä»¶
        if report_files:
            logger.info("å¼€å§‹ç”Ÿæˆæ€»ç»“ç´¢å¼•...")
            self.generate_summary_index(report_files)

        logger.info(f"åˆ†æå®Œæˆï¼Œç”Ÿæˆäº† {len(report_files)} ä¸ªæŠ¥å‘Š")
        print(f"\nğŸ‰ åˆ†æå®Œæˆï¼")
        print(f"   â€¢ ç”ŸæˆæŠ¥å‘Š: {len(report_files)} ä»½")
        print(f"   â€¢ è¾“å‡ºç›®å½•: {self.output_dir}")
        print(f"   â€¢ æŸ¥çœ‹ç´¢å¼•: {self.output_dir}/README.md")


def main():
    """ä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description="Redditç—›ç‚¹æœºä¼šåˆ†æå™¨")
    parser.add_argument("--min-score", type=float, default=0.8, help="æœ€ä½æœºä¼šè¯„åˆ†")
    parser.add_argument("--limit", type=int, default=15, help="æœ€å¤§åˆ†ææ•°é‡")
    parser.add_argument("--legacy-db", action="store_true", help="ä½¿ç”¨æ—§çš„å¤šæ•°æ®åº“æ¨¡å¼")
    parser.add_argument("--dry-run", action="store_true", help="è¯•è¿è¡Œæ¨¡å¼ï¼ˆä»…è·å–æ•°æ®ï¼Œä¸ç”ŸæˆæŠ¥å‘Šï¼‰")

    args = parser.parse_args()

    logger.info("=" * 50)
    logger.info("ç—›ç‚¹åˆ†æå™¨å¼€å§‹è¿è¡Œ")
    logger.info(f"æ•°æ®åº“æ¨¡å¼: {'å¤šæ•°æ®åº“æ–‡ä»¶' if args.legacy_db else 'ç»Ÿä¸€æ•°æ®åº“'}")
    logger.info(f"æœ€ä½è¯„åˆ†: {args.min_score}, æœ€å¤§æ•°é‡: {args.limit}")
    logger.info("=" * 50)

    try:
        logger.info("åˆå§‹åŒ– PainPointAnalyzer...")
        analyzer = PainPointAnalyzer(unified_db=not args.legacy_db)

        if args.dry_run:
            # è¯•è¿è¡Œï¼šä»…è·å–æ•°æ®å¹¶æ˜¾ç¤º
            logger.info("è¯•è¿è¡Œæ¨¡å¼ï¼šè·å–èšç±»æ•°æ®...")
            clusters = analyzer.get_top_clusters(min_score=args.min_score, limit=args.limit)
            logger.info(f"æ‰¾åˆ° {len(clusters)} ä¸ªèšç±»")

            print(f"\nğŸ“Š è¯•è¿è¡Œç»“æœï¼š")
            print(f"æ‰¾åˆ° {len(clusters)} ä¸ªç¬¦åˆæ¡ä»¶çš„èšç±»ï¼š")
            for i, cluster in enumerate(clusters, 1):
                print(f"  {i}. {cluster['name']} (è¯„åˆ†: {cluster['max_opportunity_score']:.2f}, æœºä¼šæ•°: {cluster['opportunity_count']})")
            return

        logger.info("å¼€å§‹è¿è¡Œåˆ†æ...")
        analyzer.run_analysis(min_score=args.min_score, limit=args.limit)
        logger.info("ç¨‹åºæ‰§è¡Œå®Œæˆ")
    except Exception as e:
        logger.error(f"ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        logger.error(traceback.format_exc())
        print(f"âŒ ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        traceback.print_exc()


if __name__ == "__main__":
    main()
```


================================================================================
æ–‡ä»¶: analyze_pain_points.py
================================================================================

```python
#!/usr/bin/env python3
"""
å¿«é€Ÿå¯åŠ¨ç—›ç‚¹åˆ†æè„šæœ¬
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from pain_point_analyzer import PainPointAnalyzer

if __name__ == "__main__":
    print("ğŸ¯ Reddit Pain Point Finder - ç—›ç‚¹æœºä¼šåˆ†æå™¨")
    print("=" * 60)

    # å¯ä»¥é€šè¿‡å‘½ä»¤è¡Œå‚æ•°è°ƒæ•´
    min_score = 0.8
    limit = 15

    if len(sys.argv) > 1:
        try:
            min_score = float(sys.argv[1])
        except:
            pass

    if len(sys.argv) > 2:
        try:
            limit = int(sys.argv[2])
        except:
            pass

    print(f"å‚æ•°è®¾ç½®:")
    print(f"  â€¢ æœ€ä½æœºä¼šè¯„åˆ†: {min_score}")
    print(f"  â€¢ æœ€å¤§åˆ†ææ•°é‡: {limit}")
    print()

    try:
        analyzer = PainPointAnalyzer()
        analyzer.run_analysis(min_score=min_score, limit=limit)
    except KeyboardInterrupt:
        print("\n\nâš ï¸  åˆ†æè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ åˆ†æå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
```


================================================================================
æ–‡ä»¶: pipeline/align_cross_sources.py
================================================================================

```python
"""
è·¨æºå¯¹é½æ¨¡å—
Cross-Source Alignment Module
"""
import json
import time
from typing import List, Dict, Any, Optional
from utils.db import WiseCollectionDB
from utils.llm_client import LLMClient
import logging

logger = logging.getLogger(__name__)

class CrossSourceAligner:
    """è·¨æºå¯¹é½å™¨ - è¯†åˆ«ä¸åŒç¤¾åŒºè®¨è®ºçš„åŒä¸€é—®é¢˜"""

    def __init__(self, db: WiseCollectionDB, llm_client: LLMClient):
        self.db = db
        self.llm_client = llm_client

    def get_unprocessed_clusters(self) -> List[Dict]:
        """è·å–æœªå¤„ç†çš„èšç±»æ•°æ®"""
        try:
            with self.db.get_connection("clusters") as conn:
                cursor = conn.execute("""
                    SELECT cluster_name, source_type, centroid_summary,
                           common_pain, pain_event_ids, cluster_size
                    FROM clusters
                    WHERE alignment_status = 'unprocessed'
                    AND cluster_size >= 3
                    ORDER BY cluster_size DESC
                """)
                return [dict(row) for row in cursor.fetchall()]
        except Exception as e:
            logger.error(f"Failed to get unprocessed clusters: {e}")
            return []

    def prepare_cluster_for_alignment(self, cluster: Dict) -> Dict:
        """ä¸ºLLMå¯¹é½å‡†å¤‡èšç±»æ•°æ®"""
        try:
            # æå–workaroundä¿¡æ¯
            typical_workaround = self._extract_workarounds(cluster["common_pain"])

            return {
                "source_type": cluster["source_type"],
                "cluster_summary": cluster["centroid_summary"],
                "typical_workaround": typical_workaround,
                "context": f"Cluster size: {cluster['cluster_size']}, "
                          f"Pain events: {len(json.loads(cluster['pain_event_ids']))}"
            }
        except Exception as e:
            logger.error(f"Failed to prepare cluster {cluster.get('cluster_name')}: {e}")
            return {}

    def align_clusters_across_sources(self, clusters: List[Dict]) -> List[Dict]:
        """ä½¿ç”¨LLMè·¨æºå¯¹é½èšç±»"""
        if len(clusters) < 2:
            logger.info("Not enough clusters for cross-source alignment")
            return []

        try:
            # æŒ‰æºç±»å‹åˆ†ç»„
            source_groups = {}
            for cluster in clusters:
                source_type = cluster["source_type"]
                if source_type not in source_groups:
                    source_groups[source_type] = []

                prepared_cluster = self.prepare_cluster_for_alignment(cluster)
                if prepared_cluster:  # Only add if preparation succeeded
                    source_groups[source_type].append(prepared_cluster)

            # è·³è¿‡å¦‚æœåªæœ‰ä¸€ç§æºç±»å‹
            if len(source_groups) < 2:
                logger.info("Only one source type found, skipping cross-source alignment")
                return []

            # æ„å»ºå¯¹é½prompt
            alignment_prompt = self._build_alignment_prompt(source_groups)

            # è°ƒç”¨LLM
            logger.info(f"Running cross-source alignment for {len(source_groups)} source types")
            messages = [{"role": "user", "content": alignment_prompt}]
            response = self.llm_client.chat_completion(
                messages=messages,
                model_type="main",
                max_tokens=2000,
                temperature=0.1
            )

            # è§£æå“åº” - chat_completionè¿”å›å­—å…¸æ ¼å¼
            if isinstance(response, dict):
                response_content = response.get('content', '')
            else:
                response_content = str(response)

            alignments = self._parse_alignment_response(response_content, clusters)
            logger.info(f"Found {len(alignments)} cross-source alignments")

            return alignments

        except Exception as e:
            logger.error(f"Failed to align clusters across sources: {e}")
            return []

    def _extract_workarounds(self, common_pain: str) -> str:
        """ä»common_painä¸­æå–workaroundä¿¡æ¯"""
        # ç®€å•å®ç°ï¼šå¦‚æœæåˆ°workaroundæˆ–solutionï¼Œæå–ç›¸å…³éƒ¨åˆ†
        if not common_pain:
            return "No specific workaround mentioned"

        # æŸ¥æ‰¾åŒ…å«workaroundã€solutionç­‰å…³é”®è¯çš„å¥å­
        pain_lower = common_pain.lower()
        workaround_keywords = ['workaround', 'solution', 'fix', 'solve', 'currently using', 'temporary']

        for keyword in workaround_keywords:
            if keyword in pain_lower:
                # è¿”å›åŒ…å«å…³é”®è¯çš„éƒ¨åˆ†æ–‡æœ¬
                sentences = common_pain.split('.')
                for sentence in sentences:
                    if keyword in sentence.lower():
                        return sentence.strip()

        return "No explicit workaround mentioned"

    def _build_alignment_prompt(self, source_groups: Dict) -> str:
        """æ„å»ºè·¨æºå¯¹é½çš„LLM prompt"""
        prompt = """You are analyzing problem summaries from different online communities to identify when they're discussing the same underlying issue.

You will receive multiple problem clusters grouped by community type:
"""

        # æ·»åŠ æ¯ä¸ªæºç»„
        for source_type, clusters in source_groups.items():
            prompt += f"\n## {source_type.upper()} Communities:\n\n"
            for i, cluster in enumerate(clusters, 1):
                prompt += f"Cluster {i}:\n"
                prompt += f"- Summary: {cluster['cluster_summary']}\n"
                prompt += f"- Typical workaround: {cluster['typical_workaround']}\n"
                prompt += f"- Context: {cluster['context']}\n\n"

        prompt += """
## Task

Identify which clusters from different communities describe the SAME underlying problem.

Rules:
1. Ignore differences in tone, maturity level, or solution sophistication
2. Focus on the core problem being described
3. Consider workarounds and context as evidence
4. Only align clusters from DIFFERENT source types
5. Be conservative - only align when you're confident it's the same problem

## Output Format

For each alignment discovered, output a JSON object with this structure:
{
  "aligned_problem_id": "AP_XX",
  "sources": ["source_type_1", "source_type_2"],
  "core_problem": "Clear description of the shared underlying problem",
  "why_they_look_different": "Explanation of how the same problem appears different across communities",
  "evidence": [
    {
      "source": "hn_ask",
      "cluster_summary": "...",
      "evidence_quote": "Specific evidence from the cluster summary"
    },
    {
      "source": "reddit",
      "cluster_summary": "...",
      "evidence_quote": "Specific evidence from the cluster summary"
    }
  ],
  "original_cluster_ids": ["cluster_id_1", "cluster_id_2"]
}

Return only valid JSON arrays of alignment objects. If no alignments exist, return an empty array.
"""

        return prompt

    def _parse_alignment_response(self, response: str, original_clusters: List[Dict]) -> List[Dict]:
        """è§£æLLMå“åº”ä¸ºå¯¹é½é—®é¢˜"""
        try:
            # æå–JSONéƒ¨åˆ†
            json_start = response.find('[')
            json_end = response.rfind(']') + 1

            if json_start == -1 or json_end == 0:
                logger.warning("No JSON array found in alignment response")
                return []

            json_str = response[json_start:json_end]
            alignments = json.loads(json_str)

            # éªŒè¯å’Œä¸°å¯Œå¯¹é½ç»“æœ
            validated_alignments = []

            for alignment in alignments:
                # éªŒè¯å¿…éœ€å­—æ®µ
                required_fields = [
                    'aligned_problem_id', 'sources', 'core_problem',
                    'why_they_look_different', 'evidence'
                ]

                if not all(field in alignment for field in required_fields):
                    logger.warning(f"Alignment missing required fields: {alignment}")
                    continue

                # å¤„ç†cluster_idså­—æ®µ
                if 'original_cluster_ids' not in alignment:
                    alignment['cluster_ids'] = []
                else:
                    alignment['cluster_ids'] = alignment['original_cluster_ids']

                # éªŒè¯sourcesæ ¼å¼
                if not isinstance(alignment['sources'], list) or len(alignment['sources']) < 2:
                    logger.warning(f"Invalid sources in alignment: {alignment['sources']}")
                    continue

                validated_alignments.append(alignment)

            return validated_alignments

        except (json.JSONDecodeError, KeyError) as e:
            logger.error(f"Error parsing alignment response: {e}")
            return []

    def process_alignments(self):
        """å¤„ç†æ‰€æœ‰è·¨æºå¯¹é½çš„ä¸»è¦æ–¹æ³•"""
        logger.info("Starting cross-source alignment...")

        # è·å–æœªå¤„ç†çš„èšç±»
        clusters = self.get_unprocessed_clusters()
        logger.info(f"Found {len(clusters)} clusters to analyze")

        if not clusters:
            logger.info("No unprocessed clusters found")
            return

        # æ‰§è¡Œå¯¹é½
        alignments = self.align_clusters_across_sources(clusters)
        logger.info(f"Found {len(alignments)} cross-source alignments")

        # ä¿å­˜å¯¹é½ç»“æœåˆ°æ•°æ®åº“
        for alignment in alignments:
            try:
                # ç”Ÿæˆå”¯ä¸€ID
                alignment['id'] = f"aligned_{alignment['aligned_problem_id']}_{int(time.time())}"

                # æ’å…¥å¯¹é½é—®é¢˜
                self._insert_aligned_problem(alignment)

                # æ›´æ–°èšç±»çŠ¶æ€
                for cluster_id in alignment['cluster_ids']:
                    self._update_cluster_alignment_status(
                        cluster_id,
                        'aligned',
                        alignment['aligned_problem_id']
                    )

            except Exception as e:
                logger.error(f"Failed to save alignment {alignment.get('aligned_problem_id')}: {e}")

        # æ ‡è®°å‰©ä½™èšç±»ä¸ºå·²å¤„ç†ä½†æœªå¯¹é½
        aligned_cluster_ids = []
        for alignment in alignments:
            aligned_cluster_ids.extend(alignment['cluster_ids'])

        for cluster in clusters:
            if cluster['cluster_name'] not in aligned_cluster_ids:
                try:
                    self._update_cluster_alignment_status(
                        cluster['cluster_name'],
                        'processed',
                        None
                    )
                except Exception as e:
                    logger.error(f"Failed to update cluster {cluster['cluster_name']}: {e}")

        logger.info("Cross-source alignment completed!")

    def _insert_aligned_problem(self, alignment_data: Dict):
        """æ’å…¥å¯¹é½é—®é¢˜åˆ°æ•°æ®åº“"""
        try:
            self.db.insert_aligned_problem(alignment_data)
        except Exception as e:
            logger.error(f"Failed to insert aligned problem: {e}")
            raise

    def _update_cluster_alignment_status(self, cluster_name: str, status: str, aligned_problem_id: str = None):
        """æ›´æ–°èšç±»å¯¹é½çŠ¶æ€"""
        try:
            self.db.update_cluster_alignment_status(cluster_name, status, aligned_problem_id)
        except Exception as e:
            logger.error(f"Failed to update cluster alignment status: {e}")
            raise
```


================================================================================
æ–‡ä»¶: pipeline/cluster.py
================================================================================

```python
"""
Cluster module for Reddit Pain Point Finder
å·¥ä½œæµçº§èšç±»æ¨¡å— - å‘ç°ç›¸ä¼¼çš„ç—›ç‚¹æ¨¡å¼
"""
import json
import logging
import time
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime
import numpy as np

from utils.embedding import pain_clustering
from utils.llm_client import llm_client
from utils.db import db

logger = logging.getLogger(__name__)

class PainEventClusterer:
    """ç—›ç‚¹äº‹ä»¶èšç±»å™¨"""

    def __init__(self):
        """åˆå§‹åŒ–èšç±»å™¨"""
        self.stats = {
            "total_events_processed": 0,
            "clusters_created": 0,
            "llm_validations": 0,
            "processing_time": 0.0,
            "avg_cluster_size": 0.0
        }

    def _find_similar_events(
        self,
        target_event: Dict[str, Any],
        candidate_events: List[Dict[str, Any]],
        threshold: float = 0.7
    ) -> List[Dict[str, Any]]:
        """æ‰¾åˆ°ä¸ç›®æ ‡äº‹ä»¶ç›¸ä¼¼çš„äº‹ä»¶"""
        try:
            # ä½¿ç”¨ç›¸ä¼¼åº¦æœç´¢æ‰¾åˆ°ç›¸ä¼¼äº‹ä»¶
            similar_events = pain_clustering.find_similar_events(
                target_event=target_event,
                candidate_events=candidate_events,
                threshold=threshold,
                top_k=20
            )

            return similar_events

        except Exception as e:
            logger.error(f"Failed to find similar events: {e}")
            return []

    def _validate_cluster_with_llm(
        self,
        pain_events: List[Dict[str, Any]],
        cluster_name: str = None
    ) -> Dict[str, Any]:
        """ä½¿ç”¨LLMéªŒè¯èšç±»æ˜¯å¦å±äºåŒä¸€å·¥ä½œæµ"""
        try:
            # è°ƒç”¨LLMè¿›è¡Œèšç±»éªŒè¯
            response = llm_client.cluster_pain_events(pain_events)

            validation_result = response["content"]

            # æ£€æŸ¥LLMæ˜¯å¦è®¤ä¸ºè¿™äº›äº‹ä»¶å±äºåŒä¸€å·¥ä½œæµ
            if validation_result.get("same_workflow", False):
                return {
                    "is_valid_cluster": True,
                    "cluster_name": validation_result.get("workflow_name", "Unnamed Cluster"),
                    "cluster_description": validation_result.get("workflow_description", ""),
                    "confidence": validation_result.get("confidence", 0.0),
                    "reasoning": validation_result.get("reasoning", "")
                }
            else:
                return {
                    "is_valid_cluster": False,
                    "reasoning": validation_result.get("reasoning", "Not same workflow")
                }

        except Exception as e:
            logger.error(f"Failed to validate cluster with LLM: {e}")
            return {
                "is_valid_cluster": False,
                "reasoning": f"Validation error: {e}"
            }

    def _create_cluster_summary(self, pain_events: List[Dict[str, Any]]) -> Dict[str, Any]:
        """åˆ›å»ºèšç±»æ‘˜è¦"""
        if not pain_events:
            return {}

        try:
            # ç»Ÿè®¡ä¿¡æ¯
            cluster_size = len(pain_events)
            subreddits = list(set(event.get("subreddit", "") for event in pain_events))

            # ç—›ç‚¹ç±»å‹ç»Ÿè®¡
            pain_types = []
            for event in pain_events:
                types = event.get("pain_types", [])
                if isinstance(types, list):
                    pain_types.extend(types)

            pain_type_counts = {}
            for pain_type in pain_types:
                pain_type_counts[pain_type] = pain_type_counts.get(pain_type, 0) + 1

            # ä¸»è¦ç—›ç‚¹ç±»å‹
            primary_pain_type = max(pain_type_counts.items(), key=lambda x: x[1])[0] if pain_type_counts else "general"

            # æƒ…ç»ªä¿¡å·ç»Ÿè®¡
            emotional_signals = [event.get("emotional_signal", "") for event in pain_events]
            emotion_counts = {}
            for signal in emotional_signals:
                if signal:
                    emotion_counts[signal] = emotion_counts.get(signal, 0) + 1

            # é¢‘ç‡åˆ†æ•°ç»Ÿè®¡
            frequency_scores = [event.get("frequency_score", 5) for event in pain_events]
            avg_frequency_score = np.mean(frequency_scores) if frequency_scores else 5.0

            # æåˆ°çš„å·¥å…·
            mentioned_tools = []
            for event in pain_events:
                tools = event.get("mentioned_tools", [])
                if isinstance(tools, list):
                    mentioned_tools.extend(tools)

            tool_counts = {}
            for tool in mentioned_tools:
                tool_counts[tool] = tool_counts.get(tool, 0) + 1

            # æå–ä»£è¡¨æ€§çš„é—®é¢˜
            problems = [event.get("problem", "") for event in pain_events if event.get("problem")]
            representative_problems = sorted(problems, key=len, reverse=True)[:5]

            # æå–ä»£è¡¨æ€§å·¥ä½œæ–¹å¼
            workarounds = [event.get("current_workaround", "") for event in pain_events if event.get("current_workaround")]
            representative_workarounds = [w for w in workarounds if w][:3]

            return {
                "cluster_size": cluster_size,
                "subreddits": subreddits,
                "primary_pain_type": primary_pain_type,
                "pain_type_distribution": pain_type_counts,
                "emotional_signals": emotion_counts,
                "avg_frequency_score": avg_frequency_score,
                "mentioned_tools": tool_counts,
                "representative_problems": representative_problems,
                "representative_workarounds": representative_workarounds,
                "total_pain_score": sum(event.get("post_pain_score", 0) for event in pain_events)
            }

        except Exception as e:
            logger.error(f"Failed to create cluster summary: {e}")
            return {}

    def _save_cluster_to_database(self, cluster_data: Dict[str, Any]) -> Optional[int]:
        """ä¿å­˜èšç±»åˆ°æ•°æ®åº“"""
        try:
            # å‡†å¤‡èšç±»æ•°æ® - æ”¯æŒæ–°çš„source-awareå­—æ®µ
            cluster_record = {
                "cluster_name": cluster_data["cluster_name"],
                "cluster_description": cluster_data["cluster_description"],
                "source_type": cluster_data.get("source_type", ""),
                "centroid_summary": cluster_data.get("centroid_summary", ""),
                "common_pain": cluster_data.get("common_pain", ""),
                "common_context": cluster_data.get("common_context", ""),
                "example_events": cluster_data.get("example_events", []),
                "pain_event_ids": cluster_data["pain_event_ids"],
                "cluster_size": cluster_data["cluster_size"],
                "avg_pain_score": cluster_data.get("avg_pain_score", 0.0),
                "workflow_confidence": cluster_data.get("workflow_confidence", 0.0)
            }

            cluster_id = db.insert_cluster(cluster_record)
            return cluster_id

        except Exception as e:
            logger.error(f"Failed to save cluster to database: {e}")
            return None

    def cluster_pain_events(self, limit: int = 200) -> Dict[str, Any]:
        """èšç±»ç—›ç‚¹äº‹ä»¶ - æŒ‰sourceåˆ†ç»„èšç±»"""
        logger.info(f"Starting source-aware clustering of up to {limit} pain events")

        start_time = time.time()

        try:
            # è·å–æ‰€æœ‰æœ‰åµŒå…¥å‘é‡çš„ç—›ç‚¹äº‹ä»¶ï¼Œå¹¶åŒ…å«sourceä¿¡æ¯
            pain_events = self._get_pain_events_with_source_and_embeddings()

            if len(pain_events) < 4:
                logger.info("Not enough pain events for clustering (need at least 4)")
                return {"clusters_created": 0, "events_processed": 0}

            # é™åˆ¶å¤„ç†æ•°é‡
            if len(pain_events) > limit:
                pain_events = pain_events[:limit]

            logger.info(f"Processing {len(pain_events)} pain events for source-aware clustering")

            # æŒ‰sourceç±»å‹åˆ†ç»„
            source_groups = self._group_events_by_source(pain_events)
            logger.info(f"Found {len(source_groups)} source groups: {list(source_groups.keys())}")

            # å¯¹æ¯ä¸ªsourceç»„åˆ†åˆ«èšç±»
            final_clusters = []
            total_validated_clusters = 0

            for source_type, events_in_source in source_groups.items():
                if len(events_in_source) < 4:
                    logger.info(f"Skipping source {source_type}: not enough events ({len(events_in_source)} < 4)")
                    continue

                logger.info(f"\n=== Processing source: {source_type} ({len(events_in_source)} events) ===")

                # ä½¿ç”¨å‘é‡èšç±»
                vector_clusters = pain_clustering.cluster_pain_events(events_in_source)

                if not vector_clusters:
                    logger.info(f"No clusters found for source {source_type}")
                    continue

                logger.info(f"Found {len(vector_clusters)} vector clusters for source {source_type}")

                # éªŒè¯å’Œä¼˜åŒ–èšç±»
                source_clusters = self._process_source_clusters(
                    vector_clusters, source_type
                )

                final_clusters.extend(source_clusters)
                total_validated_clusters += len(source_clusters)

                logger.info(f"Source {source_type}: {len(source_clusters)} validated clusters")

            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            processing_time = time.time() - start_time
            self.stats["total_events_processed"] = len(pain_events)
            self.stats["clusters_created"] = total_validated_clusters
            self.stats["processing_time"] = processing_time

            if total_validated_clusters > 0:
                self.stats["avg_cluster_size"] = sum(len(cluster["pain_event_ids"]) for cluster in final_clusters) / total_validated_clusters

            logger.info(f"""
=== Source-Aware Clustering Summary ===
Pain events processed: {len(pain_events)}
Source groups processed: {len(source_groups)}
Validated clusters created: {total_validated_clusters}
Average cluster size: {self.stats['avg_cluster_size']:.1f}
Processing time: {processing_time:.2f}s
""")

            return {
                "clusters_created": total_validated_clusters,
                "events_processed": len(pain_events),
                "source_groups": len(source_groups),
                "final_clusters": final_clusters,
                "clustering_stats": self.get_statistics()
            }

        except Exception as e:
            logger.error(f"Failed to cluster pain events: {e}")
            raise

    def _get_pain_events_with_source_and_embeddings(self) -> List[Dict[str, Any]]:
        """è·å–æœ‰åµŒå…¥å‘é‡å’Œsourceä¿¡æ¯çš„ç—›ç‚¹äº‹ä»¶"""
        try:
            import pickle
            with db.get_connection("pain") as conn:
                # å¦‚æœæ˜¯ç»Ÿä¸€æ•°æ®åº“ï¼Œå¯ä»¥ä»postsè¡¨è·å–sourceä¿¡æ¯
                if db.is_unified():
                    cursor = conn.execute("""
                        SELECT p.*, e.embedding_vector, e.embedding_model,
                               COALESCE(po.source, 'reddit') as source_type
                        FROM pain_events p
                        JOIN pain_embeddings e ON p.id = e.pain_event_id
                        LEFT JOIN filtered_posts fp ON p.post_id = fp.id
                        LEFT JOIN posts po ON p.post_id = po.id
                        ORDER BY p.extracted_at DESC
                    """)
                else:
                    # å¤šæ•°æ®åº“æ¨¡å¼ï¼Œé»˜è®¤ä¸ºreddit
                    cursor = conn.execute("""
                        SELECT p.*, e.embedding_vector, e.embedding_model,
                               'reddit' as source_type
                        FROM pain_events p
                        JOIN pain_embeddings e ON p.id = e.pain_event_id
                        ORDER BY p.extracted_at DESC
                    """)

                results = []
                for row in cursor.fetchall():
                    event_data = dict(row)
                    # ååºåˆ—åŒ–åµŒå…¥å‘é‡
                    if event_data["embedding_vector"]:
                        event_data["embedding_vector"] = pickle.loads(event_data["embedding_vector"])
                    results.append(event_data)
                return results
        except Exception as e:
            logger.error(f"Failed to get pain events with source and embeddings: {e}")
            return []

    def _group_events_by_source(self, pain_events: List[Dict[str, Any]]) -> Dict[str, List[Dict[str, Any]]]:
        """æŒ‰sourceç±»å‹åˆ†ç»„ç—›ç‚¹äº‹ä»¶"""
        source_groups = {}
        for event in pain_events:
            source = event.get('source_type', 'reddit')
            if source not in source_groups:
                source_groups[source] = []
            source_groups[source].append(event)
        return source_groups

    def _process_source_clusters(self, vector_clusters: List[Dict[str, Any]], source_type: str) -> List[Dict[str, Any]]:
        """å¤„ç†å•ä¸ªsourceçš„èšç±»"""
        final_clusters = []
        cluster_counter = 1

        for i, cluster in enumerate(vector_clusters):
            logger.info(f"Validating {source_type} cluster {i+1}/{len(vector_clusters)} (size: {cluster['cluster_size']})")

            # è·å–èšç±»ä¸­çš„äº‹ä»¶
            cluster_events = cluster["events"]

            # ä¸¥æ ¼çš„è·³è¿‡è§„åˆ™ï¼šè‡³å°‘4ä¸ªäº‹ä»¶
            if len(cluster_events) < 4:
                logger.debug(f"Skipping cluster {i+1}: too small ({len(cluster_events)} < 4 events)")
                continue

            # å¯¹äºå¤§èšç±»ï¼Œé‡‡æ ·å‰20ä¸ªäº‹ä»¶è¿›è¡ŒéªŒè¯å’Œæ‘˜è¦
            events_for_processing = cluster_events
            if len(cluster_events) > 20:
                events_for_processing = cluster_events[:20]
                logger.info(f"Sampling first 20 events from large cluster of {len(cluster_events)} for processing")

            # ä½¿ç”¨LLMéªŒè¯èšç±»
            validation_result = self._validate_cluster_with_llm(events_for_processing)
            self.stats["llm_validations"] += 1

            if validation_result["is_valid_cluster"]:
                # ä½¿ç”¨Cluster Summarizerç”Ÿæˆsourceå†…æ‘˜è¦
                summary_result = self._summarize_source_cluster(events_for_processing, source_type)

                # ç”Ÿæˆæ ‡å‡†åŒ–cluster ID
                cluster_id = f"{source_type.replace('-', '_')}_{cluster_counter:02d}"
                cluster_counter += 1

                # å‡†å¤‡æœ€ç»ˆèšç±»æ•°æ®
                final_cluster = {
                    "cluster_name": f"{source_type}: {validation_result['cluster_name']}",
                    "cluster_description": validation_result["cluster_description"],
                    "source_type": source_type,
                    "cluster_id": cluster_id,
                    "centroid_summary": summary_result.get("centroid_summary", ""),
                    "common_pain": summary_result.get("common_pain", ""),
                    "common_context": summary_result.get("common_context", ""),
                    "example_events": summary_result.get("example_events", []),
                    "pain_event_ids": [event["id"] for event in cluster_events],
                    "cluster_size": len(cluster_events),
                    "workflow_confidence": validation_result["confidence"],
                    "validation_reasoning": validation_result["reasoning"]
                }

                # ä¿å­˜åˆ°æ•°æ®åº“
                saved_cluster_id = self._save_cluster_to_database(final_cluster)
                if saved_cluster_id:
                    final_cluster["saved_cluster_id"] = saved_cluster_id
                    final_clusters.append(final_cluster)

                    logger.info(f"âœ… Saved {cluster_id}: {validation_result['cluster_name']} ({len(cluster_events)} events)")
            else:
                logger.warning(f"âŒ Cluster {i+1} rejected by LLM: {validation_result['reasoning']}")

            # æ·»åŠ å»¶è¿Ÿé¿å…APIé™åˆ¶
            time.sleep(1)

        return final_clusters

    def _summarize_source_cluster(self, pain_events: List[Dict[str, Any]], source_type: str) -> Dict[str, Any]:
        """ä½¿ç”¨Cluster Summarizerç”Ÿæˆsourceå†…èšç±»æ‘˜è¦"""
        try:
            response = llm_client.summarize_source_cluster(pain_events, source_type)
            return response.get("content", {})
        except Exception as e:
            logger.error(f"Failed to summarize source cluster: {e}")
            return {
                "centroid_summary": "",
                "common_pain": "",
                "common_context": "",
                "example_events": [],
                "coherence_score": 0.0,
                "reasoning": f"Summary failed: {e}"
            }

    def get_cluster_analysis(self, cluster_id: int) -> Optional[Dict[str, Any]]:
        """è·å–èšç±»è¯¦ç»†åˆ†æ"""
        try:
            # ä»æ•°æ®åº“è·å–èšç±»ä¿¡æ¯
            with db.get_connection("clusters") as conn:
                cursor = conn.execute("""
                    SELECT * FROM clusters WHERE id = ?
                """, (cluster_id,))
                cluster_data = cursor.fetchone()

            if not cluster_data:
                return None

            cluster_info = dict(cluster_data)

            # è·å–èšç±»ä¸­çš„ç—›ç‚¹äº‹ä»¶
            pain_event_ids = json.loads(cluster_info["pain_event_ids"])
            pain_events = []

            with db.get_connection("pain") as conn:
                for event_id in pain_event_ids:
                    cursor = conn.execute("""
                        SELECT * FROM pain_events WHERE id = ?
                    """, (event_id,))
                    event_data = cursor.fetchone()
                    if event_data:
                        pain_events.append(dict(event_data))

            cluster_info["pain_events"] = pain_events

            # é‡æ–°è®¡ç®—èšç±»æ‘˜è¦
            cluster_summary = self._create_cluster_summary(pain_events)
            cluster_info["cluster_summary"] = cluster_summary

            return cluster_info

        except Exception as e:
            logger.error(f"Failed to get cluster analysis: {e}")
            return None

    def get_all_clusters_summary(self) -> List[Dict[str, Any]]:
        """è·å–æ‰€æœ‰èšç±»çš„æ‘˜è¦"""
        try:
            with db.get_connection("clusters") as conn:
                cursor = conn.execute("""
                    SELECT id, cluster_name, cluster_size, avg_pain_score, workflow_confidence, created_at
                    FROM clusters
                    ORDER BY cluster_size DESC, workflow_confidence DESC
                """)
                clusters = [dict(row) for row in cursor.fetchall()]

            return clusters

        except Exception as e:
            logger.error(f"Failed to get clusters summary: {e}")
            return []

    def get_statistics(self) -> Dict[str, Any]:
        """è·å–èšç±»ç»Ÿè®¡ä¿¡æ¯"""
        stats = self.stats.copy()

        if stats["total_events_processed"] > 0:
            stats["clustering_rate"] = stats["clusters_created"] / stats["total_events_processed"]
            stats["processing_rate"] = stats["total_events_processed"] / max(stats["processing_time"], 1)
        else:
            stats["clustering_rate"] = 0
            stats["processing_rate"] = 0

        # æ·»åŠ åµŒå…¥å®¢æˆ·ç«¯ç»Ÿè®¡
        embedding_stats = pain_clustering.embedding_client.get_embedding_statistics()
        stats["embedding_stats"] = embedding_stats

        return stats

    def reset_statistics(self):
        """é‡ç½®ç»Ÿè®¡ä¿¡æ¯"""
        self.stats = {
            "total_events_processed": 0,
            "clusters_created": 0,
            "llm_validations": 0,
            "processing_time": 0.0,
            "avg_cluster_size": 0.0
        }

def main():
    """ä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description="Cluster pain events into workflow groups")
    parser.add_argument("--limit", type=int, default=200, help="Limit number of pain events to process")
    parser.add_argument("--analyze", type=int, help="Analyze specific cluster ID")
    parser.add_argument("--list", action="store_true", help="List all clusters summary")
    args = parser.parse_args()

    try:
        logger.info("Starting pain event clustering...")

        clusterer = PainEventClusterer()

        if args.analyze:
            # åˆ†æç‰¹å®šèšç±»
            cluster_analysis = clusterer.get_cluster_analysis(args.analyze)
            if cluster_analysis:
                print(json.dumps(cluster_analysis, indent=2))
            else:
                logger.error(f"Cluster {args.analyze} not found")

        elif args.list:
            # åˆ—å‡ºæ‰€æœ‰èšç±»
            clusters_summary = clusterer.get_all_clusters_summary()
            print(json.dumps(clusters_summary, indent=2))

        else:
            # æ‰§è¡Œèšç±»
            result = clusterer.cluster_pain_events(limit=args.limit)

            logger.info(f"""
=== Clustering Complete ===
Clusters created: {result['clusters_created']}
Events processed: {result['events_processed']}
Clustering stats: {result['clustering_stats']}
""")

    except Exception as e:
        logger.error(f"Fatal error: {e}")
        raise

if __name__ == "__main__":
    main()
```


================================================================================
æ–‡ä»¶: pipeline/embed.py
================================================================================

```python
"""
Embed module for Reddit Pain Point Finder
ç—›ç‚¹äº‹ä»¶å‘é‡åŒ–æ¨¡å— - ä¸ºèšç±»åšå‡†å¤‡
"""
import logging
import time
from typing import List, Dict, Any, Optional
from datetime import datetime

from utils.embedding import embedding_client
from utils.db import db

logger = logging.getLogger(__name__)

class PainEventEmbedder:
    """ç—›ç‚¹äº‹ä»¶å‘é‡åŒ–å™¨"""

    def __init__(self):
        """åˆå§‹åŒ–å‘é‡åŒ–å™¨"""
        self.stats = {
            "total_processed": 0,
            "embeddings_created": 0,
            "errors": 0,
            "processing_time": 0.0,
            "cache_hits": 0
        }

    def _create_embedding_text(self, pain_event: Dict[str, Any]) -> str:
        """åˆ›å»ºç”¨äºåµŒå…¥çš„æ–‡æœ¬"""
        text_parts = []

        # æ ¸å¿ƒè¦ç´ 
        if pain_event.get("actor"):
            text_parts.append(pain_event["actor"])

        if pain_event.get("context"):
            text_parts.append(pain_event["context"])

        if pain_event.get("problem"):
            text_parts.append(pain_event["problem"])

        if pain_event.get("current_workaround"):
            text_parts.append(pain_event["current_workaround"])

        # ç”¨è¿æ¥ç¬¦ä¿æŒè¯­ä¹‰ç»“æ„
        embedding_text = " | ".join(text_parts)

        # æ£€æŸ¥æ–‡æœ¬é•¿åº¦
        if len(embedding_text) > 2000:
            logger.warning(f"Embedding text too long ({len(embedding_text)} chars), truncating")
            # ä¼˜å…ˆä¿ç•™problemå’Œcontext
            core_text = f"{pain_event.get('context', '')} | {pain_event.get('problem', '')}"
            if len(core_text) > 1000:
                # è¿›ä¸€æ­¥æˆªæ–­
                embedding_text = core_text[:1000]
            else:
                embedding_text = core_text

        return embedding_text

    def embed_single_event(self, pain_event: Dict[str, Any]) -> Optional[List[float]]:
        """ä¸ºå•ä¸ªç—›ç‚¹äº‹ä»¶åˆ›å»ºåµŒå…¥å‘é‡"""
        try:
            # åˆ›å»ºåµŒå…¥æ–‡æœ¬
            embedding_text = self._create_embedding_text(pain_event)

            if not embedding_text:
                logger.warning(f"Empty embedding text for pain event {pain_event.get('id')}")
                return None

            # åˆ›å»ºåµŒå…¥å‘é‡
            embedding = embedding_client.create_embedding(embedding_text)

            self.stats["embeddings_created"] += 1
            return embedding

        except Exception as e:
            logger.error(f"Failed to create embedding for pain event {pain_event.get('id')}: {e}")
            self.stats["errors"] += 1
            return None

    def save_embedding(self, pain_event_id: int, embedding: List[float]) -> bool:
        """ä¿å­˜åµŒå…¥å‘é‡åˆ°æ•°æ®åº“"""
        try:
            success = db.insert_pain_embedding(
                pain_event_id=pain_event_id,
                embedding_vector=embedding,
                model_name=embedding_client.model_name
            )
            return success

        except Exception as e:
            logger.error(f"Failed to save embedding for pain event {pain_event_id}: {e}")
            return False

    def process_pain_events_batch(self, pain_events: List[Dict[str, Any]], batch_size: int = 20) -> int:
        """æ‰¹é‡å¤„ç†ç—›ç‚¹äº‹ä»¶çš„å‘é‡åŒ–"""
        logger.info(f"Creating embeddings for {len(pain_events)} pain events")

        start_time = time.time()
        saved_count = 0

        for i, event in enumerate(pain_events):
            if i % 10 == 0:
                logger.info(f"Processed {i}/{len(pain_events)} pain events")

            # åˆ›å»ºåµŒå…¥å‘é‡
            embedding = self.embed_single_event(event)
            if embedding is None:
                continue

            # ä¿å­˜åˆ°æ•°æ®åº“
            if self.save_embedding(event["id"], embedding):
                saved_count += 1

            # æ‰¹é‡å¤„ç†å»¶è¿Ÿ
            if i % batch_size == 0 and i > 0:
                time.sleep(1)  # é¿å…APIé™åˆ¶

        # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        processing_time = time.time() - start_time
        self.stats["total_processed"] = len(pain_events)
        self.stats["processing_time"] = processing_time

        # æ·»åŠ åµŒå…¥å®¢æˆ·ç«¯ç»Ÿè®¡
        embedding_stats = embedding_client.get_embedding_statistics()
        self.stats["cache_hits"] = embedding_stats.get("cache_hits", 0)

        logger.info(f"Embedding complete: {saved_count}/{len(pain_events)} embeddings saved")
        logger.info(f"Processing time: {processing_time:.2f}s")

        return saved_count

    def process_missing_embeddings(self, limit: int = 100) -> Dict[str, Any]:
        """å¤„ç†ç¼ºå¤±åµŒå…¥å‘é‡çš„ç—›ç‚¹äº‹ä»¶"""
        logger.info(f"Processing up to {limit} pain events without embeddings")

        try:
            # è·å–æ²¡æœ‰åµŒå…¥å‘é‡çš„ç—›ç‚¹äº‹ä»¶
            pain_events = db.get_pain_events_without_embeddings(limit=limit)

            if not pain_events:
                logger.info("No pain events without embeddings found")
                return {"processed": 0, "embeddings_created": 0}

            logger.info(f"Found {len(pain_events)} pain events to embed")

            # æ‰¹é‡åˆ›å»ºåµŒå…¥å‘é‡
            saved_count = self.process_pain_events_batch(pain_events)

            return {
                "processed": len(pain_events),
                "embeddings_created": saved_count,
                "embedding_stats": self.get_statistics()
            }

        except Exception as e:
            logger.error(f"Failed to process missing embeddings: {e}")
            raise

    def get_embedding_statistics(self) -> Dict[str, Any]:
        """è·å–å‘é‡åŒ–ç»Ÿè®¡ä¿¡æ¯"""
        stats = self.stats.copy()

        if stats["total_processed"] > 0:
            stats["success_rate"] = stats["embeddings_created"] / stats["total_processed"]
            stats["processing_rate"] = stats["total_processed"] / max(stats["processing_time"], 1)
        else:
            stats["success_rate"] = 0
            stats["processing_rate"] = 0

        # æ·»åŠ åµŒå…¥å®¢æˆ·ç«¯ç»Ÿè®¡
        embedding_stats = embedding_client.get_embedding_statistics()
        stats["embedding_client_stats"] = embedding_stats

        return stats

    def reset_statistics(self):
        """é‡ç½®ç»Ÿè®¡ä¿¡æ¯"""
        self.stats = {
            "total_processed": 0,
            "embeddings_created": 0,
            "errors": 0,
            "processing_time": 0.0,
            "cache_hits": 0
        }

    def verify_embeddings(self, limit: int = 50) -> Dict[str, Any]:
        """éªŒè¯åµŒå…¥å‘é‡çš„è´¨é‡"""
        logger.info(f"Verifying {limit} embeddings")

        try:
            # è·å–æ‰€æœ‰æœ‰åµŒå…¥å‘é‡çš„ç—›ç‚¹äº‹ä»¶
            pain_events = db.get_all_pain_events_with_embeddings()

            if len(pain_events) > limit:
                pain_events = pain_events[:limit]

            if not pain_events:
                return {"verified": 0, "issues": []}

            issues = []
            verified_count = 0

            for event in pain_events:
                try:
                    embedding = event.get("embedding_vector")
                    if not embedding:
                        issues.append(f"Event {event['id']}: Missing embedding vector")
                        continue

                    # æ£€æŸ¥ç»´åº¦
                    if len(embedding) == 0:
                        issues.append(f"Event {event['id']}: Empty embedding vector")
                        continue

                    # æ£€æŸ¥æ˜¯å¦åŒ…å«æœ‰æ•ˆæ•°å€¼
                    if not all(isinstance(x, (int, float)) for x in embedding):
                        issues.append(f"Event {event['id']}: Invalid embedding data types")
                        continue

                    # æ£€æŸ¥æ˜¯å¦å…¨ä¸ºé›¶ï¼ˆå¼‚å¸¸ï¼‰
                    if all(abs(x) < 1e-6 for x in embedding):
                        issues.append(f"Event {event['id']}: All-zero embedding vector")
                        continue

                    verified_count += 1

                except Exception as e:
                    issues.append(f"Event {event.get('id', 'unknown')}: Verification error - {e}")

            logger.info(f"Embedding verification complete: {verified_count}/{len(pain_events)} passed")

            return {
                "verified": verified_count,
                "total": len(pain_events),
                "issues": issues
            }

        except Exception as e:
            logger.error(f"Failed to verify embeddings: {e}")
            return {"verified": 0, "issues": [f"Verification failed: {e}"]}

    def get_statistics(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        stats = self.stats.copy()
        if stats["total_processed"] > 0:
            stats["success_rate"] = stats["embeddings_created"] / stats["total_processed"]
        else:
            stats["success_rate"] = 0.0
        return stats

def main():
    """ä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description="Create embeddings for pain events")
    parser.add_argument("--limit", type=int, default=100, help="Limit number of pain events to process")
    parser.add_argument("--verify", action="store_true", help="Verify existing embeddings")
    parser.add_argument("--batch-size", type=int, default=20, help="Batch size for processing")
    args = parser.parse_args()

    try:
        logger.info("Starting pain event embedding...")

        embedder = PainEventEmbedder()

        if args.verify:
            # éªŒè¯ç°æœ‰åµŒå…¥
            result = embedder.verify_embeddings(limit=args.limit)
            logger.info(f"Verification result: {result}")
        else:
            # å¤„ç†ç¼ºå¤±çš„åµŒå…¥
            result = embedder.process_missing_embeddings(limit=args.limit)

            logger.info(f"""
=== Embedding Summary ===
Pain events processed: {result['processed']}
Embeddings created: {result['embeddings_created']}
Embedding stats: {result['embedding_stats']}
""")

    except Exception as e:
        logger.error(f"Fatal error: {e}")
        raise

if __name__ == "__main__":
    main()
```


================================================================================
æ–‡ä»¶: pipeline/extract_pain.py
================================================================================

```python
"""
Extract Pain module for Reddit Pain Point Finder
ç—›ç‚¹äº‹ä»¶æŠ½å–æ¨¡å— - ä½¿ç”¨LLMè¿›è¡Œç»“æ„åŒ–æŠ½å–
"""
import json
import logging
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime
import time

from utils.llm_client import llm_client
from utils.db import db

logger = logging.getLogger(__name__)

class PainPointExtractor:
    """ç—›ç‚¹äº‹ä»¶æŠ½å–å™¨"""

    def __init__(self):
        """åˆå§‹åŒ–æŠ½å–å™¨"""
        self.stats = {
            "total_processed": 0,
            "total_pain_events": 0,
            "extraction_errors": 0,
            "avg_confidence": 0.0,
            "processing_time": 0.0
        }

    def _extract_from_single_post(self, post_data: Dict[str, Any], retry_count: int = 0) -> List[Dict[str, Any]]:
        """ä»å•ä¸ªå¸–å­æŠ½å–ç—›ç‚¹äº‹ä»¶"""
        max_retries = 2
        try:
            title = post_data.get("title", "")
            body = post_data.get("body", "")
            subreddit = post_data.get("subreddit", "")
            upvotes = post_data.get("score", 0)
            comments_count = post_data.get("num_comments", 0)

            # è°ƒç”¨LLMè¿›è¡ŒæŠ½å–
            response = llm_client.extract_pain_points(
                title=title,
                body=body,
                subreddit=subreddit,
                upvotes=upvotes,
                comments_count=comments_count
            )

            extraction_result = response["content"]
            pain_events = extraction_result.get("pain_events", [])

            # ä¸ºæ¯ä¸ªç—›ç‚¹äº‹ä»¶æ·»åŠ å…ƒæ•°æ®
            for event in pain_events:
                event.update({
                    "post_id": post_data["id"],
                    "subreddit": subreddit,
                    "original_score": upvotes,
                    "extraction_model": response["model"],
                    "extraction_timestamp": datetime.now().isoformat(),
                    "confidence": event.get("confidence", 0.0)
                })

            self.stats["total_pain_events"] += len(pain_events)
            logger.debug(f"Extracted {len(pain_events)} pain events from post {post_data['id']}")

            return pain_events

        except Exception as e:
            error_msg = f"Failed to extract pain from post {post_data.get('id')}: {e}"

            # å¦‚æœæ˜¯è¶…æ—¶é”™è¯¯ä¸”è¿˜æœ‰é‡è¯•æœºä¼š
            if "timeout" in str(e).lower() and retry_count < max_retries:
                logger.warning(f"{error_msg} (retry {retry_count + 1}/{max_retries})")
                time.sleep(5)  # ç­‰å¾…5ç§’åé‡è¯•
                return self._extract_from_single_post(post_data, retry_count + 1)
            else:
                logger.error(error_msg)
                self.stats["extraction_errors"] += 1
                return []

    def _validate_pain_event(self, pain_event: Dict[str, Any]) -> bool:
        """éªŒè¯ç—›ç‚¹äº‹ä»¶çš„è´¨é‡"""
        try:
            # æ£€æŸ¥å¿…éœ€å­—æ®µ
            required_fields = ["problem", "post_id"]
            for field in required_fields:
                if not pain_event.get(field):
                    logger.warning(f"Missing required field '{field}' in pain event")
                    return False

            # æ£€æŸ¥é—®é¢˜æè¿°é•¿åº¦
            problem = pain_event.get("problem", "")
            if len(problem) < 10:
                logger.warning(f"Problem description too short: {problem}")
                return False

            if len(problem) > 1000:
                logger.warning(f"Problem description too long: {len(problem)} characters")
                return False

            # æ£€æŸ¥ç½®ä¿¡åº¦
            confidence = pain_event.get("confidence", 0.0)
            if confidence < 0.3:
                logger.warning(f"Low confidence pain event: {confidence}")
                return False

            # æ£€æŸ¥æ˜¯å¦è¿‡äºæ³›æ³›
            generic_problems = [
                "it's slow", "it's bad", "it doesn't work", "it's broken",
                "i don't like it", "it's annoying", "it's frustrating"
            ]
            problem_lower = problem.lower()
            for generic in generic_problems:
                if problem_lower == generic:
                    logger.warning(f"Too generic problem: {problem}")
                    return False

            return True

        except Exception as e:
            logger.error(f"Error validating pain event: {e}")
            return False

    def _enhance_pain_event(self, pain_event: Dict[str, Any], post_data: Dict[str, Any]) -> Dict[str, Any]:
        """å¢å¼ºç—›ç‚¹äº‹ä»¶ä¿¡æ¯"""
        try:
            enhanced = pain_event.copy()

            # æ·»åŠ å¸–å­ä¸Šä¸‹æ–‡
            enhanced.update({
                "post_title": post_data.get("title", ""),
                "post_category": post_data.get("category", ""),
                "post_pain_score": post_data.get("pain_score", 0.0),
                "post_comments": post_data.get("num_comments", 0)
            })

            # åˆ†æç—›ç‚¹ç±»å‹
            problem_text = enhanced.get("problem", "").lower()
            context_text = enhanced.get("context", "").lower()
            full_text = f"{problem_text} {context_text}"

            # ç—›ç‚¹ç±»å‹åˆ†ç±»
            pain_types = {
                "workflow": ["workflow", "process", "flow", "pipeline", "automation"],
                "technical": ["code", "programming", "development", "technical", "bug"],
                "efficiency": ["slow", "time", "inefficient", "productivity", "performance"],
                "complexity": ["complex", "complicated", "difficult", "hard", "confusing"],
                "integration": ["integration", "connect", "api", "compatibility", "sync"],
                "cost": ["expensive", "cost", "price", "pricing", "budget"],
                "user_experience": ["ui", "ux", "interface", "usability", "experience"],
                "data": ["data", "database", "storage", "backup", "analysis"]
            }

            detected_types = []
            for pain_type, keywords in pain_types.items():
                if any(keyword in full_text for keyword in keywords):
                    detected_types.append(pain_type)

            enhanced["pain_types"] = detected_types
            enhanced["primary_pain_type"] = detected_types[0] if detected_types else "general"

            # æå–æåˆ°çš„å·¥å…·
            mentioned_tools = enhanced.get("mentioned_tools", [])
            if not isinstance(mentioned_tools, list):
                mentioned_tools = []

            # ä»æ–‡æœ¬ä¸­æå–æ›´å¤šå·¥å…·åï¼ˆç®€å•è§„åˆ™ï¼‰
            common_tools = [
                "excel", "google sheets", "slack", "discord", "jira", "trello", "asana",
                "github", "gitlab", "vscode", "intellij", "docker", "kubernetes", "aws",
                "azure", "gcp", "mysql", "postgresql", "mongodb", "redis", "figma",
                "sketch", "photoshop", "wordpress", "shopify", "salesforce"
            ]

            for tool in common_tools:
                if tool in full_text and tool not in mentioned_tools:
                    mentioned_tools.append(tool)

            enhanced["mentioned_tools"] = mentioned_tools

            # åˆ†æé¢‘ç‡
            frequency = enhanced.get("frequency", "").lower()
            if "daily" in frequency or "every day" in frequency:
                enhanced["frequency_score"] = 10
            elif "weekly" in frequency or "every week" in frequency:
                enhanced["frequency_score"] = 8
            elif "monthly" in frequency or "every month" in frequency:
                enhanced["frequency_score"] = 6
            elif "often" in frequency or "frequent" in frequency:
                enhanced["frequency_score"] = 7
            elif "sometimes" in frequency or "occasional" in frequency:
                enhanced["frequency_score"] = 4
            elif "rarely" in frequency:
                enhanced["frequency_score"] = 2
            else:
                enhanced["frequency_score"] = 5  # é»˜è®¤ä¸­ç­‰é¢‘ç‡

            return enhanced

        except Exception as e:
            logger.error(f"Error enhancing pain event: {e}")
            return pain_event

    def extract_from_posts_batch(self, posts: List[Dict[str, Any]], batch_size: int = 10) -> List[Dict[str, Any]]:
        """æ‰¹é‡ä»å¸–å­ä¸­æŠ½å–ç—›ç‚¹äº‹ä»¶"""
        logger.info(f"Extracting pain points from {len(posts)} posts")

        all_pain_events = []
        start_time = time.time()

        for i, post in enumerate(posts):
            if i % 10 == 0:
                logger.info(f"Processed {i}/{len(posts)} posts")

            # æŠ½å–ç—›ç‚¹äº‹ä»¶
            pain_events = self._extract_from_single_post(post)

            # éªŒè¯å’Œå¢å¼ºæ¯ä¸ªç—›ç‚¹äº‹ä»¶
            for event in pain_events:
                if self._validate_pain_event(event):
                    enhanced_event = self._enhance_pain_event(event, post)
                    all_pain_events.append(enhanced_event)

            # æ·»åŠ å»¶è¿Ÿé¿å…APIé™åˆ¶
            time.sleep(2.0)  # å¢åŠ åˆ°2ç§’é—´éš”

        # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        processing_time = time.time() - start_time
        self.stats["total_processed"] = len(posts)
        self.stats["processing_time"] = processing_time

        if all_pain_events:
            avg_confidence = sum(event.get("confidence", 0) for event in all_pain_events) / len(all_pain_events)
            self.stats["avg_confidence"] = avg_confidence

        logger.info(f"Extraction complete: {len(all_pain_events)} pain events from {len(posts)} posts")
        logger.info(f"Processing time: {processing_time:.2f}s, Avg per post: {processing_time/len(posts):.2f}s")

        return all_pain_events

    def save_pain_events(self, pain_events: List[Dict[str, Any]]) -> int:
        """ä¿å­˜ç—›ç‚¹äº‹ä»¶åˆ°æ•°æ®åº“"""
        saved_count = 0

        for event in pain_events:
            try:
                # å‡†å¤‡æ•°æ®åº“è®°å½•
                event_data = {
                    "post_id": event["post_id"],
                    "actor": event.get("actor", ""),
                    "context": event.get("context", ""),
                    "problem": event["problem"],
                    "current_workaround": event.get("current_workaround", ""),
                    "frequency": event.get("frequency", ""),
                    "emotional_signal": event.get("emotional_signal", ""),
                    "mentioned_tools": event.get("mentioned_tools", []),
                    "extraction_confidence": event.get("confidence", 0.0)
                }

                # ä¿å­˜åˆ°æ•°æ®åº“
                pain_event_id = db.insert_pain_event(event_data)
                if pain_event_id:
                    saved_count += 1
                    logger.debug(f"Saved pain event {pain_event_id}: {event['problem'][:50]}...")

            except Exception as e:
                logger.error(f"Failed to save pain event: {e}")

        logger.info(f"Saved {saved_count}/{len(pain_events)} pain events to database")
        return saved_count

    def process_unextracted_posts(self, limit: int = 100) -> Dict[str, Any]:
        """å¤„ç†æœªæŠ½å–çš„å¸–å­"""
        logger.info(f"Processing up to {limit} unextracted posts")

        try:
            # è·å–æœªå¤„ç†çš„è¿‡æ»¤å¸–å­
            unextracted_posts = db.get_filtered_posts(limit=limit, min_pain_score=0.3)

            if not unextracted_posts:
                logger.info("No unextracted posts found")
                return {"processed": 0, "pain_events": 0}

            logger.info(f"Found {len(unextracted_posts)} posts to extract from")

            # è®°å½•å¤±è´¥çš„å¸–å­IDï¼Œç”¨äºåç»­è·³è¿‡
            failed_posts = []

            # æŠ½å–ç—›ç‚¹äº‹ä»¶ï¼ˆå¸¦å¤±è´¥æ¢å¤ï¼‰
            pain_events = []
            for i, post in enumerate(unextracted_posts):
                if i % 10 == 0:
                    logger.info(f"Processed {i}/{len(unextracted_posts)} posts")

                try:
                    # å°è¯•æŠ½å–å•ä¸ªå¸–å­
                    post_events = self._extract_from_single_post(post)

                    # éªŒè¯å’Œå¢å¼ºæ¯ä¸ªç—›ç‚¹äº‹ä»¶
                    for event in post_events:
                        if self._validate_pain_event(event):
                            enhanced_event = self._enhance_pain_event(event, post)
                            pain_events.append(enhanced_event)

                    logger.debug(f"Successfully processed post {post.get('id')}")

                except Exception as e:
                    logger.error(f"Failed to process post {post.get('id')}: {e}")
                    failed_posts.append(post.get('id'))
                    self.stats["extraction_errors"] += 1
                    continue

                # æ·»åŠ å»¶è¿Ÿé¿å…APIé™åˆ¶ï¼Œä½¿ç”¨åŠ¨æ€å»¶è¿Ÿ
                delay = 3.0 + (i % 5)  # 3-7ç§’åŠ¨æ€å»¶è¿Ÿ
                logger.debug(f"Waiting {delay:.1f}s before next post...")
                time.sleep(delay)

            # ä¿å­˜æˆåŠŸå¤„ç†çš„ç—›ç‚¹äº‹ä»¶
            saved_count = self.save_pain_events(pain_events)

            # è®°å½•å¤±è´¥ç»Ÿè®¡
            if failed_posts:
                logger.warning(f"Failed to process {len(failed_posts)} posts: {failed_posts}")

            return {
                "processed": len(unextracted_posts) - len(failed_posts),
                "failed": len(failed_posts),
                "pain_events_extracted": len(pain_events),
                "pain_events_saved": saved_count,
                "extraction_stats": self.get_statistics(),
                "failed_posts": failed_posts
            }

        except Exception as e:
            logger.error(f"Failed to process unextracted posts: {e}")
            raise

    def get_statistics(self) -> Dict[str, Any]:
        """è·å–æŠ½å–ç»Ÿè®¡ä¿¡æ¯"""
        stats = self.stats.copy()
        if stats["total_processed"] > 0:
            stats["avg_events_per_post"] = stats["total_pain_events"] / stats["total_processed"]
            stats["processing_rate"] = stats["total_processed"] / max(stats["processing_time"], 1)
        else:
            stats["avg_events_per_post"] = 0
            stats["processing_rate"] = 0

        # æ·»åŠ LLMå®¢æˆ·ç«¯ç»Ÿè®¡
        llm_stats = llm_client.get_statistics()
        stats["llm_stats"] = llm_stats

        return stats

    def reset_statistics(self):
        """é‡ç½®ç»Ÿè®¡ä¿¡æ¯"""
        self.stats = {
            "total_processed": 0,
            "total_pain_events": 0,
            "extraction_errors": 0,
            "avg_confidence": 0.0,
            "processing_time": 0.0
        }

def main():
    """ä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description="Extract pain points from filtered Reddit posts")
    parser.add_argument("--limit", type=int, default=100, help="Limit number of posts to process")
    parser.add_argument("--min-score", type=float, default=0.3, help="Minimum pain score threshold")
    parser.add_argument("--batch-size", type=int, default=10, help="Batch size for processing")
    args = parser.parse_args()

    try:
        logger.info("Starting pain point extraction...")

        extractor = PainPointExtractor()

        # å¤„ç†æœªæŠ½å–çš„å¸–å­
        result = extractor.process_unextracted_posts(limit=args.limit)

        logger.info(f"""
=== Extraction Summary ===
Posts processed: {result['processed']}
Pain events extracted: {result['pain_events_extracted']}
Pain events saved: {result['pain_events_saved']}
Extraction stats: {result['extraction_stats']}
""")

    except Exception as e:
        logger.error(f"Fatal error: {e}")
        raise

if __name__ == "__main__":
    main()
```


================================================================================
æ–‡ä»¶: pipeline/fetch.py
================================================================================

```python
"""
Fetch module for Reddit Pain Point Finder
åŸºäºåŸæœ‰reddit_collection.pyä¼˜åŒ–çš„Redditæ•°æ®æŠ“å–æ¨¡å—
"""
import os
import json
import sys
import time
import logging
import praw
import yaml
from typing import List, Dict, Any, Optional
from datetime import datetime, timedelta

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# å¯¼å…¥å·¥å…·æ¨¡å—
try:
    from utils.db import db
except ImportError:
    logger.warning("Could not import db utility, will use local file storage")

class RedditSourceFetcher:
    """Redditç—›ç‚¹æ•°æ®æŠ“å–å™¨"""

    def __init__(self, config_path: str = "config/subreddits.yaml"):
        """åˆå§‹åŒ–æŠ“å–å™¨"""
        self.config = self._load_config(config_path)
        self.reddit_client = self._init_reddit_client()
        self.processed_posts = set()
        self.stats = {
            "total_fetched": 0,
            "total_saved": 0,
            "filtered_out": 0,
            "errors": 0,
            "start_time": None
        }

    def _load_config(self, config_path: str) -> Dict[str, Any]:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                return yaml.safe_load(f)
        except Exception as e:
            logger.error(f"Failed to load config from {config_path}: {e}")
            raise

    def _init_reddit_client(self) -> praw.Reddit:
        """åˆå§‹åŒ–Redditå®¢æˆ·ç«¯"""
        try:
            # ä»ç¯å¢ƒå˜é‡è·å–APIå‡­è¯
            client_id = os.getenv('REDDIT_CLIENT_ID')
            client_secret = os.getenv('REDDIT_CLIENT_SECRET')

            if not client_id or not client_secret:
                raise ValueError("Reddit API credentials not found in environment variables")

            reddit = praw.Reddit(
                client_id=client_id,
                client_secret=client_secret,
                user_agent="python:PainPointFinder:v1.0",
                read_only=True
            )

            # æµ‹è¯•è®¤è¯
            test_subreddit = reddit.subreddit('test')
            test_subreddit.display_name
            logger.info("Reddit authentication successful")

            return reddit

        except Exception as e:
            logger.error(f"Failed to initialize Reddit client: {e}")
            raise

    def _load_processed_posts(self):
        """åŠ è½½å·²å¤„ç†çš„å¸–å­IDï¼ˆæ”¯æŒæ–°æ—§IDæ ¼å¼ï¼‰"""
        try:
            # å°è¯•ä»æ•°æ®åº“åŠ è½½
            if 'db' in globals():
                # ä»æ•°æ®åº“è·å–å·²å¤„ç†çš„å¸–å­ID
                with db.get_connection("raw") as conn:
                    cursor = conn.execute("SELECT id, source FROM posts")
                    self.processed_posts = {row[0] for row in cursor.fetchall()}

                    # ç»Ÿè®¡å„ç§æ•°æ®æº
                    source_counts = {}
                    cursor = conn.execute("SELECT source, COUNT(*) as count FROM posts GROUP BY source")
                    for row in cursor.fetchall():
                        source_counts[row[0]] = row[1]
                    logger.info(f"Loaded posts by source: {source_counts}")
            else:
                # ä»æ–‡ä»¶åŠ è½½ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰
                processed_file = "data/processed_posts.json"
                if os.path.exists(processed_file):
                    with open(processed_file, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                        self.processed_posts = set(data.get("processed_ids", []))

            logger.info(f"Loaded {len(self.processed_posts)} previously processed post IDs")

        except Exception as e:
            logger.error(f"Failed to load processed posts: {e}")
            self.processed_posts = set()

    def _save_processed_posts(self):
        """ä¿å­˜å·²å¤„ç†çš„å¸–å­ID"""
        try:
            if 'db' in globals():
                # æ•°æ®åº“æ¨¡å¼ä¸éœ€è¦é¢å¤–ä¿å­˜ï¼Œå› ä¸ºæ¯æ¡è®°å½•éƒ½å•ç‹¬å­˜å‚¨
                pass
            else:
                # ä¿å­˜åˆ°æ–‡ä»¶ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰
                processed_file = "data/processed_posts.json"
                os.makedirs(os.path.dirname(processed_file), exist_ok=True)
                with open(processed_file, 'w', encoding='utf-8') as f:
                    json.dump({
                        "processed_ids": list(self.processed_posts),
                        "last_updated": datetime.now().isoformat()
                    }, f, indent=2)

        except Exception as e:
            logger.error(f"Failed to save processed posts: {e}")

    def _build_search_query(self, subreddit_config: Dict[str, Any]) -> str:
        """æ„å»ºæœç´¢æŸ¥è¯¢"""
        search_focus = subreddit_config.get("search_focus", [])
        pain_keywords = self.config.get("pain_keywords", {})

        query_parts = []
        for category in search_focus:
            if category in pain_keywords:
                # ä½¿ç”¨å¼•å·ç¡®ä¿ç²¾ç¡®åŒ¹é…
                category_keywords = pain_keywords[category][:5]  # é™åˆ¶å…³é”®è¯æ•°é‡
                quoted_keywords = [f'"{kw}"' for kw in category_keywords]
                query_parts.append(f"({' OR '.join(quoted_keywords)})")

        return " OR ".join(query_parts) if query_parts else ""

    def _calculate_pain_score(self, submission, subreddit_config: Dict[str, Any]) -> float:
        """è®¡ç®—å¸–å­ç—›ç‚¹åˆ†æ•°"""
        score = 0.0

        # 1. è´¨é‡åŸºç¡€åˆ†
        thresholds = subreddit_config.get("thresholds", {})
        min_upvotes = thresholds.get("min_upvotes", 5)
        min_comments = thresholds.get("min_comments", 3)

        if submission.score >= min_upvotes:
            score += 0.3
        if submission.num_comments >= min_comments:
            score += 0.2

        # 2. ç—›ç‚¹å…³é”®è¯åŒ¹é…
        title = (submission.title or "").lower()
        body = (submission.selftext or "").lower()
        full_text = f"{title} {body}"

        pain_keywords = self.config.get("pain_keywords", {})
        keyword_matches = 0

        for category_keywords in pain_keywords.values():
            for keyword in category_keywords:
                if keyword.lower() in full_text:
                    keyword_matches += 1
                    score += 0.1

        # 3. é•¿åº¦åˆ†æï¼ˆæ›´é•¿çš„å¸–å­å¯èƒ½åŒ…å«æ›´å¤šç—›ç‚¹ç»†èŠ‚ï¼‰
        if len(full_text) > 200:
            score += 0.1
        if len(full_text) > 500:
            score += 0.1

        # 4. æƒ…ç»ªä¿¡å·æ£€æµ‹ï¼ˆç®€å•è§„åˆ™ï¼‰
        emotion_indicators = ["frustrated", "annoying", "struggling", "can't", "doesn't work", "broken"]
        emotion_count = sum(1 for indicator in emotion_indicators if indicator in full_text)
        score += emotion_count * 0.05

        return min(score, 1.0)  # é™åˆ¶åœ¨0-1èŒƒå›´å†…

    def _is_pain_post(self, submission, subreddit_config: Dict[str, Any]) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºç—›ç‚¹å¸–å­"""
        # 1. åŸºç¡€è´¨é‡æ£€æŸ¥
        if submission.score < subreddit_config.get("min_upvotes", 5):
            return False
        if submission.num_comments < subreddit_config.get("min_comments", 3):
            return False

        # 2. ç—›ç‚¹å…³é”®è¯æ£€æŸ¥
        title = (submission.title or "").lower()
        body = (submission.selftext or "").lower()
        full_text = f"{title} {body}"

        pain_keywords = self.config.get("pain_keywords", {})
        keyword_matches = 0

        for category_keywords in pain_keywords.values():
            for keyword in category_keywords:
                if keyword.lower() in full_text:
                    keyword_matches += 1
                    if keyword_matches >= 1:  # è‡³å°‘åŒ¹é…ä¸€ä¸ªå…³é”®è¯
                        return True

        # 3. æ’é™¤æ¨¡å¼æ£€æŸ¥
        exclude_patterns = self.config.get("exclude_patterns", {})
        for pattern_category, patterns in exclude_patterns.items():
            for pattern in patterns:
                if pattern.lower() in full_text:
                    logger.debug(f"Excluded post due to {pattern_category}: {pattern}")
                    return False

        return False

    def _extract_post_data(self, submission, subreddit_config: Dict[str, Any]) -> Dict[str, Any]:
        """æå–å¸–å­æ•°æ®ï¼ˆæ”¯æŒå¤šæ•°æ®æºschemaï¼‰"""
        try:
            # è·å–è¯„è®º
            comments = []
            try:
                submission.comment_sort = "top"
                submission.comments.replace_more(limit=0)
                for comment in submission.comments.list()[:20]:  # è·å–å‰20æ¡è¯„è®º
                    if hasattr(comment, 'author') and comment.author:
                        comments.append({
                            "author": comment.author.name,
                            "body": comment.body,
                            "score": comment.score
                        })
            except Exception as e:
                logger.warning(f"Failed to fetch comments for {submission.id}: {e}")

            # è®¡ç®—ç—›ç‚¹åˆ†æ•°
            pain_score = self._calculate_pain_score(submission, subreddit_config)

            # Redditç‰¹æœ‰æ•°æ®å­˜å‚¨åœ¨platform_dataä¸­
            platform_data = {
                "subreddit": subreddit_config["name"],
                "upvote_ratio": getattr(submission, 'upvote_ratio', 0.0),
                "is_self": getattr(submission, 'is_self', False),
                "reddit_url": f"https://reddit.com{submission.permalink}",
                "flair": getattr(submission, 'link_flair_text', None)
            }

            # ç”Ÿæˆç»Ÿä¸€IDå’Œæ ‡å‡†åŒ–æ—¶é—´
            reddit_source_id = submission.id
            unified_id = f"reddit_{reddit_source_id}"
            created_at = datetime.fromtimestamp(submission.created_utc).isoformat() + "Z"

            return {
                "id": unified_id,                           # æ–°çš„ç»Ÿä¸€ID
                "source": "reddit",                         # æ•°æ®æºæ ‡è¯†
                "source_id": reddit_source_id,              # RedditåŸå§‹ID
                "title": submission.title,
                "body": submission.selftext,
                "subreddit": subreddit_config["name"],     # ä¿æŒå‘åå…¼å®¹
                "category": subreddit_config["category"],
                "url": submission.url,
                "platform_data": platform_data,             # Redditç‰¹æœ‰æ•°æ®
                "score": submission.score,
                "num_comments": submission.num_comments,
                "upvote_ratio": getattr(submission, 'upvote_ratio', 0.0),  # å‘åå…¼å®¹
                "is_self": getattr(submission, 'is_self', False),          # å‘åå…¼å®¹
                "created_utc": submission.created_utc,        # å‘åå…¼å®¹
                "created_at": created_at,                    # æ–°çš„æ ‡å‡†åŒ–æ—¶é—´
                "author": submission.author.name if submission.author else "[deleted]",
                "comments": comments,
                "pain_score": pain_score,
                "collected_at": datetime.now().isoformat()
            }

        except Exception as e:
            logger.error(f"Failed to extract data for submission {submission.id}: {e}")
            return None

    def _process_submission(self, submission, subreddit_config: Dict[str, Any]) -> bool:
        """å¤„ç†å•ä¸ªå¸–å­"""
        try:
            # æ£€æŸ¥æ˜¯å¦å·²å¤„ç†ï¼ˆä½¿ç”¨ç»Ÿä¸€IDï¼‰
            unified_id = f"reddit_{submission.id}"
            if unified_id in self.processed_posts:
                return False

            # æ£€æŸ¥æ˜¯å¦ä¸ºç—›ç‚¹å¸–å­
            if not self._is_pain_post(submission, subreddit_config):
                self.stats["filtered_out"] += 1
                return False

            # æå–å¸–å­æ•°æ®
            post_data = self._extract_post_data(submission, subreddit_config)
            if not post_data:
                self.stats["errors"] += 1
                return False

            # ä¿å­˜åˆ°æ•°æ®åº“
            if 'db' in globals():
                success = db.insert_raw_post(post_data)
            else:
                # å¤‡ç”¨æ–‡ä»¶å­˜å‚¨æ–¹æ¡ˆ
                success = self._save_post_to_file(post_data)

            if success:
                self.processed_posts.add(unified_id)  # ä½¿ç”¨ç»Ÿä¸€ID
                self.stats["total_saved"] += 1
                logger.info(f"Saved post: {submission.title[:60]}... (Score: {submission.score}, Pain: {post_data['pain_score']:.2f})")
                return True
            else:
                self.stats["errors"] += 1
                return False

        except Exception as e:
            logger.error(f"Failed to process submission: {e}")
            self.stats["errors"] += 1
            return False

    def _save_post_to_file(self, post_data: Dict[str, Any]) -> bool:
        """ä¿å­˜å¸–å­åˆ°æ–‡ä»¶ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰"""
        try:
            output_dir = "data/raw_posts"
            os.makedirs(output_dir, exist_ok=True)
            file_path = os.path.join(output_dir, f"{post_data['id']}.json")

            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(post_data, f, indent=2, ensure_ascii=False)

            return True
        except Exception as e:
            logger.error(f"Failed to save post to file: {e}")
            return False

    def fetch_subreddit(self, subreddit_config: Dict[str, Any]) -> int:
        """æŠ“å–å•ä¸ªå­ç‰ˆå—"""
        subreddit_name = subreddit_config["name"]
        category = subreddit_config["category"]
        methods = subreddit_config.get("methods", ["hot"])

        logger.info(f"Fetching from r/{subreddit_name} (Category: {category})")

        try:
            subreddit = self.reddit_client.subreddit(subreddit_name)
            total_found = 0

            # æ„å»ºæœç´¢æŸ¥è¯¢
            search_query = self._build_search_query(subreddit_config)
            if search_query:
                logger.debug(f"Search query for r/{subreddit_name}: {search_query}")

            # è·å–å¸–å­é™åˆ¶
            max_results = self.config.get("search_strategy", {}).get("max_results_per_method", 100)

            for method in methods:
                logger.debug(f"Using method: {method}")

                try:
                    submissions = []

                    if method == "hot":
                        submissions = subreddit.hot(limit=max_results)
                    elif method == "new":
                        submissions = subreddit.new(limit=max_results)
                    elif method == "rising":
                        submissions = subreddit.rising(limit=max_results)
                    elif method == "controversial":
                        submissions = subreddit.controversial('week', limit=max_results)
                    elif method.startswith("top_"):
                        time_filter = method.split("_", 1)[1] if "_" in method else "week"
                        submissions = subreddit.top(time_filter=time_filter, limit=max_results)
                    elif method == "search" and search_query:
                        submissions = subreddit.search(search_query, sort='new', limit=max_results)
                    else:
                        logger.warning(f"Unknown method: {method}")
                        continue

                    # å¤„ç†å¸–å­
                    method_count = 0
                    for submission in submissions:
                        if self._process_submission(submission, subreddit_config):
                            method_count += 1

                    total_found += method_count
                    logger.info(f"Method {method}: found {method_count} posts in r/{subreddit_name}")

                    # æ·»åŠ å»¶è¿Ÿé¿å…APIé™åˆ¶
                    time.sleep(1)

                except Exception as e:
                    logger.error(f"Error with method {method} in r/{subreddit_name}: {e}")
                    continue

            return total_found

        except Exception as e:
            logger.error(f"Failed to fetch subreddit r/{subreddit_name}: {e}")
            return 0

    def fetch_all(self, limit_sources: Optional[int] = None) -> Dict[str, Any]:
        """æŠ“å–æ‰€æœ‰é…ç½®çš„å­ç‰ˆå—"""
        self.stats["start_time"] = datetime.now()

        logger.info("Starting Wise Collection posts fetching...")

        # åŠ è½½å·²å¤„ç†çš„å¸–å­
        self._load_processed_posts()

        # ä»é…ç½®ä¸­æ„å»º subreddit åˆ—è¡¨
        subreddits = []

        # å¤„ç†æ‰€æœ‰åˆ†ç»„ä¸­çš„ subreddits
        for group_name, group_data in self.config.items():
            if group_name in ["ignore", "search_strategy"]:
                continue  # è·³è¿‡å¿½ç•¥åˆ—è¡¨å’Œæœç´¢ç­–ç•¥é…ç½®

            if isinstance(group_data, dict):
                for subreddit_name, subreddit_config in group_data.items():
                    if isinstance(subreddit_config, dict):
                        # æ„å»ºæœŸæœ›çš„é…ç½®æ ¼å¼
                        subreddit_data = {
                            "name": subreddit_name,
                            "category": group_name,
                            "min_upvotes": subreddit_config.get("min_upvotes", 0),
                            "min_comments": subreddit_config.get("min_comments", 0),
                            "methods": ["hot", "new", "top_week"]  # é»˜è®¤ä½¿ç”¨è¿™äº›æ–¹æ³•
                        }
                        subreddits.append(subreddit_data)

        # å¦‚æœæŒ‡å®šäº†é™åˆ¶ï¼Œåˆ™æˆªå–åˆ—è¡¨
        if limit_sources:
            subreddits = subreddits[:limit_sources]

        logger.info(f"Will fetch from {len(subreddits)} subreddits")

        total_found = 0
        for i, subreddit_config in enumerate(subreddits, 1):
            logger.info(f"Processing subreddit {i}/{len(subreddits)}")
            found = self.fetch_subreddit(subreddit_config)
            total_found += found

        # ä¿å­˜å·²å¤„ç†çš„å¸–å­
        self._save_processed_posts()

        # è®¡ç®—è¿è¡Œæ—¶é—´
        runtime = datetime.now() - self.stats["start_time"]

        # æ›´æ–°ç»Ÿè®¡
        self.stats["total_fetched"] = total_found
        self.stats["runtime_seconds"] = runtime.total_seconds()

        # è¾“å‡ºæ€»ç»“
        logger.info(f"""
=== Fetch Summary ===
Total subreddits processed: {len(subreddits)}
Total posts found: {total_found}
Total posts saved: {self.stats["total_saved"]}
Posts filtered out: {self.stats["filtered_out"]}
Errors encountered: {self.stats["errors"]}
Runtime: {runtime}
Posts per minute: {self.stats["total_saved"] / max(runtime.total_seconds() / 60, 1):.1f}
""")

        return self.stats.copy()

class HackerNewsSourceFetcher:
    """Hacker Newsç—›ç‚¹æ•°æ®æŠ“å–å™¨"""

    def __init__(self):
        """åˆå§‹åŒ–æŠ“å–å™¨"""
        # å¯¼å…¥HNæŠ“å–å™¨
        from .hn_fetch import HackerNewsFetcher
        self.hn_fetcher = HackerNewsFetcher()

    def fetch_all(self) -> Dict[str, Any]:
        """æŠ“å–HNæ•°æ®ï¼ˆå…¼å®¹ç°æœ‰æ¥å£ï¼‰"""
        return self.hn_fetcher.fetch_all()


class MultiSourceFetcher:
    """å¤šæ•°æ®æºæŠ“å–å™¨"""

    def __init__(self, sources: List[str] = None, config_path: str = "config/subreddits.yaml"):
        """åˆå§‹åŒ–æŠ“å–å™¨

        Args:
            sources: è¦æŠ“å–çš„æ•°æ®æºåˆ—è¡¨ï¼Œå¦‚ ['reddit', 'hackernews']
            config_path: Reddité…ç½®æ–‡ä»¶è·¯å¾„
        """
        self.sources = sources or ['reddit', 'hackernews']
        self.fetchers = {}

        # åˆå§‹åŒ–å„ä¸ªæŠ“å–å™¨
        if 'reddit' in self.sources:
            self.fetchers['reddit'] = RedditSourceFetcher(config_path)
        if 'hackernews' in self.sources:
            self.fetchers['hackernews'] = HackerNewsSourceFetcher()

    def fetch_all(self, limit_sources: Optional[int] = None) -> Dict[str, Any]:
        """æŠ“å–æ‰€æœ‰é…ç½®çš„æ•°æ®æº"""
        overall_stats = {
            "sources_processed": [],
            "total_saved": 0,
            "total_filtered": 0,
            "total_errors": 0,
            "source_stats": {},
            "runtime_seconds": 0
        }

        start_time = datetime.now()

        for source_name in self.sources:
            if source_name in self.fetchers:
                logger.info(f"Fetching from {source_name}...")
                try:
                    stats = self.fetchers[source_name].fetch_all()
                    overall_stats["sources_processed"].append(source_name)
                    overall_stats["total_saved"] += stats.get("total_saved", 0)
                    overall_stats["total_filtered"] += stats.get("filtered_out", 0)
                    overall_stats["total_errors"] += stats.get("errors", 0)
                    overall_stats["source_stats"][source_name] = stats

                except Exception as e:
                    logger.error(f"Failed to fetch from {source_name}: {e}")
                    overall_stats["total_errors"] += 1
                    overall_stats["source_stats"][source_name] = {"error": str(e)}

        # è®¡ç®—æ€»è¿è¡Œæ—¶é—´
        runtime = datetime.now() - start_time
        overall_stats["runtime_seconds"] = runtime.total_seconds()

        # è¾“å‡ºæ€»ç»“
        logger.info(f"""
=== Multi-Source Fetch Summary ===
Sources processed: {overall_stats["sources_processed"]}
Total posts saved: {overall_stats["total_saved"]}
Total posts filtered: {overall_stats["total_filtered"]}
Total errors: {overall_stats["total_errors"]}
Runtime: {runtime}
Posts per minute: {overall_stats["total_saved"] / max(runtime.total_seconds() / 60, 1):.1f}
""")

        # è¾“å‡ºå„æ•°æ®æºç»Ÿè®¡
        for source, stats in overall_stats.get("source_stats", {}).items():
            if "error" not in stats:
                logger.info(f"   - {source}: {stats.get('total_saved', 0)} saved, {stats.get('filtered_out', 0)} filtered")
            else:
                logger.error(f"   - {source}: ERROR - {stats['error']}")

        return overall_stats


def main():
    """ä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description="Fetch posts for pain point discovery")
    parser.add_argument("--limit", type=int, help="Limit number of sources to process")
    parser.add_argument("--config", default="config/subreddits.yaml", help="Reddit config file path")
    parser.add_argument("--sources", nargs="+", choices=["reddit", "hackernews"],
                       default=["reddit", "hackernews"], help="Data sources to fetch")
    args = parser.parse_args()

    try:
        # ä½¿ç”¨æ–°çš„å¤šæ•°æ®æºæŠ“å–å™¨
        fetcher = MultiSourceFetcher(sources=args.sources, config_path=args.config)
        stats = fetcher.fetch_all(limit_sources=args.limit)

        # è¾“å‡ºJSONæ ¼å¼çš„ç»Ÿè®¡ä¿¡æ¯ï¼ˆç”¨äºè„šæœ¬é›†æˆï¼‰
        print(json.dumps(stats, indent=2))

    except Exception as e:
        logger.error(f"Fatal error: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()
```


================================================================================
æ–‡ä»¶: pipeline/filter_signal.py
================================================================================

```python
"""
Filter Signal module for Reddit Pain Point Finder
ç—›ç‚¹ä¿¡å·è¿‡æ»¤æ¨¡å— - å†·è¡€å®ˆé—¨å‘˜
"""
import os
import json
import logging
import re
import yaml
from typing import List, Dict, Any, Set, Tuple
from datetime import datetime

logger = logging.getLogger(__name__)

class PainSignalFilter:
    """ç—›ç‚¹ä¿¡å·è¿‡æ»¤å™¨"""

    def __init__(self, config_path: str = "config/thresholds.yaml"):
        """åˆå§‹åŒ–è¿‡æ»¤å™¨"""
        self.thresholds = self._load_thresholds(config_path)
        self.subreddits_config = self._load_subreddits_config("config/subreddits.yaml")
        self.stats = {
            "total_processed": 0,
            "passed_filter": 0,
            "filtered_out": 0,
            "filter_reasons": {}
        }

    def _load_thresholds(self, config_path: str) -> Dict[str, Any]:
        """åŠ è½½é˜ˆå€¼é…ç½®"""
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                return yaml.safe_load(f)
        except Exception as e:
            logger.error(f"Failed to load thresholds from {config_path}: {e}")
            return {}

    def _load_subreddits_config(self, config_path: str) -> Dict[str, Any]:
        """åŠ è½½å­ç‰ˆå—é…ç½®"""
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                return yaml.safe_load(f)
        except Exception as e:
            logger.error(f"Failed to load subreddits config from {config_path}: {e}")
            return {}

    def _check_quality_thresholds(self, post_data: Dict[str, Any]) -> Tuple[bool, str]:
        """æ£€æŸ¥è´¨é‡é˜ˆå€¼"""
        quality_config = self.thresholds.get("reddit_quality", {})
        base_thresholds = quality_config.get("base", {})

        score = post_data.get("score", 0)
        comments = post_data.get("num_comments", 0)
        upvote_ratio = post_data.get("upvote_ratio", 0.0)
        text_length = len(post_data.get("title", "") + " " + post_data.get("body", ""))

        # æ£€æŸ¥åŸºç¡€é˜ˆå€¼
        if score < base_thresholds.get("min_upvotes", 5):
            return False, f"Too few upvotes: {score} < {base_thresholds.get('min_upvotes')}"

        if comments < base_thresholds.get("min_comments", 3):
            return False, f"Too few comments: {comments} < {base_thresholds.get('min_comments')}"

        if upvote_ratio < base_thresholds.get("min_upvote_ratio", 0.1):
            return False, f"Too low upvote ratio: {upvote_ratio:.2f} < {base_thresholds.get('min_upvote_ratio')}"

        if text_length < base_thresholds.get("min_post_length", 50):
            return False, f"Post too short: {text_length} < {base_thresholds.get('min_post_length')}"

        if text_length > base_thresholds.get("max_post_length", 5000):
            return False, f"Post too long: {text_length} > {base_thresholds.get('max_post_length')}"

        return True, "Passed quality thresholds"

    def _check_pain_keywords(self, post_data: Dict[str, Any]) -> Tuple[bool, List[str], float]:
        """æ£€æŸ¥ç—›ç‚¹å…³é”®è¯"""
        title = (post_data.get("title", "")).lower()
        body = (post_data.get("body", "")).lower()
        full_text = f"{title} {body}"

        pain_keywords = self.subreddits_config.get("pain_keywords", {})
        matched_keywords = []
        keyword_scores = {}

        # ç»Ÿè®¡å„ç±»åˆ«å…³é”®è¯åŒ¹é…
        for category, keywords in pain_keywords.items():
            category_matches = 0
            category_weight = {"frustration": 1.0, "inefficiency": 0.8, "complexity": 0.7, "workflow": 0.9, "cost": 0.6}.get(category, 0.5)

            for keyword in keywords:
                if keyword.lower() in full_text:
                    matched_keywords.append(f"{category}:{keyword}")
                    category_matches += 1
                    keyword_scores[keyword] = category_weight

            # è®¡ç®—è¯¥ç±»åˆ«çš„å¾—åˆ†
            if category_matches > 0:
                keyword_scores[f"category_{category}"] = category_matches * category_weight

        # è®¡ç®—æ€»ç—›ç‚¹åˆ†æ•°
        total_score = sum(score for score in keyword_scores.values() if isinstance(score, (int, float)))

        # æ ‡å‡†åŒ–åˆ†æ•°ï¼ˆ0-1èŒƒå›´ï¼‰
        normalized_score = min(total_score / 5.0, 1.0)  # å‡è®¾5åˆ†ä¸ºæ»¡åˆ†

        return len(matched_keywords) > 0, matched_keywords, normalized_score

    def _check_pain_patterns(self, post_data: Dict[str, Any]) -> Tuple[bool, List[str]]:
        """æ£€æŸ¥ç—›ç‚¹å¥å¼æ¨¡å¼"""
        title = (post_data.get("title", "")).lower()
        body = (post_data.get("body", "")).lower()
        full_text = f"{title} {body}"

        pain_config = self.thresholds.get("pain_signal", {})
        required_patterns = pain_config.get("pain_patterns", {}).get("required_patterns", [])
        strong_signals = pain_config.get("pain_patterns", {}).get("strong_signals", [])

        matched_patterns = []
        matched_strong = []

        # æ£€æŸ¥å¿…é¡»åŒ¹é…çš„å¥å¼
        for pattern in required_patterns:
            if pattern.lower() in full_text:
                matched_patterns.append(pattern)

        # æ£€æŸ¥å¼ºåŒ–ä¿¡å·å¥å¼
        for pattern in strong_signals:
            if pattern.lower() in full_text:
                matched_strong.append(pattern)

        # åˆ¤æ–­æ˜¯å¦é€šè¿‡æ¨¡å¼æ£€æŸ¥
        min_pattern_matches = pain_config.get("pain_patterns", {}).get("min_pattern_matches", 1)
        min_strong_signals = pain_config.get("pain_patterns", {}).get("min_strong_signals", 0)

        has_required = len(matched_patterns) >= min_pattern_matches
        has_strong = len(matched_strong) >= min_strong_signals

        all_matches = matched_patterns + matched_strong

        return (has_required or has_strong), all_matches

    def _check_exclusion_patterns(self, post_data: Dict[str, Any]) -> Tuple[bool, str]:
        """æ£€æŸ¥æ’é™¤æ¨¡å¼"""
        title = (post_data.get("title", "")).lower()
        body = (post_data.get("body", "")).lower()
        full_text = f"{title} {body}"

        exclude_patterns = self.subreddits_config.get("exclude_patterns", {})

        for category, patterns in exclude_patterns.items():
            for pattern in patterns:
                if pattern.lower() in full_text:
                    return False, f"Excluded due to {category}: {pattern}"

        return True, "No exclusion patterns matched"

    def _calculate_emotional_intensity(self, post_data: Dict[str, Any]) -> float:
        """è®¡ç®—æƒ…ç»ªå¼ºåº¦"""
        title = (post_data.get("title", "")).lower()
        body = (post_data.get("body", "")).lower()
        full_text = f"{title} {body}"

        # é«˜å¼ºåº¦æƒ…ç»ªè¯æ±‡
        high_intensity_words = [
            "frustrated", "frustrating", "annoying", "annoyed", "hate", "terrible",
            "awful", "horrible", "disaster", "catastrophe", "nightmare", "hell",
            "impossible", "useless", "worthless", "broken", "crashed", "failed"
        ]

        # ä¸­å¼ºåº¦æƒ…ç»ªè¯æ±‡
        medium_intensity_words = [
            "difficult", "hard", "struggling", "trouble", "problem", "issue",
            "challenge", "confusing", "complicated", "complex", "slow", "tedious"
        ]

        # ä½å¼ºåº¦æƒ…ç»ªè¯æ±‡
        low_intensity_words = [
            "annoyance", "minor", "slight", "inconvenient", "suboptimal", "could be better"
        ]

        high_count = sum(1 for word in high_intensity_words if word in full_text)
        medium_count = sum(1 for word in medium_intensity_words if word in full_text)
        low_count = sum(1 for word in low_intensity_words if word in full_text)

        # è®¡ç®—åŠ æƒæƒ…ç»ªå¼ºåº¦
        intensity = (high_count * 1.0 + medium_count * 0.6 + low_count * 0.3) / max(len(full_text.split()) / 100, 1)
        return min(intensity, 1.0)

    def _check_post_type_specific(self, post_data: Dict[str, Any]) -> Tuple[bool, str]:
        """æ£€æŸ¥ç‰¹å®šç±»å‹å¸–å­çš„é˜ˆå€¼"""
        subreddit = post_data.get("subreddit", "").lower()
        score = post_data.get("score", 0)
        comments = post_data.get("num_comments", 0)

        quality_config = self.thresholds.get("reddit_quality", {})
        type_specific = quality_config.get("type_specific", {})

        # æ ¹æ®å­ç‰ˆå—ç±»åˆ«åˆ¤æ–­ç±»å‹
        post_type = "general"  # é»˜è®¤ç±»å‹
        if any(keyword in subreddit for keyword in ["programming", "sysadmin", "webdev", "technical"]):
            post_type = "technical"
        elif any(keyword in subreddit for keyword in ["entrepreneur", "startups", "business"]):
            post_type = "business"
        elif "discussion" in subreddit or comments > score * 2:
            post_type = "discussion"

        if post_type in type_specific:
            type_config = type_specific[post_type]
            if score < type_config.get("min_upvotes", 0):
                return False, f"Type {post_type}: too few upvotes: {score} < {type_config.get('min_upvotes')}"
            if comments < type_config.get("min_comments", 0):
                return False, f"Type {post_type}: too few comments: {comments} < {type_config.get('min_comments')}"

        return True, f"Type {post_type} check passed"

    def filter_post(self, post_data: Dict[str, Any]) -> Tuple[bool, Dict[str, Any]]:
        """è¿‡æ»¤å•ä¸ªå¸–å­"""
        self.stats["total_processed"] += 1

        filter_result = {
            "post_id": post_data.get("id"),
            "passed": False,
            "pain_score": 0.0,
            "reasons": [],
            "matched_keywords": [],
            "matched_patterns": [],
            "emotional_intensity": 0.0,
            "filter_summary": {}
        }

        # 1. è´¨é‡é˜ˆå€¼æ£€æŸ¥
        quality_passed, quality_reason = self._check_quality_thresholds(post_data)
        if not quality_passed:
            self.stats["filtered_out"] += 1
            self.stats["filter_reasons"][quality_reason] = self.stats["filter_reasons"].get(quality_reason, 0) + 1
            filter_result["reasons"].append(quality_reason)
            filter_result["filter_summary"] = {"reason": "quality_threshold", "details": quality_reason}
            return False, filter_result

        # 2. æ’é™¤æ¨¡å¼æ£€æŸ¥
        exclusion_passed, exclusion_reason = self._check_exclusion_patterns(post_data)
        if not exclusion_passed:
            self.stats["filtered_out"] += 1
            self.stats["filter_reasons"][exclusion_reason] = self.stats["filter_reasons"].get(exclusion_reason, 0) + 1
            filter_result["reasons"].append(exclusion_reason)
            filter_result["filter_summary"] = {"reason": "exclusion_pattern", "details": exclusion_reason}
            return False, filter_result

        # 3. ç—›ç‚¹å…³é”®è¯æ£€æŸ¥
        has_keywords, matched_keywords, keyword_score = self._check_pain_keywords(post_data)
        filter_result["matched_keywords"] = matched_keywords

        # 4. ç—›ç‚¹å¥å¼æ£€æŸ¥
        has_patterns, matched_patterns = self._check_pain_patterns(post_data)
        filter_result["matched_patterns"] = matched_patterns

        # 5. æƒ…ç»ªå¼ºåº¦è®¡ç®—
        emotional_intensity = self._calculate_emotional_intensity(post_data)
        filter_result["emotional_intensity"] = emotional_intensity

        # 6. ç±»å‹ç‰¹å®šæ£€æŸ¥
        type_passed, type_reason = self._check_post_type_specific(post_data)
        if not type_passed:
            self.stats["filtered_out"] += 1
            self.stats["filter_reasons"][type_reason] = self.stats["filter_reasons"].get(type_reason, 0) + 1
            filter_result["reasons"].append(type_reason)
            filter_result["filter_summary"] = {"reason": "type_specific", "details": type_reason}
            return False, filter_result

        # è®¡ç®—ç»¼åˆç—›ç‚¹åˆ†æ•°
        pain_score = 0.0

        # å…³é”®è¯åˆ†æ•° (40%)
        pain_score += keyword_score * 0.4

        # å¥å¼åˆ†æ•° (30%)
        pattern_score = min(len(matched_patterns) / 3.0, 1.0) * 0.3
        pain_score += pattern_score

        # æƒ…ç»ªå¼ºåº¦åˆ†æ•° (20%)
        pain_score += emotional_intensity * 0.2

        # åŸºç¡€è´¨é‡åˆ†æ•° (10%)
        score_normalized = min(post_data.get("score", 0) / 100.0, 1.0)
        comments_normalized = min(post_data.get("num_comments", 0) / 50.0, 1.0)
        quality_score = (score_normalized + comments_normalized) / 2.0 * 0.1
        pain_score += quality_score

        # ç¡®ä¿åˆ†æ•°åœ¨0-1èŒƒå›´å†…
        pain_score = min(max(pain_score, 0.0), 1.0)

        filter_result["pain_score"] = pain_score

        # åˆ¤æ–­æ˜¯å¦é€šè¿‡ç—›ç‚¹ä¿¡å·æ£€æŸ¥
        pain_config = self.thresholds.get("pain_signal", {})
        min_keyword_matches = pain_config.get("keyword_match", {}).get("min_matches", 1)
        min_emotional_intensity = pain_config.get("emotional_intensity", {}).get("min_score", 0.3)

        # æœ€ç»ˆåˆ¤æ–­
        passed = (
            has_keywords and
            len(matched_keywords) >= min_keyword_matches and
            emotional_intensity >= min_emotional_intensity and
            pain_score >= 0.3  # ç»¼åˆåˆ†æ•°é˜ˆå€¼
        )

        if passed:
            self.stats["passed_filter"] += 1
            filter_result["passed"] = True
            filter_result["filter_summary"] = {
                "reason": "passed",
                "pain_score": pain_score,
                "components": {
                    "keywords": keyword_score,
                    "patterns": pattern_score,
                    "emotion": emotional_intensity,
                    "quality": quality_score
                }
            }
        else:
            self.stats["filtered_out"] += 1
            failure_reasons = []
            if not has_keywords or len(matched_keywords) < min_keyword_matches:
                failure_reasons.append("insufficient_keywords")
            if emotional_intensity < min_emotional_intensity:
                failure_reasons.append("low_emotional_intensity")
            if pain_score < 0.3:
                failure_reasons.append("low_overall_score")

            reason_str = "; ".join(failure_reasons)
            self.stats["filter_reasons"][reason_str] = self.stats["filter_reasons"].get(reason_str, 0) + 1
            filter_result["reasons"].append(f"Failed: {reason_str}")
            filter_result["filter_summary"] = {"reason": "failed", "details": failure_reasons}

        return passed, filter_result

    def filter_posts_batch(self, posts: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """æ‰¹é‡è¿‡æ»¤å¸–å­"""
        logger.info(f"Filtering {len(posts)} posts through pain signal detector")

        filtered_posts = []
        for i, post in enumerate(posts):
            if i % 100 == 0:
                logger.info(f"Processed {i}/{len(posts)} posts")

            passed, result = self.filter_post(post)

            if passed:
                # ä¸ºå¸–å­æ·»åŠ è¿‡æ»¤ç»“æœ
                filtered_post = post.copy()
                filtered_post.update({
                    "pain_score": result["pain_score"],
                    "pain_keywords": result["matched_keywords"],
                    "pain_patterns": result["matched_patterns"],
                    "emotional_intensity": result["emotional_intensity"],
                    "filter_reason": "pain_signal_passed"
                })
                filtered_posts.append(filtered_post)

        logger.info(f"Filter complete: {len(filtered_posts)}/{len(posts)} posts passed")
        return filtered_posts

    def get_statistics(self) -> Dict[str, Any]:
        """è·å–è¿‡æ»¤ç»Ÿè®¡ä¿¡æ¯"""
        stats = self.stats.copy()
        if stats["total_processed"] > 0:
            stats["pass_rate"] = stats["passed_filter"] / stats["total_processed"]
        else:
            stats["pass_rate"] = 0.0
        return stats

    def reset_statistics(self):
        """é‡ç½®ç»Ÿè®¡ä¿¡æ¯"""
        self.stats = {
            "total_processed": 0,
            "passed_filter": 0,
            "filtered_out": 0,
            "filter_reasons": {}
        }

def main():
    """ä¸»å‡½æ•° - è¿‡æ»¤åŸå§‹å¸–å­"""
    import argparse
    from utils.db import db

    parser = argparse.ArgumentParser(description="Filter Reddit posts for pain signals")
    parser.add_argument("--limit", type=int, default=1000, help="Limit number of posts to process")
    parser.add_argument("--min-score", type=float, default=0.0, help="Minimum pain score threshold")
    args = parser.parse_args()

    try:
        logger.info("Starting pain signal filtering...")

        # åˆå§‹åŒ–è¿‡æ»¤å™¨
        filter = PainSignalFilter()

        # è·å–æœªè¿‡æ»¤çš„å¸–å­
        logger.info(f"Fetching up to {args.limit} unprocessed posts...")
        unfiltered_posts = db.get_unprocessed_posts(limit=args.limit)

        if not unfiltered_posts:
            logger.info("No unprocessed posts found")
            return

        logger.info(f"Found {len(unfiltered_posts)} posts to filter")

        # æ‰¹é‡è¿‡æ»¤
        filtered_posts = filter.filter_posts_batch(unfiltered_posts)

        # åº”ç”¨æœ€å°åˆ†æ•°é˜ˆå€¼
        if args.min_score > 0:
            filtered_posts = [p for p in filtered_posts if p.get("pain_score", 0) >= args.min_score]
            logger.info(f"After applying min_score threshold: {len(filtered_posts)} posts")

        # ä¿å­˜è¿‡æ»¤ç»“æœ
        saved_count = 0
        for post in filtered_posts:
            if db.insert_filtered_post(post):
                saved_count += 1

        logger.info(f"Saved {saved_count}/{len(filtered_posts)} filtered posts to database")

        # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
        stats = filter.get_statistics()
        logger.info(f"""
=== Filter Summary ===
Total processed: {stats['total_processed']}
Passed filter: {stats['passed_filter']}
Filtered out: {stats['filtered_out']}
Pass rate: {stats['pass_rate']:.2%}
Filter reasons: {stats['filter_reasons']}
""")

    except Exception as e:
        logger.error(f"Fatal error: {e}")
        raise

if __name__ == "__main__":
    main()
```


================================================================================
æ–‡ä»¶: pipeline/hn_fetch.py
================================================================================

```python
"""
Hacker News æ•°æ®æŠ“å–å™¨
åŸºäº HN API v0ï¼ŒåªæŠ“å– Ask HNã€Show HN å’Œé«˜è¯„è®ºæ•…äº‹
"""
import requests
import json
import time
import logging
from typing import List, Dict, Any, Optional
from datetime import datetime

logger = logging.getLogger(__name__)

class HackerNewsFetcher:
    """Hacker News æ•°æ®æŠ“å–å™¨"""

    def __init__(self):
        self.base_url = "https://hacker-news.firebaseio.com/v0"
        self.processed_ids = set()
        self.stats = {
            "total_fetched": 0,
            "total_saved": 0,
            "filtered_out": 0,
            "errors": 0,
            "start_time": None
        }

    def _is_pain_story(self, item: Dict[str, Any]) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºç—›ç‚¹ç›¸å…³æ•…äº‹"""
        # åªæŠ“ä¸‰ç±»å†…å®¹:
        # 1. Ask HN
        # 2. Show HN
        # 3. æ™®é€šæ•…äº‹ä½†è¯„è®ºæ•° > 10

        title = item.get("title", "").lower()

        # Ask HN / Show HN ç›´æ¥æ”¶å½•
        if "ask hn" in title or "show hn" in title:
            return True

        # æ™®é€šæ•…äº‹éœ€è¦è¯„è®ºæ•° > 10
        if item.get("descendants", 0) > 10:
            return True

        return False

    def _extract_story_data(self, item: Dict[str, Any]) -> Dict[str, Any]:
        """æå–æ•…äº‹æ•°æ®ä¸ºç»Ÿä¸€æ ¼å¼"""
        hn_id = item["id"]
        unified_id = f"hackernews_{hn_id}"

        # è·å–è¯„è®ºï¼ˆç®€åŒ–ç‰ˆï¼Œåªè·å–å‰10æ¡ï¼‰
        comments = []
        kids = item.get("kids", [])

        for kid_id in kids[:10]:  # åªè·å–å‰10æ¡è¯„è®º
            try:
                comment_url = f"{self.base_url}/item/{kid_id}.json"
                comment_resp = requests.get(comment_url, timeout=10)
                if comment_resp.status_code == 200:
                    comment_data = comment_resp.json()
                    if comment_data and comment_data.get("type") == "comment":
                        comments.append({
                            "author": comment_data.get("by", ""),
                            "body": comment_data.get("text", ""),
                            "score": 0  # HNè¯„è®ºæ²¡æœ‰score
                        })
            except Exception as e:
                logger.warning(f"Failed to fetch comment {kid_id}: {e}")

        # ç¡®å®šsource_type
        title_lower = item.get("title", "").lower()
        if "ask hn" in title_lower:
            source_type = "HN_ASK"
        elif "show hn" in title_lower:
            source_type = "HN_SHOW"
        else:
            source_type = "HN_STORY"

        # HNç‰¹æœ‰çš„platform_data
        platform_data = {
            "hn_id": hn_id,
            "type": item.get("type", "story"),
            "descendants": item.get("descendants", 0),
            "hn_url": f"https://news.ycombinator.com/item?id={hn_id}"
        }

        created_at = datetime.fromtimestamp(item.get("time", 0)).isoformat() + "Z"

        return {
            "id": unified_id,
            "source": "hackernews",
            "source_id": str(hn_id),
            "title": item.get("title", ""),
            "body": item.get("text", ""),
            "subreddit": source_type,  # å¤ç”¨subredditå­—æ®µå­˜å‚¨source_type
            "category": "hackernews",
            "url": item.get("url", f"https://news.ycombinator.com/item?id={hn_id}"),
            "platform_data": platform_data,
            "score": item.get("score", 0),
            "num_comments": item.get("descendants", 0),
            "upvote_ratio": 1.0,  # HNæ²¡æœ‰upvote_ratioï¼Œè®¾ä¸º1.0
            "is_self": 1 if item.get("text") else 0,
            "created_utc": item.get("time", 0),
            "created_at": created_at,
            "author": item.get("by", ""),
            "comments": comments,
            "pain_score": 0.5,  # é»˜è®¤pain_scoreï¼Œåç»­pipelineä¼šé‡æ–°è®¡ç®—
            "collected_at": datetime.now().isoformat()
        }

    def fetch_from_endpoint(self, endpoint: str, limit: int = 50) -> int:
        """ä»ç‰¹å®šç«¯ç‚¹æŠ“å–æ•°æ®"""
        try:
            # è·å–æ•…äº‹IDåˆ—è¡¨
            url = f"{self.base_url}/{endpoint}.json"
            resp = requests.get(url, timeout=10)
            if resp.status_code != 200:
                logger.error(f"Failed to fetch {endpoint}: {resp.status_code}")
                return 0

            story_ids = resp.json()[:limit] if limit else resp.json()
            saved_count = 0

            for story_id in story_ids:
                try:
                    # è·å–æ•…äº‹è¯¦æƒ…
                    item_url = f"{self.base_url}/item/{story_id}.json"
                    item_resp = requests.get(item_url, timeout=10)
                    if item_resp.status_code != 200:
                        continue

                    item = item_resp.json()
                    if not item or item.get("type") != "story":
                        continue

                    # æ£€æŸ¥æ˜¯å¦å·²å¤„ç†
                    unified_id = f"hackernews_{story_id}"
                    if unified_id in self.processed_ids:
                        continue

                    # æ£€æŸ¥æ˜¯å¦ä¸ºç—›ç‚¹å†…å®¹
                    if not self._is_pain_story(item):
                        self.stats["filtered_out"] += 1
                        continue

                    # æå–å¹¶ä¿å­˜æ•°æ®
                    story_data = self._extract_story_data(item)

                    # è°ƒç”¨ç°æœ‰çš„db.insert_raw_post
                    try:
                        from utils.db import db
                        if db.insert_raw_post(story_data):
                            self.processed_ids.add(unified_id)
                            self.stats["total_saved"] += 1
                            saved_count += 1
                            logger.info(f"Saved HN story: {item.get('title', '')[:60]}... (ID: {story_id})")
                        else:
                            self.stats["errors"] += 1
                    except Exception as db_e:
                        logger.error(f"Failed to save HN story {story_id}: {db_e}")
                        self.stats["errors"] += 1

                    # é¿å…è¯·æ±‚è¿‡å¿«
                    time.sleep(0.1)

                except Exception as e:
                    logger.error(f"Error processing story {story_id}: {e}")
                    self.stats["errors"] += 1

            return saved_count

        except Exception as e:
            logger.error(f"Error fetching from {endpoint}: {e}")
            return 0

    def fetch_all(self) -> Dict[str, Any]:
        """æŠ“å–æ‰€æœ‰HNæ•°æ®"""
        from datetime import datetime
        self.stats["start_time"] = datetime.now()

        logger.info("Starting Hacker News data fetching...")

        # åŠ è½½å·²å¤„ç†çš„å¸–å­ID
        try:
            from utils.db import db
            with db.get_connection("raw") as conn:
                cursor = conn.execute("SELECT id FROM posts WHERE source = 'hackernews'")
                self.processed_ids = {row['id'] for row in cursor.fetchall()}
            logger.info(f"Loaded {len(self.processed_ids)} previously processed HN posts")
        except Exception as e:
            logger.error(f"Failed to load processed posts: {e}")
            self.processed_ids = set()

        # æŠ“å–ä¸‰ç±»å†…å®¹
        endpoints = [
            ("askstories", 25),    # Ask HN - æŠ“å–25æ¡
            ("showstories", 25),   # Show HN - æŠ“å–25æ¡
            ("topstories", 50)     # Top Stories - æŠ“å–50æ¡ï¼Œè¿‡æ»¤è¯„è®ºæ•°>10çš„
        ]

        total_saved = 0
        for endpoint, limit in endpoints:
            logger.info(f"Fetching from {endpoint}...")
            saved = self.fetch_from_endpoint(endpoint, limit)
            total_saved += saved
            logger.info(f"Saved {saved} stories from {endpoint}")

        self.stats["total_fetched"] = total_saved

        # è®¡ç®—è¿è¡Œæ—¶é—´
        runtime = datetime.now() - self.stats["start_time"]
        self.stats["runtime_seconds"] = runtime.total_seconds()

        logger.info(f"""
=== HN Fetch Summary ===
Total stories saved: {total_saved}
Posts filtered out: {self.stats["filtered_out"]}
Errors encountered: {self.stats["errors"]}
Runtime: {runtime}
Posts per minute: {self.stats["total_saved"] / max(runtime.total_seconds() / 60, 1):.1f}
""")

        return self.stats.copy()
```


================================================================================
æ–‡ä»¶: pipeline/map_opportunity.py
================================================================================

```python
"""
Map Opportunity module for Reddit Pain Point Finder
æœºä¼šæ˜ å°„æ¨¡å— - ä»ç—›ç‚¹èšç±»ä¸­å‘ç°å·¥å…·æœºä¼š
"""
import json
import logging
import time
from typing import List, Dict, Any, Optional
from datetime import datetime

from utils.llm_client import llm_client
from utils.db import db

logger = logging.getLogger(__name__)

class OpportunityMapper:
    """æœºä¼šæ˜ å°„å™¨"""

    def __init__(self):
        """åˆå§‹åŒ–æœºä¼šæ˜ å°„å™¨"""
        self.stats = {
            "total_clusters_processed": 0,
            "opportunities_identified": 0,
            "viable_opportunities": 0,
            "processing_time": 0.0,
            "avg_opportunity_score": 0.0
        }

    def _enrich_cluster_data(self, cluster_data: Dict[str, Any]) -> Dict[str, Any]:
        """ä¸°å¯Œèšç±»æ•°æ®"""
        try:
            # è·å–èšç±»ä¸­çš„ç—›ç‚¹äº‹ä»¶è¯¦æƒ…
            pain_event_ids = json.loads(cluster_data.get("pain_event_ids", "[]"))

            pain_events = []
            with db.get_connection("pain") as conn:
                for event_id in pain_event_ids:
                    cursor = conn.execute("""
                        SELECT * FROM pain_events WHERE id = ?
                    """, (event_id,))
                    event_data = cursor.fetchone()
                    if event_data:
                        pain_events.append(dict(event_data))

            # æ·»åŠ åŸå§‹å¸–å­ä¿¡æ¯
            for event in pain_events:
                with db.get_connection("filtered") as conn:
                    cursor = conn.execute("""
                        SELECT title, subreddit, score, num_comments, pain_score
                        FROM filtered_posts WHERE id = ?
                    """, (event["post_id"],))
                    post_data = cursor.fetchone()
                    if post_data:
                        event.update(dict(post_data))

            # æ„å»ºä¸°å¯Œçš„èšç±»æ‘˜è¦
            enriched_cluster = {
                "cluster_id": cluster_data["id"],
                "cluster_name": cluster_data["cluster_name"],
                "cluster_description": cluster_data["cluster_description"],
                "cluster_size": cluster_data["cluster_size"],
                "workflow_confidence": cluster_data.get("workflow_confidence", 0.0),
                "pain_events": pain_events,
                "created_at": cluster_data["created_at"]
            }

            # åˆ†æèšç±»ç‰¹å¾
            self._analyze_cluster_characteristics(enriched_cluster)

            return enriched_cluster

        except Exception as e:
            logger.error(f"Failed to enrich cluster data: {e}")
            return cluster_data

    def _analyze_cluster_characteristics(self, cluster_data: Dict[str, Any]):
        """åˆ†æèšç±»ç‰¹å¾"""
        try:
            pain_events = cluster_data.get("pain_events", [])

            if not pain_events:
                return

            # ç»Ÿè®¡å­ç‰ˆå—åˆ†å¸ƒ
            subreddits = {}
            for event in pain_events:
                subreddit = event.get("subreddit", "unknown")
                subreddits[subreddit] = subreddits.get(subreddit, 0) + 1

            # ç»Ÿè®¡æåˆ°çš„å·¥å…·
            mentioned_tools = []
            for event in pain_events:
                tools = event.get("mentioned_tools", [])
                if isinstance(tools, list):
                    mentioned_tools.extend(tools)
                elif isinstance(tools, str):
                    mentioned_tools.append(tools)

            tool_counts = {}
            for tool in mentioned_tools:
                if tool:
                    tool_counts[tool] = tool_counts.get(tool, 0) + 1

            # ç»Ÿè®¡æƒ…ç»ªä¿¡å·
            emotional_signals = {}
            for event in pain_events:
                signal = event.get("emotional_signal", "")
                if signal:
                    emotional_signals[signal] = emotional_signals.get(signal, 0) + 1

            # ç»Ÿè®¡é¢‘ç‡åˆ†æ•°
            frequency_scores = [event.get("frequency_score", 5) for event in pain_events if event.get("frequency_score")]
            avg_frequency_score = sum(frequency_scores) / len(frequency_scores) if frequency_scores else 5.0

            # æå–ä»£è¡¨æ€§é—®é¢˜
            problems = [event.get("problem", "") for event in pain_events if event.get("problem")]
            unique_problems = list(set(problems))

            # æå–å·¥ä½œæ–¹å¼
            workarounds = [event.get("current_workaround", "") for event in pain_events if event.get("current_workaround")]
            unique_workarounds = [w for w in set(workarounds) if w]

            # æ›´æ–°èšç±»æ•°æ®
            cluster_data.update({
                "subreddit_distribution": subreddits,
                "mentioned_tools": tool_counts,
                "emotional_signals": emotional_signals,
                "avg_frequency_score": avg_frequency_score,
                "representative_problems": unique_problems[:10],  # æœ€å¤š10ä¸ª
                "representative_workarounds": unique_workarounds[:5],  # æœ€å¤š5ä¸ª
                "total_pain_score": sum(event.get("post_pain_score", 0) for event in pain_events)
            })

        except Exception as e:
            logger.error(f"Failed to analyze cluster characteristics: {e}")

    def _map_opportunity_with_llm(self, cluster_data: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """ä½¿ç”¨LLMæ˜ å°„æœºä¼š"""
        try:
            # è°ƒç”¨LLMè¿›è¡Œæœºä¼šæ˜ å°„
            response = llm_client.map_opportunity(cluster_data)

            opportunity_data = response["content"]

            # æ£€æŸ¥æ˜¯å¦æ‰¾åˆ°æœºä¼š
            if "opportunity" in opportunity_data and opportunity_data["opportunity"]:
                # ä¸ºäº†ä¿æŒä¸€è‡´æ€§ï¼ŒåŒ…è£…åœ¨contentä¸­
                return {"content": opportunity_data}
            else:
                logger.info(f"No viable opportunity found for cluster {cluster_data['cluster_name']}")
                return None

        except Exception as e:
            logger.error(f"Failed to map opportunity with LLM: {e}")
            return None

    def _evaluate_opportunity_quality(self, opportunity_data: Dict[str, Any], cluster_data: Dict[str, Any]) -> Dict[str, Any]:
        """è¯„ä¼°æœºä¼šè´¨é‡"""
        try:
            # å¤„ç†å¯èƒ½çš„æ•°æ®ç»“æ„å·®å¼‚
            if "content" in opportunity_data:
                opportunity = opportunity_data["content"].get("opportunity", {})
            else:
                opportunity = opportunity_data.get("opportunity", {})

            if not opportunity:
                return {"is_viable": False, "reason": "No opportunity data"}

            # åŸºç¡€è´¨é‡æ£€æŸ¥
            required_fields = ["name", "description", "target_users"]
            for field in required_fields:
                if not opportunity.get(field):
                    return {"is_viable": False, "reason": f"Missing required field: {field}"}

            # è´¨é‡è¯„åˆ†
            quality_score = 0.0
            reasons = []

            # ç—›ç‚¹é¢‘ç‡ (20%)
            pain_frequency = opportunity.get("pain_frequency", 0)
            if pain_frequency >= 7:
                quality_score += 0.2
                reasons.append("High pain frequency")
            elif pain_frequency >= 5:
                quality_score += 0.1
                reasons.append("Medium pain frequency")

            # å¸‚åœºè§„æ¨¡ (20%)
            market_size = opportunity.get("market_size", 0)
            if market_size >= 7:
                quality_score += 0.2
                reasons.append("Large market size")
            elif market_size >= 5:
                quality_score += 0.1
                reasons.append("Medium market size")

            # MVPå¤æ‚åº¦ (25%) - è¶Šä½è¶Šå¥½
            mvp_complexity = opportunity.get("mvp_complexity", 10)
            if mvp_complexity <= 4:
                quality_score += 0.25
                reasons.append("Simple MVP")
            elif mvp_complexity <= 6:
                quality_score += 0.15
                reasons.append("Moderate MVP complexity")

            # ç«äº‰é£é™© (20%) - è¶Šä½è¶Šå¥½
            competition_risk = opportunity.get("competition_risk", 10)
            if competition_risk <= 4:
                quality_score += 0.2
                reasons.append("Low competition")
            elif competition_risk <= 6:
                quality_score += 0.1
                reasons.append("Moderate competition")

            # é›†æˆéš¾åº¦ (15%) - è¶Šä½è¶Šå¥½
            integration_complexity = opportunity.get("integration_complexity", 10)
            if integration_complexity <= 5:
                quality_score += 0.15
                reasons.append("Easy integration")
            elif integration_complexity <= 7:
                quality_score += 0.08
                reasons.append("Moderate integration")

            # èšç±»å¤§å°åŠ åˆ†
            cluster_size = cluster_data.get("cluster_size", 0)
            if cluster_size >= 10:
                quality_score += 0.1
                reasons.append("Large cluster size")

            # æ€»åˆ†èŒƒå›´ï¼š0-1
            total_score = min(quality_score, 1.0)

            # åˆ¤æ–­æ˜¯å¦å¯è¡Œ
            is_viable = total_score >= 0.4  # 40%ä»¥ä¸Šè®¤ä¸ºå¯è¡Œ

            return {
                "is_viable": is_viable,
                "quality_score": total_score,
                "quality_reasons": reasons,
                "detailed_scores": {
                    "pain_frequency": pain_frequency,
                    "market_size": market_size,
                    "mvp_complexity": mvp_complexity,
                    "competition_risk": competition_risk,
                    "integration_complexity": integration_complexity
                }
            }

        except Exception as e:
            logger.error(f"Failed to evaluate opportunity quality: {e}")
            return {"is_viable": False, "reason": f"Evaluation error: {e}"}

    def _process_aligned_cluster(self, aligned_cluster: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """å¤„ç†å¯¹é½é—®é¢˜èšç±»"""
        try:
            # è·å–æ”¯æŒèšç±»ä¿¡æ¯
            supporting_clusters = db.get_clusters_for_aligned_problem(
                aligned_cluster['cluster_name']
            )

            # åˆ›å»ºå¤šæºéªŒè¯çš„æœºä¼š
            opportunity_data = {
                "content": {
                    "opportunity": {
                        "name": f"Multi-Source: {aligned_cluster['centroid_summary'][:50]}...",
                        "description": f"This opportunity is validated by multiple communities across different platforms. Core problem: {aligned_cluster['centroid_summary']}",
                        "target_users": f"Users from {len(supporting_clusters)} different communities",
                        "pain_frequency": 9,  # High frequency due to multi-source validation
                        "market_size": 9,  # Large market due to cross-platform demand
                        "mvp_complexity": 6,  # Moderate complexity
                        "competition_risk": 4,  # Lower risk due to validated demand
                        "integration_complexity": 5,  # Moderate integration complexity
                        "source_diversity": len(supporting_clusters),
                        "platform_insights": self._extract_platform_insights(supporting_clusters),
                        "current_tools": self._aggregate_current_tools(supporting_clusters),
                        "missing_capability": aligned_cluster['centroid_summary']
                    }
                }
            }

            logger.info(f"Created multi-source opportunity for aligned problem {aligned_cluster['cluster_name']}")
            return opportunity_data

        except Exception as e:
            logger.error(f"Failed to process aligned cluster {aligned_cluster['cluster_name']}: {e}")
            return None

    def _extract_platform_insights(self, supporting_clusters: List[Dict]) -> List[str]:
        """æå–å¹³å°æ´å¯Ÿ"""
        insights = []
        for cluster in supporting_clusters:
            source_type = cluster.get('source_type', 'unknown')
            summary = cluster.get('centroid_summary', '')[:100]
            insights.append(f"{source_type}: {summary}")
        return insights

    def _aggregate_current_tools(self, supporting_clusters: List[Dict]) -> List[str]:
        """èšåˆå½“å‰å·¥å…·"""
        tools = set()
        for cluster in supporting_clusters:
            common_pain = cluster.get('common_pain', '')
            # ç®€å•çš„å·¥å…·æå–é€»è¾‘
            if 'slack' in common_pain.lower():
                tools.add('Slack')
            if 'email' in common_pain.lower():
                tools.add('Email')
            if 'discord' in common_pain.lower():
                tools.add('Discord')
        return list(tools)

    def _save_opportunity_to_database(self, cluster_id: int, opportunity_data: Dict[str, Any], quality_result: Dict[str, Any]) -> Optional[int]:
        """ä¿å­˜æœºä¼šåˆ°æ•°æ®åº“"""
        try:
            # å¤„ç†å¯èƒ½çš„æ•°æ®ç»“æ„å·®å¼‚
            if "content" in opportunity_data:
                content = opportunity_data["content"]
                opportunity = content.get("opportunity", {})
                current_tools = content.get("current_tools", [])
                missing_capability = content.get("missing_capability", "")
                why_existing_fail = content.get("why_existing_fail", "")
            else:
                opportunity = opportunity_data.get("opportunity", {})
                current_tools = opportunity_data.get("current_tools", [])
                missing_capability = opportunity_data.get("missing_capability", "")
                why_existing_fail = opportunity_data.get("why_existing_fail", "")

            # å‡†å¤‡æœºä¼šæ•°æ®
            opportunity_record = {
                "cluster_id": cluster_id,
                "opportunity_name": opportunity.get("name", ""),
                "description": opportunity.get("description", ""),
                "current_tools": json.dumps(current_tools),
                "missing_capability": missing_capability,
                "why_existing_fail": why_existing_fail,
                "target_users": opportunity.get("target_users", ""),
                "pain_frequency_score": opportunity.get("pain_frequency", 0),
                "market_size_score": opportunity.get("market_size", 0),
                "mvp_complexity_score": opportunity.get("mvp_complexity", 0),
                "competition_risk_score": opportunity.get("competition_risk", 0),
                "integration_complexity_score": opportunity.get("integration_complexity", 0),
                "total_score": quality_result["quality_score"],
                "killer_risks": json.dumps([]),  # ç¨ååœ¨viability scoringä¸­å¡«å……
                "recommendation": ""  # ç¨ååœ¨viability scoringä¸­å¡«å……
            }

            opportunity_id = db.insert_opportunity(opportunity_record)
            return opportunity_id

        except Exception as e:
            logger.error(f"Failed to save opportunity to database: {e}")
            return None

    def map_opportunities_for_clusters(self, limit: int = 50) -> Dict[str, Any]:
        """ä¸ºèšç±»æ˜ å°„æœºä¼š"""
        logger.info(f"Mapping opportunities for up to {limit} clusters")

        start_time = time.time()

        try:
            # è·å–èšç±»ï¼ˆåŒ…æ‹¬å¯¹é½çš„è™šæ‹Ÿèšç±»ï¼‰
            clusters = db.get_clusters_for_opportunity_mapping()

            # å¦‚æœæœ‰æŒ‡å®šé™åˆ¶ï¼Œæˆªå–
            if limit and len(clusters) > limit:
                clusters = clusters[:limit]

            if not clusters:
                logger.info("No clusters found for opportunity mapping")
                return {"opportunities_identified": 0, "clusters_processed": 0}

            logger.info(f"Processing {len(clusters)} clusters for opportunity mapping")

            opportunities_created = []
            viable_opportunities = 0

            for i, cluster in enumerate(clusters):
                logger.info(f"Processing cluster {i+1}/{len(clusters)}: {cluster['cluster_name']}")

                try:
                    # æ£€æŸ¥æ˜¯å¦æ˜¯å¯¹é½çš„è™šæ‹Ÿèšç±»
                    if cluster.get('source_type') == 'aligned':
                        # å¤„ç†å¯¹é½é—®é¢˜èšç±»
                        opportunity_data = self._process_aligned_cluster(cluster)
                    else:
                        # åŸå§‹èšç±»å¤„ç†
                        enriched_cluster = self._enrich_cluster_data(cluster)
                        opportunity_data = self._map_opportunity_with_llm(enriched_cluster)

                    if opportunity_data:
                        # è¯„ä¼°æœºä¼šè´¨é‡
                        if cluster.get('source_type') == 'aligned':
                            # å¯¹é½èšç±»ä½¿ç”¨é¢„è®¾çš„é«˜è´¨é‡è¯„åˆ†
                            quality_result = {
                                "is_viable": True,
                                "quality_score": 0.95,  # é«˜è´¨é‡è¯„åˆ†
                                "quality_reasons": [
                                    "Multi-source validation",
                                    "Large market size",
                                    "High pain frequency",
                                    "Moderate competition"
                                ]
                            }
                        else:
                            # åŸå§‹èšç±»ä½¿ç”¨æ ‡å‡†è¯„ä¼°
                            quality_result = self._evaluate_opportunity_quality(opportunity_data, enriched_cluster)

                        if quality_result["is_viable"]:
                            # ä¿å­˜åˆ°æ•°æ®åº“
                            cluster_id = cluster.get("id", 0)  # å¯¹é½èšç±»å¯èƒ½æ²¡æœ‰id
                            opportunity_id = self._save_opportunity_to_database(
                                cluster_id, opportunity_data, quality_result
                            )

                            if opportunity_id:
                                opportunity_summary = {
                                    "opportunity_id": opportunity_id,
                                    "cluster_id": cluster_id,
                                    "cluster_name": cluster["cluster_name"],
                                    "opportunity_name": opportunity_data["content"]["opportunity"]["name"],
                                    "opportunity_description": opportunity_data["content"]["opportunity"]["description"],
                                    "quality_score": quality_result["quality_score"],
                                    "quality_reasons": quality_result["quality_reasons"]
                                }

                                opportunities_created.append(opportunity_summary)
                                viable_opportunities += 1

                                logger.info(f"Created opportunity: {opportunity_data['content']['opportunity']['name']} (Score: {quality_result['quality_score']:.2f})")
                        else:
                            logger.debug(f"Opportunity not viable: {quality_result.get('reason', 'Unknown')}")
                    else:
                        logger.debug(f"No opportunity found for cluster {cluster['cluster_name']}")

                except Exception as e:
                    logger.error(f"Failed to process cluster {cluster['cluster_name']}: {e}")
                    continue

                # æ·»åŠ å»¶è¿Ÿé¿å…APIé™åˆ¶
                time.sleep(2)

            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            processing_time = time.time() - start_time
            self.stats["total_clusters_processed"] = len(clusters)
            self.stats["opportunities_identified"] = len(opportunities_created)
            self.stats["viable_opportunities"] = viable_opportunities
            self.stats["processing_time"] = processing_time

            if opportunities_created:
                self.stats["avg_opportunity_score"] = sum(opp["quality_score"] for opp in opportunities_created) / len(opportunities_created)

            logger.info(f"""
=== Opportunity Mapping Summary ===
Clusters processed: {len(clusters)}
Opportunities identified: {len(opportunities_created)}
Viable opportunities: {viable_opportunities}
Average opportunity score: {self.stats['avg_opportunity_score']:.2f}
Processing time: {processing_time:.2f}s
""")

            return {
                "opportunities_created": len(opportunities_created),
                "viable_opportunities": viable_opportunities,
                "clusters_processed": len(clusters),
                "opportunity_details": opportunities_created,
                "mapping_stats": self.get_statistics()
            }

        except Exception as e:
            logger.error(f"Failed to map opportunities: {e}")
            raise

    def get_opportunities_summary(self, min_score: float = 0.0, limit: int = 50) -> List[Dict[str, Any]]:
        """è·å–æœºä¼šæ‘˜è¦"""
        try:
            with db.get_connection("clusters") as conn:
                cursor = conn.execute("""
                    SELECT o.*, c.cluster_name, c.cluster_description, c.cluster_size
                    FROM opportunities o
                    JOIN clusters c ON o.cluster_id = c.id
                    WHERE o.total_score >= ?
                    ORDER BY o.total_score DESC
                    LIMIT ?
                """, (min_score, limit))
                opportunities = [dict(row) for row in cursor.fetchall()]

            return opportunities

        except Exception as e:
            logger.error(f"Failed to get opportunities summary: {e}")
            return []

    def get_statistics(self) -> Dict[str, Any]:
        """è·å–æ˜ å°„ç»Ÿè®¡ä¿¡æ¯"""
        stats = self.stats.copy()

        if stats["total_clusters_processed"] > 0:
            stats["opportunity_rate"] = stats["opportunities_identified"] / stats["total_clusters_processed"]
            stats["viable_rate"] = stats["viable_opportunities"] / stats["total_clusters_processed"]
            stats["processing_rate"] = stats["total_clusters_processed"] / max(stats["processing_time"], 1)
        else:
            stats["opportunity_rate"] = 0
            stats["viable_rate"] = 0
            stats["processing_rate"] = 0

        return stats

    def reset_statistics(self):
        """é‡ç½®ç»Ÿè®¡ä¿¡æ¯"""
        self.stats = {
            "total_clusters_processed": 0,
            "opportunities_identified": 0,
            "viable_opportunities": 0,
            "processing_time": 0.0,
            "avg_opportunity_score": 0.0
        }

def main():
    """ä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description="Map opportunities from pain point clusters")
    parser.add_argument("--limit", type=int, default=50, help="Limit number of clusters to process")
    parser.add_argument("--min-score", type=float, default=0.0, help="Minimum opportunity score")
    parser.add_argument("--list", action="store_true", help="List existing opportunities")
    args = parser.parse_args()

    try:
        logger.info("Starting opportunity mapping...")

        mapper = OpportunityMapper()

        if args.list:
            # åˆ—å‡ºç°æœ‰æœºä¼š
            opportunities = mapper.get_opportunities_summary(min_score=args.min_score)
            print(json.dumps(opportunities, indent=2, default=str))

        else:
            # æ˜ å°„æ–°æœºä¼š
            result = mapper.map_opportunities_for_clusters(limit=args.limit)

            logger.info(f"""
=== Opportunity Mapping Complete ===
Opportunities created: {result['opportunities_created']}
Viable opportunities: {result['viable_opportunities']}
Clusters processed: {result['clusters_processed']}
Mapping stats: {result['mapping_stats']}
""")

    except Exception as e:
        logger.error(f"Fatal error: {e}")
        raise

if __name__ == "__main__":
    main()
```


================================================================================
æ–‡ä»¶: pipeline/score_viability.py
================================================================================

```python
"""
Score Viability module for Reddit Pain Point Finder
å¯è¡Œæ€§è¯„åˆ†æ¨¡å— - é’ˆå¯¹ä¸€äººå…¬å¸çš„å¯è¡Œæ€§è¯„ä¼°
"""
import json
import logging
import time
from typing import List, Dict, Any, Optional
from datetime import datetime
import yaml
from pathlib import Path

from utils.llm_client import llm_client
from utils.db import db

logger = logging.getLogger(__name__)

class ViabilityScorer:
    """å¯è¡Œæ€§è¯„åˆ†å™¨"""

    def __init__(self):
        """åˆå§‹åŒ–è¯„åˆ†å™¨"""
        self.stats = {
            "total_opportunities_scored": 0,
            "viable_opportunities": 0,
            "good_opportunities": 0,
            "excellent_opportunities": 0,
            "processing_time": 0.0,
            "avg_total_score": 0.0,
            "skipped_clusters": 0
        }

        # åŠ è½½é…ç½®
        self.config = self._load_config()
        self.filtering_rules = self.config.get("filtering_rules", {})

        logger.info(f"ViabilityScorer initialized with filtering rules enabled: {self.filtering_rules.get('enabled', False)}")

    def _load_config(self) -> Dict[str, Any]:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            config_path = Path(__file__).parent.parent / "config" / "thresholds.yaml"
            with open(config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
            logger.info(f"Configuration loaded from {config_path}")
            return config
        except Exception as e:
            logger.error(f"Failed to load configuration: {e}")
            return {"filtering_rules": {"enabled": False}}

    def _calculate_unique_authors(self, pain_event_ids: List[int]) -> int:
        """è®¡ç®—ç‹¬ç«‹ä½œè€…æ•°é‡"""
        if not pain_event_ids:
            return 0

        try:
            with db.get_connection("pain") as conn:
                placeholders = ','.join(['?' for _ in pain_event_ids])
                cursor = conn.execute(f"""
                    SELECT COUNT(DISTINCT fp.author) as unique_count
                    FROM pain_events pe
                    JOIN filtered_posts fp ON pe.post_id = fp.id
                    WHERE pe.id IN ({placeholders})
                """, pain_event_ids)
                result = cursor.fetchone()
                return result['unique_count'] if result else 0
        except Exception as e:
            logger.error(f"Failed to calculate unique authors: {e}")
            return 0

    def _calculate_cross_subreddit_count(self, pain_event_ids: List[int]) -> int:
        """è®¡ç®—è·¨å­ç‰ˆå—æ•°é‡"""
        if not pain_event_ids:
            return 0

        try:
            with db.get_connection("pain") as conn:
                placeholders = ','.join(['?' for _ in pain_event_ids])
                cursor = conn.execute(f"""
                    SELECT COUNT(DISTINCT fp.subreddit) as subreddit_count
                    FROM pain_events pe
                    JOIN filtered_posts fp ON pe.post_id = fp.id
                    WHERE pe.id IN ({placeholders})
                """, pain_event_ids)
                result = cursor.fetchone()
                return result['subreddit_count'] if result else 0
        except Exception as e:
            logger.error(f"Failed to calculate cross-subreddit count: {e}")
            return 0

    def _calculate_avg_frequency_score(self, pain_event_ids: List[int]) -> float:
        """è®¡ç®—å¹³å‡é¢‘ç‡è¯„åˆ†"""
        if not pain_event_ids:
            return 0.0

        try:
            with db.get_connection("pain") as conn:
                placeholders = ','.join(['?' for _ in pain_event_ids])
                cursor = conn.execute(f"""
                    SELECT pe.frequency
                    FROM pain_events pe
                    WHERE pe.id IN ({placeholders})
                """, pain_event_ids)

                frequencies = [row['frequency'] or '' for row in cursor.fetchall()]
                return self._frequency_to_score(frequencies)
        except Exception as e:
            logger.error(f"Failed to calculate average frequency score: {e}")
            return 0.0

    def _frequency_to_score(self, frequencies: List[str]) -> float:
        """å°†é¢‘ç‡æ–‡æœ¬è½¬æ¢ä¸ºè¯„åˆ†"""
        if not frequencies:
            return 0.0

        score_map = self.filtering_rules.get("frequency_score_mapping", {
            'daily': 10, 'æ¯å¤©': 10, 'day': 9,
            'weekly': 8, 'æ¯å‘¨': 8, 'week': 7,
            'monthly': 6, 'æ¯æœˆ': 6, 'month': 5,
            'often': 7, 'ç»å¸¸': 7, 'frequently': 8,
            'sometimes': 5, 'æœ‰æ—¶': 5, 'occasionally': 4,
            'rarely': 3, 'å¾ˆå°‘': 3, 'seldom': 2,
            'always': 9, 'æ€»æ˜¯': 9, 'constantly': 8,
            'never': 1, 'ä»ä¸': 1, 'default': 4
        })

        scores = []
        for freq in frequencies:
            if not freq:
                scores.append(score_map.get('default', 4))
                continue

            freq_lower = freq.lower()
            matched = False
            for key, score in score_map.items():
                if key == 'default':
                    continue
                if key in freq_lower:
                    scores.append(score)
                    matched = True
                    break

            if not matched:
                scores.append(score_map.get('default', 4))

        return sum(scores) / len(scores) if scores else 0.0

    def should_skip_solution_design(self, cluster_data: Dict[str, Any]) -> tuple[bool, str]:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥è·³è¿‡è§£å†³æ–¹æ¡ˆè®¾è®¡"""
        if not self.filtering_rules.get("enabled", False):
            return False, ""

        # 1. æ£€æŸ¥èšç±»å¤§å°
        cluster_size = cluster_data.get('cluster_size', 0)
        min_cluster_size = self.filtering_rules.get("min_cluster_size", 8)
        if cluster_size < min_cluster_size:
            reason = self.filtering_rules.get("skip_reasons", {}).get("cluster_size",
                    f"èšç±»è§„æ¨¡è¿‡å° ({cluster_size} < {min_cluster_size})")
            formatted_reason = reason.format(actual=cluster_size, required=min_cluster_size)
            if self.filtering_rules.get("log_skipped_clusters", True):
                logger.info(f"Skipping cluster due to size: {formatted_reason}")
            return True, formatted_reason

        # 2. æ£€æŸ¥ç‹¬ç«‹ä½œè€…æ•°
        pain_event_ids = cluster_data.get('pain_event_ids', [])
        if isinstance(pain_event_ids, str):
            import json
            try:
                pain_event_ids = json.loads(pain_event_ids)
            except:
                pain_event_ids = []

        unique_authors = self._calculate_unique_authors(pain_event_ids)
        min_unique_authors = self.filtering_rules.get("min_unique_authors", 5)
        if unique_authors < min_unique_authors:
            reason = self.filtering_rules.get("skip_reasons", {}).get("unique_authors",
                    f"ç‹¬ç«‹ä½œè€…ä¸è¶³ ({unique_authors} < {min_unique_authors})")
            formatted_reason = reason.format(actual=unique_authors, required=min_unique_authors)
            if self.filtering_rules.get("log_skipped_clusters", True):
                logger.info(f"Skipping cluster due to unique authors: {formatted_reason}")
            return True, formatted_reason

        # 3. æ£€æŸ¥è·¨å­ç‰ˆå—æ•°é‡
        cross_subreddit_count = self._calculate_cross_subreddit_count(pain_event_ids)
        min_cross_subreddit_count = self.filtering_rules.get("min_cross_subreddit_count", 2)
        if cross_subreddit_count < min_cross_subreddit_count:
            reason = self.filtering_rules.get("skip_reasons", {}).get("cross_subreddit",
                    f"è·¨å­ç‰ˆå—æ•°é‡ä¸è¶³ ({cross_subreddit_count} < {min_cross_subreddit_count})")
            formatted_reason = reason.format(actual=cross_subreddit_count, required=min_cross_subreddit_count)
            if self.filtering_rules.get("log_skipped_clusters", True):
                logger.info(f"Skipping cluster due to cross-subreddit: {formatted_reason}")
            return True, formatted_reason

        # 4. æ£€æŸ¥å¹³å‡é¢‘ç‡è¯„åˆ†
        avg_frequency_score = self._calculate_avg_frequency_score(pain_event_ids)
        min_avg_frequency_score = self.filtering_rules.get("min_avg_frequency_score", 6)
        if avg_frequency_score < min_avg_frequency_score:
            reason = self.filtering_rules.get("skip_reasons", {}).get("frequency_score",
                    f"ç—›ç‚¹é¢‘ç‡ä¸å¤Ÿé«˜ ({avg_frequency_score:.1f} < {min_avg_frequency_score})")
            formatted_reason = reason.format(actual=avg_frequency_score, required=min_avg_frequency_score)
            if self.filtering_rules.get("log_skipped_clusters", True):
                logger.info(f"Skipping cluster due to frequency score: {formatted_reason}")
            return True, formatted_reason

        if self.filtering_rules.get("log_detailed_metrics", False):
            logger.info(f"Cluster passed filtering checks - size: {cluster_size}, "
                       f"authors: {unique_authors}, subreddits: {cross_subreddit_count}, "
                       f"frequency: {avg_frequency_score:.1f}")

        return False, ""

    def _update_opportunities_recommendation(self, cluster_id: int, recommendation: str, skip_reason: str = "") -> bool:
        """æ›´æ–°èšç±»ä¸‹æ‰€æœ‰æœºä¼šçš„æ¨èçŠ¶æ€"""
        try:
            with db.get_connection("clusters") as conn:
                # è·å–æ‰€æœ‰ç›¸å…³æœºä¼š
                cursor = conn.execute("""
                    SELECT id FROM opportunities WHERE cluster_id = ?
                """, (cluster_id,))
                opportunity_ids = [row['id'] for row in cursor.fetchall()]

                # æ›´æ–°æ¯ä¸ªæœºä¼šçš„æ¨è
                for opp_id in opportunity_ids:
                    full_recommendation = f"abandon - {skip_reason}" if skip_reason else recommendation
                    conn.execute("""
                        UPDATE opportunities
                        SET recommendation = ?
                        WHERE id = ?
                    """, (full_recommendation, opp_id))

                conn.commit()
                logger.info(f"Updated {len(opportunity_ids)} opportunities with recommendation: {full_recommendation}")
                return True

        except Exception as e:
            logger.error(f"Failed to update opportunities recommendation: {e}")
            return False

    def _apply_filtering_rules(self, opportunities: List[Dict[str, Any]], processed_clusters: set) -> List[Dict[str, Any]]:
        """åº”ç”¨è¿‡æ»¤è§„åˆ™"""
        filtered_opportunities = []
        skipped_count = 0

        for opportunity in opportunities:
            cluster_id = opportunity["cluster_id"]

            # é¿å…é‡å¤å¤„ç†åŒä¸€ä¸ªèšç±»
            if cluster_id in processed_clusters:
                filtered_opportunities.append(opportunity)
                continue

            # è·å–èšç±»æ•°æ®
            try:
                with db.get_connection("clusters") as conn:
                    cursor = conn.execute("""
                        SELECT * FROM clusters WHERE id = ?
                    """, (cluster_id,))
                    cluster_data = cursor.fetchone()
                    cluster_data = dict(cluster_data) if cluster_data else None

                if cluster_data:
                    # æ£€æŸ¥è¿‡æ»¤è§„åˆ™
                    should_skip, skip_reason = self.should_skip_solution_design(cluster_data)

                    if should_skip:
                        # è·³è¿‡è¯¥èšç±»çš„æ‰€æœ‰æœºä¼š
                        self._update_opportunities_recommendation(cluster_id, "abandon", skip_reason)
                        processed_clusters.add(cluster_id)
                        skipped_count += 1
                        self.stats["skipped_clusters"] += 1

                        # ç»Ÿè®¡è·³è¿‡çš„æœºä¼šæ•°é‡
                        with db.get_connection("clusters") as conn:
                            cursor = conn.execute("""
                                SELECT COUNT(*) as count FROM opportunities WHERE cluster_id = ?
                            """, (cluster_id,))
                            opp_count = cursor.fetchone()['count']
                            logger.info(f"Skipped {opp_count} opportunities from cluster {cluster_id}: {skip_reason}")

                        continue
                    else:
                        # èšç±»é€šè¿‡è¿‡æ»¤ï¼Œä¿ç•™å…¶æœºä¼š
                        processed_clusters.add(cluster_id)
                        filtered_opportunities.append(opportunity)
                        logger.info(f"Cluster {cluster_id} passed filtering checks")
                else:
                    logger.warning(f"Cluster {cluster_id} not found, including opportunity")
                    filtered_opportunities.append(opportunity)

            except Exception as e:
                logger.error(f"Error processing cluster {cluster_id}: {e}")
                filtered_opportunities.append(opportunity)

        logger.info(f"Filtering applied: {skipped_count} clusters skipped, {len(filtered_opportunities)} opportunities remaining")
        return filtered_opportunities

    def _enhance_opportunity_data(self, opportunity_data: Dict[str, Any]) -> Dict[str, Any]:
        """å¢å¼ºæœºä¼šæ•°æ®"""
        try:
            # è·å–èšç±»ä¿¡æ¯
            cluster_id = opportunity_data["cluster_id"]
            with db.get_connection("clusters") as conn:
                cursor = conn.execute("""
                    SELECT * FROM clusters WHERE id = ?
                """, (cluster_id,))
                cluster_data = cursor.fetchone()

            if not cluster_data:
                return opportunity_data

            cluster_info = dict(cluster_data)

            # è·å–èšç±»ä¸­çš„ç—›ç‚¹äº‹ä»¶
            pain_event_ids = json.loads(cluster_info.get("pain_event_ids", "[]"))
            pain_events = []

            with db.get_connection("pain") as conn:
                for event_id in pain_event_ids:
                    cursor = conn.execute("""
                        SELECT * FROM pain_events WHERE id = ?
                    """, (event_id,))
                    event_data = cursor.fetchone()
                    if event_data:
                        pain_events.append(dict(event_data))

            # å¢å¼ºæœºä¼šæ•°æ®
            enhanced_opportunity = opportunity_data.copy()
            enhanced_opportunity.update({
                "cluster_info": {
                    "cluster_name": cluster_info["cluster_name"],
                    "cluster_description": cluster_info["cluster_description"],
                    "cluster_size": cluster_info["cluster_size"],
                    "workflow_confidence": cluster_info.get("workflow_confidence", 0.0),
                    "pain_events": pain_events
                }
            })

            # æ·»åŠ å¸‚åœºè§„æ¨¡ä¼°ç®—
            self._estimate_market_size(enhanced_opportunity)

            # æ·»åŠ ç«äº‰åˆ†æ
            self._analyze_competition(enhanced_opportunity)

            return enhanced_opportunity

        except Exception as e:
            logger.error(f"Failed to enhance opportunity data: {e}")
            return opportunity_data

    def _estimate_market_size(self, opportunity_data: Dict[str, Any]):
        """ä¼°ç®—å¸‚åœºè§„æ¨¡"""
        try:
            cluster_info = opportunity_data.get("cluster_info", {})
            pain_events = cluster_info.get("pain_events", [])

            # åŸºäºå­ç‰ˆå—åˆ†å¸ƒä¼°ç®—ç”¨æˆ·ç¾¤ä½“
            subreddit_distribution = {}
            for event in pain_events:
                with db.get_connection("filtered") as conn:
                    cursor = conn.execute("""
                        SELECT subreddit FROM filtered_posts WHERE id = ?
                    """, (event["post_id"],))
                    post_data = cursor.fetchone()
                    if post_data:
                        subreddit = post_data[0]
                        subreddit_distribution[subreddit] = subreddit_distribution.get(subreddit, 0) + 1

            # ä¼°ç®—ç”¨æˆ·åŸºæ•°
            subreddit_estimates = {
                "programming": 5000000,  # 500ä¸‡å¼€å‘è€…
                "MachineLearning": 2000000,  # 200ä¸‡MLä»ä¸šè€…
                "Entrepreneur": 1000000,  # 100ä¸‡åˆ›ä¸šè€…
                "startups": 2000000,  # 200ä¸‡åˆåˆ›å…¬å¸äººå‘˜
                "dataisbeautiful": 500000,  # 50ä¸‡æ•°æ®çˆ±å¥½è€…
                "webdev": 3000000,  # 300ä¸‡Webå¼€å‘è€…
                "sysadmin": 1500000,  # 150ä¸‡ç³»ç»Ÿç®¡ç†å‘˜
                "ChatGPT": 10000000,  # 1000ä¸‡ChatGPTç”¨æˆ·
                "LocalLLaMA": 500000,  # 50ä¸‡æœ¬åœ°LLMç”¨æˆ·
            }

            # è®¡ç®—æ€»å¸‚åœºè§„æ¨¡
            total_estimated_users = 0
            for subreddit, count in subreddit_distribution.items():
                estimated_users = subreddit_estimates.get(subreddit, 100000)  # é»˜è®¤10ä¸‡
                weight = count / len(pain_events)  # åŸºäºå‡ºç°é¢‘ç‡çš„æƒé‡
                total_estimated_users += estimated_users * weight

            # å¸‚åœºæ¸—é€ç‡ä¼°ç®—ï¼ˆä¿å®ˆä¼°è®¡ï¼‰
            penetration_rate = 0.001  # 0.1%çš„å¸‚åœºæ¸—é€ç‡
            addressable_market = total_estimated_users * penetration_rate

            opportunity_data["market_analysis"] = {
                "subreddit_distribution": subreddit_distribution,
                "estimated_total_users": int(total_estimated_users),
                "conservative_penetration_rate": penetration_rate,
                "addressable_market_size": int(addressable_market),
                "market_tier": self._get_market_tier(addressable_market)
            }

        except Exception as e:
            logger.error(f"Failed to estimate market size: {e}")

    def _get_market_tier(self, market_size: int) -> str:
        """è·å–å¸‚åœºå±‚çº§"""
        if market_size > 100000:  # 10ä¸‡+
            return "large"
        elif market_size > 50000:  # 5ä¸‡-10ä¸‡
            return "medium"
        elif market_size > 10000:  # 1ä¸‡-5ä¸‡
            return "small"
        else:  # 1ä¸‡ä»¥ä¸‹
            return "niche"

    def _analyze_competition(self, opportunity_data: Dict[str, Any]):
        """åˆ†æç«äº‰æƒ…å†µ"""
        try:
            opportunity_name = opportunity_data.get("opportunity_name", "").lower()
            description = opportunity_data.get("description", "").lower()
            target_users = opportunity_data.get("target_users", "").lower()

            # ç«äº‰å¯¹æ‰‹å…³é”®è¯ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
            competitor_keywords = {
                "automation": ["zapier", "ifttt", "integromat", "make.com"],
                "data_analysis": ["tableau", "power bi", "looker", "metabase"],
                "project_management": ["jira", "trello", "asana", "monday.com"],
                "documentation": ["notion", "confluence", "obsidian", "roam research"],
                "api_tools": ["postman", "insomnia", "swagger", "openapi"],
                "monitoring": ["datadog", "new relic", "grafana", "prometheus"],
                "testing": ["jest", "cypress", "selenium", "playwright"],
                "development": ["vs code", "github", "gitlab", "intellij"],
                "communication": ["slack", "discord", "teams", "zoom"]
            }

            # æ£€æµ‹ç«äº‰å¯¹æ‰‹
            detected_competitors = []
            for category, competitors in competitor_keywords.items():
                for competitor in competitors:
                    if (competitor in opportunity_name or
                        competitor in description or
                        competitor in target_users):
                        detected_competitors.append({
                            "name": competitor,
                            "category": category
                        })

            # ç«äº‰å¼ºåº¦è¯„ä¼°
            if len(detected_competitors) == 0:
                competition_level = "low"
                competition_score = 2  # 1-10åˆ†ï¼Œè¶Šä½è¶Šå¥½
            elif len(detected_competitors) <= 2:
                competition_level = "medium"
                competition_score = 5
            else:
                competition_level = "high"
                competition_score = 8

            opportunity_data["competition_analysis"] = {
                "detected_competitors": detected_competitors,
                "competition_level": competition_level,
                "competition_score": competition_score,
                "differentiation_opportunity": self._identify_differentiation_opportunity(opportunity_data, detected_competitors)
            }

        except Exception as e:
            logger.error(f"Failed to analyze competition: {e}")

    def _identify_differentiation_opportunity(self, opportunity_data: Dict[str, Any], competitors: List[Dict[str, Any]]) -> str:
        """è¯†åˆ«å·®å¼‚åŒ–æœºä¼š"""
        try:
            # ç®€å•çš„å·®å¼‚åŒ–åˆ†æ
            if not competitors:
                return "No direct competitors detected"

            opportunity_name = opportunity_data.get("opportunity_name", "").lower()

            # æ£€æŸ¥æ˜¯å¦æœ‰ç»†åˆ†å¸‚åœºæœºä¼š
            niche_indicators = ["for startups", "for indie", "for solo", "for small", "simple", "lightweight", "minimal"]
            for indicator in niche_indicators:
                if indicator in opportunity_name:
                    return f"Niche focus on {indicator}"

            return "Generic space, needs clear differentiation"

        except Exception as e:
            logger.error(f"Failed to identify differentiation opportunity: {e}")
            return "Unable to determine"

    def _score_with_llm(self, opportunity_data: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """ä½¿ç”¨LLMè¿›è¡Œå¯è¡Œæ€§è¯„åˆ†"""
        try:
            # æ„å»ºæœºä¼šæè¿°æ–‡æœ¬
            description = f"""
Opportunity: {opportunity_data.get('opportunity_name', '')}

Description: {opportunity_data.get('description', '')}

Target Users: {opportunity_data.get('target_users', '')}

Current Tools: {opportunity_data.get('current_tools', '')}

Missing Capability: {opportunity_data.get('missing_capability', '')}

Why Existing Tools Fail: {opportunity_data.get('why_existing_fail', '')}

Market Analysis: {opportunity_data.get('market_analysis', {})}

Competition Analysis: {opportunity_data.get('competition_analysis', {})}
"""

            # è°ƒç”¨LLMè¿›è¡Œè¯„åˆ†
            response = llm_client.score_viability(description)

            scoring_result = response["content"]

            return scoring_result

        except Exception as e:
            logger.error(f"Failed to score with LLM: {e}")
            return None

    def _combine_scores(self, llm_scores: Dict[str, Any], opportunity_data: Dict[str, Any]) -> Dict[str, Any]:
        """ç»“åˆLLMè¯„åˆ†å’Œè§„åˆ™è¯„åˆ†"""
        try:
            # LLMè¯„åˆ†
            llm_component_scores = llm_scores.get("scores", {})
            llm_total_score = llm_scores.get("total_score", 0.0)

            # è§„åˆ™è¯„åˆ†
            market_analysis = opportunity_data.get("market_analysis", {})
            competition_analysis = opportunity_data.get("competition_analysis", {})

            # å¸‚åœºè§„æ¨¡è¯„åˆ† (0-10)
            market_tier = market_analysis.get("market_tier", "niche")
            market_score_by_tier = {
                "large": 9,
                "medium": 7,
                "small": 5,
                "niche": 3
            }
            market_score = market_score_by_tier.get(market_tier, 3)

            # ç«äº‰è¯„åˆ† (0-10, è¶Šä½è¶Šå¥½)
            competition_score = competition_analysis.get("competition_score", 8)
            competition_normalized = max(10 - competition_score, 1)  # è½¬æ¢ä¸ºè¶Šé«˜è¶Šå¥½

            # èšç±»å¤§å°è¯„åˆ† (0-10)
            cluster_info = opportunity_data.get("cluster_info", {})
            cluster_size = cluster_info.get("cluster_size", 0)
            cluster_score = min(cluster_size, 10)  # æ¯ä¸ªäº‹ä»¶1åˆ†ï¼Œæœ€å¤š10åˆ†

            # å·¥ä½œæµç½®ä¿¡åº¦è¯„åˆ† (0-10)
            workflow_confidence = cluster_info.get("workflow_confidence", 0.0)
            workflow_score = workflow_confidence * 10

            # ç»¼åˆè¯„åˆ†è®¡ç®—
            final_component_scores = {
                "pain_frequency": llm_component_scores.get("pain_frequency", 5),
                "clear_buyer": llm_component_scores.get("clear_buyer", 5),
                "mvp_buildable": llm_component_scores.get("mvp_buildable", 5),
                "crowded_market": competition_normalized,
                "integration": llm_component_scores.get("integration", 5),
                "market_size": market_score,
                "cluster_strength": cluster_score,
                "workflow_confidence": workflow_score
            }

            # è®¡ç®—åŠ æƒæ€»åˆ†
            weights = {
                "pain_frequency": 0.15,
                "clear_buyer": 0.15,
                "mvp_buildable": 0.20,
                "crowded_market": 0.15,
                "integration": 0.10,
                "market_size": 0.10,
                "cluster_strength": 0.10,
                "workflow_confidence": 0.05
            }

            weighted_total = sum(
                final_component_scores[component] * weight
                for component, weight in weights.items()
            )

            # ç¡®ä¿åˆ†æ•°åœ¨0-10èŒƒå›´å†…
            final_total_score = min(max(weighted_total, 0), 10)

            # ç”Ÿæˆæ€æ‰‹é£é™©
            killer_risks = self._generate_killer_risks(final_component_scores, opportunity_data)

            return {
                "component_scores": final_component_scores,
                "total_score": final_total_score,
                "llm_total_score": llm_total_score,
                "killer_risks": killer_risks,
                "recommendation": self._generate_recommendation(final_total_score, killer_risks)
            }

        except Exception as e:
            logger.error(f"Failed to combine scores: {e}")
            return {"total_score": 0.0, "component_scores": {}, "killer_risks": [], "recommendation": "Error in scoring"}

    def _generate_killer_risks(self, component_scores: Dict[str, Any], opportunity_data: Dict[str, Any]) -> List[str]:
        """ç”Ÿæˆæ€æ‰‹é£é™©"""
        risks = []

        # åŸºäºåˆ†é¡¹è¯„åˆ†ç”Ÿæˆé£é™©
        if component_scores.get("market_size", 0) < 4:
            risks.append("Small market size may not sustain business")

        if component_scores.get("crowded_market", 0) < 4:
            risks.append("Highly competitive market with established players")

        if component_scores.get("mvp_buildable", 0) < 4:
            risks.append("Technical complexity too high for solo founder")

        if component_scores.get("clear_buyer", 0) < 4:
            risks.append("Unclear who will pay for this solution")

        if component_scores.get("pain_frequency", 0) < 4:
            risks.append("Problem may not be frequent enough to drive adoption")

        if component_scores.get("integration", 0) < 4:
            risks.append("Difficult integration with existing workflows")

        # åŸºäºç«äº‰åˆ†æç”Ÿæˆé£é™©
        competition_analysis = opportunity_data.get("competition_analysis", {})
        if competition_analysis.get("competition_level") == "high":
            risks.append("Direct competition with well-funded incumbents")

        # åŸºäºå¸‚åœºåˆ†æç”Ÿæˆé£é™©
        market_analysis = opportunity_data.get("market_analysis", {})
        if market_analysis.get("market_tier") == "niche":
            risks.append("Very niche market may limit growth potential")

        return risks[:3]  # æœ€å¤šè¿”å›3ä¸ªé£é™©

    def _generate_recommendation(self, total_score: float, killer_risks: List[str]) -> str:
        """ç”Ÿæˆå»ºè®®"""
        if total_score >= 8.0:
            return "pursue - Strong opportunity with high potential"
        elif total_score >= 6.5:
            return "pursue - Good opportunity with manageable risks"
        elif total_score >= 5.0:
            return "modify - Viable with some adjustments needed"
        elif total_score >= 3.5:
            return "research - Needs more validation before pursuing"
        else:
            return "abandon - Too many risks or unclear value proposition"

    def _update_opportunity_in_database(self, opportunity_id: int, scoring_result: Dict[str, Any]) -> bool:
        """æ›´æ–°æ•°æ®åº“ä¸­çš„æœºä¼šè¯„åˆ†"""
        try:
            with db.get_connection("clusters") as conn:
                conn.execute("""
                    UPDATE opportunities
                    SET pain_frequency_score = ?,
                        market_size_score = ?,
                        mvp_complexity_score = ?,
                        competition_risk_score = ?,
                        integration_complexity_score = ?,
                        total_score = ?,
                        killer_risks = ?,
                        recommendation = ?
                    WHERE id = ?
                """, (
                    scoring_result["component_scores"].get("pain_frequency", 0),
                    scoring_result["component_scores"].get("market_size", 0),
                    scoring_result["component_scores"].get("mvp_buildable", 0),
                    scoring_result["component_scores"].get("crowded_market", 0),
                    scoring_result["component_scores"].get("integration", 0),
                    scoring_result["total_score"],
                    json.dumps(scoring_result["killer_risks"]),
                    scoring_result.get("recommendation", ""),
                    opportunity_id
                ))
                conn.commit()
                return True

        except Exception as e:
            logger.error(f"Failed to update opportunity in database: {e}")
            return False

    def score_opportunities(self, limit: int = 100) -> Dict[str, Any]:
        """ä¸ºæœºä¼šè¯„åˆ†"""
        logger.info(f"Scoring up to {limit} opportunities")

        start_time = time.time()

        try:
            # è·å–æœªè¯„åˆ†çš„æœºä¼š
            with db.get_connection("clusters") as conn:
                cursor = conn.execute("""
                    SELECT * FROM opportunities
                    WHERE total_score = 0 OR total_score IS NULL
                    ORDER BY cluster_id DESC
                    LIMIT ?
                """, (limit,))
                opportunities = [dict(row) for row in cursor.fetchall()]

            if not opportunities:
                logger.info("No unscored opportunities found")
                return {"opportunities_scored": 0, "viable_opportunities": 0}

            logger.info(f"Found {len(opportunities)} opportunities to score")

            # å¦‚æœå¯ç”¨äº†è¿‡æ»¤è§„åˆ™ï¼Œé¦–å…ˆæ£€æŸ¥èšç±»
            processed_clusters = set()
            if self.filtering_rules.get("enabled", False):
                logger.info("Filtering rules enabled, checking clusters first...")
                opportunities = self._apply_filtering_rules(opportunities, processed_clusters)

            scored_opportunities = []
            viable_count = 0
            good_count = 0
            excellent_count = 0

            for i, opportunity in enumerate(opportunities):
                logger.info(f"Scoring opportunity {i+1}/{len(opportunities)}: {opportunity['opportunity_name']}")

                try:
                    # å¢å¼ºæœºä¼šæ•°æ®
                    enhanced_opportunity = self._enhance_opportunity_data(opportunity)

                    # LLMè¯„åˆ†
                    llm_result = self._score_with_llm(enhanced_opportunity)

                    if llm_result:
                        # ç»“åˆè¯„åˆ†
                        final_scores = self._combine_scores(llm_result, enhanced_opportunity)

                        # æ›´æ–°æ•°æ®åº“
                        if self._update_opportunity_in_database(opportunity["id"], final_scores):
                            # ç»Ÿè®¡
                            total_score = final_scores["total_score"]
                            if total_score >= 8.5:
                                excellent_count += 1
                            elif total_score >= 7.0:
                                good_count += 1
                            elif total_score >= 5.0:
                                viable_count += 1

                            opportunity_summary = {
                                "opportunity_id": opportunity["id"],
                                "opportunity_name": opportunity["opportunity_name"],
                                "total_score": total_score,
                                "recommendation": final_scores["recommendation"],
                                "killer_risks": final_scores["killer_risks"]
                            }

                            scored_opportunities.append(opportunity_summary)

                            logger.info(f"Scored: {opportunity['opportunity_name']} - {total_score:.1f}/10 ({final_scores['recommendation']})")
                        else:
                            logger.error(f"Failed to update opportunity {opportunity['id']} in database")

                except Exception as e:
                    logger.error(f"Failed to score opportunity {opportunity['opportunity_name']}: {e}")
                    continue

                # æ·»åŠ å»¶è¿Ÿé¿å…APIé™åˆ¶
                time.sleep(2)

            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            processing_time = time.time() - start_time
            self.stats["total_opportunities_scored"] = len(scored_opportunities)
            self.stats["viable_opportunities"] = viable_count
            self.stats["good_opportunities"] = good_count
            self.stats["excellent_opportunities"] = excellent_count
            self.stats["processing_time"] = processing_time

            if scored_opportunities:
                self.stats["avg_total_score"] = sum(opp["total_score"] for opp in scored_opportunities) / len(scored_opportunities)

            logger.info(f"""
=== Viability Scoring Summary ===
Opportunities scored: {len(scored_opportunities)}
Viable opportunities (5.0+): {viable_count}
Good opportunities (7.0+): {good_count}
Excellent opportunities (8.5+): {excellent_count}
Average score: {self.stats['avg_total_score']:.2f}
Processing time: {processing_time:.2f}s
""")

            return {
                "opportunities_scored": len(scored_opportunities),
                "viable_opportunities": viable_count,
                "good_opportunities": good_count,
                "excellent_opportunities": excellent_count,
                "scored_opportunities": scored_opportunities,
                "scoring_stats": self.get_statistics()
            }

        except Exception as e:
            logger.error(f"Failed to score opportunities: {e}")
            raise

    def get_top_opportunities(self, min_score: float = 5.0, limit: int = 20) -> List[Dict[str, Any]]:
        """è·å–æœ€é«˜åˆ†çš„æœºä¼š"""
        try:
            with db.get_connection("clusters") as conn:
                cursor = conn.execute("""
                    SELECT o.*, c.cluster_name, c.cluster_size
                    FROM opportunities o
                    JOIN clusters c ON o.cluster_id = c.id
                    WHERE o.total_score >= ?
                    ORDER BY o.total_score DESC
                    LIMIT ?
                """, (min_score, limit))
                opportunities = [dict(row) for row in cursor.fetchall()]

            return opportunities

        except Exception as e:
            logger.error(f"Failed to get top opportunities: {e}")
            return []

    def get_statistics(self) -> Dict[str, Any]:
        """è·å–è¯„åˆ†ç»Ÿè®¡ä¿¡æ¯"""
        stats = self.stats.copy()

        if stats["total_opportunities_scored"] > 0:
            stats["viable_rate"] = stats["viable_opportunities"] / stats["total_opportunities_scored"]
            stats["good_rate"] = stats["good_opportunities"] / stats["total_opportunities_scored"]
            stats["excellent_rate"] = stats["excellent_opportunities"] / stats["total_opportunities_scored"]
            stats["processing_rate"] = stats["total_opportunities_scored"] / max(stats["processing_time"], 1)
        else:
            stats["viable_rate"] = 0
            stats["good_rate"] = 0
            stats["excellent_rate"] = 0
            stats["processing_rate"] = 0

        return stats

    def reset_statistics(self):
        """é‡ç½®ç»Ÿè®¡ä¿¡æ¯"""
        self.stats = {
            "total_opportunities_scored": 0,
            "viable_opportunities": 0,
            "good_opportunities": 0,
            "excellent_opportunities": 0,
            "processing_time": 0.0,
            "avg_total_score": 0.0
        }

def main():
    """ä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description="Score opportunity viability for solo founders")
    parser.add_argument("--limit", type=int, default=100, help="Limit number of opportunities to score")
    parser.add_argument("--min-score", type=float, default=5.0, help="Minimum score for top opportunities")
    parser.add_argument("--list", action="store_true", help="List top scored opportunities")
    args = parser.parse_args()

    try:
        logger.info("Starting viability scoring...")

        scorer = ViabilityScorer()

        if args.list:
            # åˆ—å‡ºæœ€é«˜åˆ†çš„æœºä¼š
            top_opportunities = scorer.get_top_opportunities(min_score=args.min_score)
            print(json.dumps(top_opportunities, indent=2, default=str))

        else:
            # ä¸ºæœºä¼šè¯„åˆ†
            result = scorer.score_opportunities(limit=args.limit)

            logger.info(f"""
=== Viability Scoring Complete ===
Opportunities scored: {result['opportunities_scored']}
Viable opportunities: {result['viable_opportunities']}
Good opportunities: {result['good_opportunities']}
Excellent opportunities: {result['excellent_opportunities']}
Scoring stats: {result['scoring_stats']}
""")

    except Exception as e:
        logger.error(f"Fatal error: {e}")
        raise

if __name__ == "__main__":
    main()
```


================================================================================
æ–‡ä»¶: utils/db.py
================================================================================

```python
"""
Database utilities for Wise Collection
SQLiteæ•°æ®åº“æ“ä½œå·¥å…·
"""
import sqlite3
import json
import logging
from datetime import datetime
from typing import List, Dict, Any, Optional
from contextlib import contextmanager
import os

logger = logging.getLogger(__name__)

class WiseCollectionDB:
    """Wise Collectionç³»ç»Ÿæ•°æ®åº“ç®¡ç†å™¨"""

    def __init__(self, db_dir: str = "data", unified: bool = True):
        """åˆå§‹åŒ–æ•°æ®åº“è¿æ¥"""
        self.db_dir = db_dir
        self.unified = unified  # æ˜¯å¦ä½¿ç”¨ç»Ÿä¸€æ•°æ®åº“
        os.makedirs(db_dir, exist_ok=True)

        if unified:
            # ä½¿ç”¨ç»Ÿä¸€æ•°æ®åº“æ–‡ä»¶
            self.unified_db_path = os.path.join(db_dir, "wise_collection.db")
            # å…¼å®¹æ€§ï¼šä¿æŒåŸæœ‰è·¯å¾„å˜é‡
            self.raw_db_path = self.unified_db_path
            self.filtered_db_path = self.unified_db_path
            self.pain_db_path = self.unified_db_path
            self.clusters_db_path = self.unified_db_path
        else:
            # ä½¿ç”¨å¤šä¸ªæ•°æ®åº“æ–‡ä»¶ï¼ˆåŸæœ‰æ¨¡å¼ï¼‰
            self.raw_db_path = os.path.join(db_dir, "raw_posts.db")
            self.filtered_db_path = os.path.join(db_dir, "filtered_posts.db")
            self.pain_db_path = os.path.join(db_dir, "pain_events.db")
            self.clusters_db_path = os.path.join(db_dir, "clusters.db")

        # åˆå§‹åŒ–æ‰€æœ‰æ•°æ®åº“
        self._init_databases()

    @contextmanager
    def get_connection(self, db_type: str = "raw"):
        """è·å–æ•°æ®åº“è¿æ¥çš„ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
        if self.unified:
            # ç»Ÿä¸€æ•°æ®åº“æ¨¡å¼ï¼šæ‰€æœ‰è¿æ¥éƒ½æŒ‡å‘åŒä¸€ä¸ªæ–‡ä»¶
            db_path = self.unified_db_path
        else:
            # å¤šæ•°æ®åº“æ¨¡å¼ï¼šæ ¹æ®db_typeé€‰æ‹©ä¸åŒçš„æ–‡ä»¶
            db_paths = {
                "raw": self.raw_db_path,
                "filtered": self.filtered_db_path,
                "pain": self.pain_db_path,
                "clusters": self.clusters_db_path
            }

            if db_type not in db_paths:
                raise ValueError(f"Invalid db_type: {db_type}")
            db_path = db_paths[db_type]

        conn = None
        try:
            conn = sqlite3.connect(db_path)
            conn.row_factory = sqlite3.Row
            yield conn
        except Exception as e:
            if conn:
                conn.rollback()
            logger.error(f"Database error: {e}")
            raise
        finally:
            if conn:
                conn.close()

    def _init_databases(self):
        """åˆå§‹åŒ–æ‰€æœ‰æ•°æ®åº“è¡¨ç»“æ„"""
        if self.unified:
            self._init_unified_database()
        else:
            self._init_raw_posts_db()
            self._init_filtered_posts_db()
            self._init_pain_events_db()
            self._init_clusters_db()

    def _init_unified_database(self):
        """åˆå§‹åŒ–ç»Ÿä¸€æ•°æ®åº“ï¼ŒåŒ…å«æ‰€æœ‰è¡¨"""
        with self.get_connection("raw") as conn:
            # åˆ›å»ºåŸå§‹å¸–å­è¡¨ï¼ˆå‡çº§ç‰ˆ - æ”¯æŒå¤šæ•°æ®æºï¼‰
            conn.execute("""
                CREATE TABLE IF NOT EXISTS posts (
                    id TEXT PRIMARY KEY,
                    title TEXT NOT NULL,
                    body TEXT,
                    subreddit TEXT,
                    url TEXT NOT NULL,
                    source TEXT NOT NULL DEFAULT 'reddit',
                    source_id TEXT NOT NULL,
                    platform_data TEXT,
                    score INTEGER NOT NULL,
                    num_comments INTEGER NOT NULL,
                    upvote_ratio REAL,
                    is_self INTEGER,
                    created_utc REAL NOT NULL,
                    created_at TIMESTAMP NOT NULL,
                    author TEXT,
                    category TEXT,
                    collected_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    raw_data TEXT,  -- åŸå§‹JSONæ•°æ®
                    UNIQUE(source, source_id)
                )
            """)

            # åˆ›å»ºè¿‡æ»¤å¸–å­è¡¨
            conn.execute("""
                CREATE TABLE IF NOT EXISTS filtered_posts (
                    id TEXT PRIMARY KEY,
                    title TEXT NOT NULL,
                    body TEXT,
                    subreddit TEXT NOT NULL,
                    url TEXT NOT NULL,
                    score INTEGER NOT NULL,
                    num_comments INTEGER NOT NULL,
                    upvote_ratio REAL NOT NULL,
                    pain_score REAL NOT NULL,
                    pain_keywords TEXT,
                    filtered_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    filter_reason TEXT
                )
            """)

            # åˆ›å»ºç—›ç‚¹äº‹ä»¶è¡¨
            conn.execute("""
                CREATE TABLE IF NOT EXISTS pain_events (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    post_id TEXT NOT NULL,
                    actor TEXT,
                    context TEXT,
                    problem TEXT NOT NULL,
                    current_workaround TEXT,
                    frequency TEXT,
                    emotional_signal TEXT,
                    mentioned_tools TEXT,
                    extraction_confidence REAL,
                    extracted_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    FOREIGN KEY (post_id) REFERENCES filtered_posts(id)
                )
            """)

            # åˆ›å»ºåµŒå…¥å‘é‡è¡¨
            conn.execute("""
                CREATE TABLE IF NOT EXISTS pain_embeddings (
                    pain_event_id INTEGER PRIMARY KEY,
                    embedding_vector BLOB NOT NULL,
                    embedding_model TEXT NOT NULL,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    FOREIGN KEY (pain_event_id) REFERENCES pain_events(id)
                )
            """)

            # åˆ›å»ºèšç±»è¡¨
            conn.execute("""
                CREATE TABLE IF NOT EXISTS clusters (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    cluster_name TEXT NOT NULL,
                    cluster_description TEXT,
                    source_type TEXT,  -- æ–°å¢ï¼šæ•°æ®æºç±»å‹ (hn_ask, hn_show, reddit, etc.)
                    centroid_summary TEXT,  -- æ–°å¢ï¼šèšç±»ä¸­å¿ƒæ‘˜è¦
                    common_pain TEXT,  -- æ–°å¢ï¼šå…±åŒç—›ç‚¹
                    common_context TEXT,  -- æ–°å¢ï¼šå…±åŒä¸Šä¸‹æ–‡
                    example_events TEXT,  -- æ–°å¢ï¼šä»£è¡¨æ€§äº‹ä»¶ (JSONæ•°ç»„)
                    pain_event_ids TEXT NOT NULL,  -- JSONæ•°ç»„
                    cluster_size INTEGER NOT NULL,
                    avg_pain_score REAL,
                    workflow_confidence REAL,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            """)

            # åˆ›å»ºæœºä¼šè¡¨
            conn.execute("""
                CREATE TABLE IF NOT EXISTS opportunities (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    cluster_id INTEGER NOT NULL,
                    opportunity_name TEXT NOT NULL,
                    description TEXT NOT NULL,
                    current_tools TEXT,
                    missing_capability TEXT,
                    why_existing_fail TEXT,
                    target_users TEXT,
                    pain_frequency_score REAL,
                    market_size_score REAL,
                    mvp_complexity_score REAL,
                    competition_risk_score REAL,
                    integration_complexity_score REAL,
                    total_score REAL,
                    killer_risks TEXT,  -- JSONæ•°ç»„
                    recommendation TEXT,  -- AIå»ºè®®ï¼špursue/modify/abandon with reason
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    FOREIGN KEY (cluster_id) REFERENCES clusters(id)
                )
            """)

            # åˆ›å»ºè·¨æºå¯¹é½é—®é¢˜è¡¨
            conn.execute("""
                CREATE TABLE IF NOT EXISTS aligned_problems (
                    id TEXT PRIMARY KEY,  -- aligned_AP_XX_timestamp format
                    aligned_problem_id TEXT UNIQUE,  -- AP_XX format
                    sources TEXT,  -- JSON array of source types
                    core_problem TEXT,
                    why_they_look_different TEXT,
                    evidence TEXT,  -- JSON array of evidence objects
                    cluster_ids TEXT,  -- JSON array of original cluster IDs
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            """)

            # åˆ›å»ºæ‰€æœ‰ç´¢å¼•
            # postsè¡¨ç´¢å¼•
            conn.execute("CREATE INDEX IF NOT EXISTS idx_posts_subreddit ON posts(subreddit)")
            conn.execute("CREATE INDEX IF NOT EXISTS idx_posts_score ON posts(score)")
            conn.execute("CREATE INDEX IF NOT EXISTS idx_posts_collected_at ON posts(collected_at)")

            # æ£€æŸ¥æ–°åˆ—æ˜¯å¦å­˜åœ¨ï¼Œç„¶ååˆ›å»ºç›¸åº”ç´¢å¼•
            cursor = conn.execute("PRAGMA table_info(posts)")
            existing_columns = {row['name'] for row in cursor.fetchall()}

            if 'source' in existing_columns:
                conn.execute("CREATE INDEX IF NOT EXISTS idx_posts_source ON posts(source)")

            if 'created_at' in existing_columns:
                conn.execute("CREATE INDEX IF NOT EXISTS idx_posts_source_created ON posts(source, created_at)")

            if 'source' in existing_columns and 'source_id' in existing_columns:
                conn.execute("CREATE INDEX IF NOT EXISTS idx_posts_unique_source ON posts(source, source_id)")

            # filtered_postsè¡¨ç´¢å¼•
            conn.execute("CREATE INDEX IF NOT EXISTS idx_filtered_pain_score ON filtered_posts(pain_score)")
            conn.execute("CREATE INDEX IF NOT EXISTS idx_filtered_subreddit ON filtered_posts(subreddit)")
            conn.execute("CREATE INDEX IF NOT EXISTS idx_filtered_at ON filtered_posts(filtered_at)")

            # pain_eventsè¡¨ç´¢å¼•
            conn.execute("CREATE INDEX IF NOT EXISTS idx_pain_post_id ON pain_events(post_id)")
            conn.execute("CREATE INDEX IF NOT EXISTS idx_pain_problem ON pain_events(problem)")
            conn.execute("CREATE INDEX IF NOT EXISTS idx_pain_extracted_at ON pain_events(extracted_at)")

            # clustersè¡¨ç´¢å¼•
            conn.execute("CREATE INDEX IF NOT EXISTS idx_clusters_size ON clusters(cluster_size)")

            # opportunitiesè¡¨ç´¢å¼•
            conn.execute("CREATE INDEX IF NOT EXISTS idx_opportunities_score ON opportunities(total_score)")
            conn.execute("CREATE INDEX IF NOT EXISTS idx_opportunities_cluster_id ON opportunities(cluster_id)")

            # æ·»åŠ å¯¹é½è·Ÿè¸ªåˆ—åˆ°clustersè¡¨ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
            self._add_alignment_columns_to_clusters(conn)

            conn.commit()
            logger.info("Unified database initialized successfully")

    def _init_raw_posts_db(self):
        """åˆå§‹åŒ–åŸå§‹å¸–å­æ•°æ®åº“"""
        with self.get_connection("raw") as conn:
            conn.execute("""
                CREATE TABLE IF NOT EXISTS posts (
                    id TEXT PRIMARY KEY,
                    title TEXT NOT NULL,
                    body TEXT,
                    subreddit TEXT,
                    url TEXT NOT NULL,
                    source TEXT NOT NULL DEFAULT 'reddit',
                    source_id TEXT NOT NULL,
                    platform_data TEXT,
                    score INTEGER NOT NULL,
                    num_comments INTEGER NOT NULL,
                    upvote_ratio REAL,
                    is_self INTEGER,
                    created_utc REAL NOT NULL,
                    created_at TIMESTAMP NOT NULL,
                    author TEXT,
                    category TEXT,
                    collected_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    raw_data TEXT,  -- åŸå§‹JSONæ•°æ®
                    UNIQUE(source, source_id)
                )
            """)

            # åˆ›å»ºç´¢å¼•
            conn.execute("CREATE INDEX IF NOT EXISTS idx_posts_subreddit ON posts(subreddit)")
            conn.execute("CREATE INDEX IF NOT EXISTS idx_posts_score ON posts(score)")
            conn.execute("CREATE INDEX IF NOT EXISTS idx_posts_collected_at ON posts(collected_at)")

            # æ£€æŸ¥æ–°åˆ—æ˜¯å¦å­˜åœ¨ï¼Œç„¶ååˆ›å»ºç›¸åº”ç´¢å¼•
            cursor = conn.execute("PRAGMA table_info(posts)")
            existing_columns = {row['name'] for row in cursor.fetchall()}

            if 'source' in existing_columns:
                conn.execute("CREATE INDEX IF NOT EXISTS idx_posts_source ON posts(source)")

            if 'created_at' in existing_columns:
                conn.execute("CREATE INDEX IF NOT EXISTS idx_posts_source_created ON posts(source, created_at)")

            if 'source' in existing_columns and 'source_id' in existing_columns:
                conn.execute("CREATE INDEX IF NOT EXISTS idx_posts_unique_source ON posts(source, source_id)")
            conn.commit()

    def _init_filtered_posts_db(self):
        """åˆå§‹åŒ–è¿‡æ»¤åçš„å¸–å­æ•°æ®åº“"""
        with self.get_connection("filtered") as conn:
            conn.execute("""
                CREATE TABLE IF NOT EXISTS filtered_posts (
                    id TEXT PRIMARY KEY,
                    title TEXT NOT NULL,
                    body TEXT,
                    subreddit TEXT NOT NULL,
                    url TEXT NOT NULL,
                    score INTEGER NOT NULL,
                    num_comments INTEGER NOT NULL,
                    upvote_ratio REAL NOT NULL,
                    pain_score REAL NOT NULL,
                    pain_keywords TEXT,
                    filtered_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    filter_reason TEXT
                )
            """)

            # åˆ›å»ºç´¢å¼•
            conn.execute("CREATE INDEX IF NOT EXISTS idx_filtered_pain_score ON filtered_posts(pain_score)")
            conn.execute("CREATE INDEX IF NOT EXISTS idx_filtered_subreddit ON filtered_posts(subreddit)")
            conn.execute("CREATE INDEX IF NOT EXISTS idx_filtered_at ON filtered_posts(filtered_at)")
            conn.commit()

    def _init_pain_events_db(self):
        """åˆå§‹åŒ–ç—›ç‚¹äº‹ä»¶æ•°æ®åº“"""
        with self.get_connection("pain") as conn:
            conn.execute("""
                CREATE TABLE IF NOT EXISTS pain_events (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    post_id TEXT NOT NULL,
                    actor TEXT,
                    context TEXT,
                    problem TEXT NOT NULL,
                    current_workaround TEXT,
                    frequency TEXT,
                    emotional_signal TEXT,
                    mentioned_tools TEXT,
                    extraction_confidence REAL,
                    extracted_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    FOREIGN KEY (post_id) REFERENCES filtered_posts(id)
                )
            """)

            # åˆ›å»ºåµŒå…¥å‘é‡è¡¨
            conn.execute("""
                CREATE TABLE IF NOT EXISTS pain_embeddings (
                    pain_event_id INTEGER PRIMARY KEY,
                    embedding_vector BLOB NOT NULL,
                    embedding_model TEXT NOT NULL,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    FOREIGN KEY (pain_event_id) REFERENCES pain_events(id)
                )
            """)

            # åˆ›å»ºç´¢å¼•
            conn.execute("CREATE INDEX IF NOT EXISTS idx_pain_post_id ON pain_events(post_id)")
            conn.execute("CREATE INDEX IF NOT EXISTS idx_pain_problem ON pain_events(problem)")
            conn.execute("CREATE INDEX IF NOT EXISTS idx_pain_extracted_at ON pain_events(extracted_at)")
            conn.commit()

    def _init_clusters_db(self):
        """åˆå§‹åŒ–èšç±»æ•°æ®åº“"""
        with self.get_connection("clusters") as conn:
            conn.execute("""
                CREATE TABLE IF NOT EXISTS clusters (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    cluster_name TEXT NOT NULL,
                    cluster_description TEXT,
                    source_type TEXT,  -- æ–°å¢ï¼šæ•°æ®æºç±»å‹ (hn_ask, hn_show, reddit, etc.)
                    centroid_summary TEXT,  -- æ–°å¢ï¼šèšç±»ä¸­å¿ƒæ‘˜è¦
                    common_pain TEXT,  -- æ–°å¢ï¼šå…±åŒç—›ç‚¹
                    common_context TEXT,  -- æ–°å¢ï¼šå…±åŒä¸Šä¸‹æ–‡
                    example_events TEXT,  -- æ–°å¢ï¼šä»£è¡¨æ€§äº‹ä»¶ (JSONæ•°ç»„)
                    pain_event_ids TEXT NOT NULL,  -- JSONæ•°ç»„
                    cluster_size INTEGER NOT NULL,
                    avg_pain_score REAL,
                    workflow_confidence REAL,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            """)

            # åˆ›å»ºæœºä¼šè¡¨
            conn.execute("""
                CREATE TABLE IF NOT EXISTS opportunities (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    cluster_id INTEGER NOT NULL,
                    opportunity_name TEXT NOT NULL,
                    description TEXT NOT NULL,
                    current_tools TEXT,
                    missing_capability TEXT,
                    why_existing_fail TEXT,
                    target_users TEXT,
                    pain_frequency_score REAL,
                    market_size_score REAL,
                    mvp_complexity_score REAL,
                    competition_risk_score REAL,
                    integration_complexity_score REAL,
                    total_score REAL,
                    killer_risks TEXT,  -- JSONæ•°ç»„
                    recommendation TEXT,  -- AIå»ºè®®ï¼špursue/modify/abandon with reason
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    FOREIGN KEY (cluster_id) REFERENCES clusters(id)
                )
            """)

            # åˆ›å»ºè·¨æºå¯¹é½é—®é¢˜è¡¨
            conn.execute("""
                CREATE TABLE IF NOT EXISTS aligned_problems (
                    id TEXT PRIMARY KEY,  -- aligned_AP_XX_timestamp format
                    aligned_problem_id TEXT UNIQUE,  -- AP_XX format
                    sources TEXT,  -- JSON array of source types
                    core_problem TEXT,
                    why_they_look_different TEXT,
                    evidence TEXT,  -- JSON array of evidence objects
                    cluster_ids TEXT,  -- JSON array of original cluster IDs
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            """)

            # åˆ›å»ºç´¢å¼•
            conn.execute("CREATE INDEX IF NOT EXISTS idx_clusters_size ON clusters(cluster_size)")
            conn.execute("CREATE INDEX IF NOT EXISTS idx_opportunities_score ON opportunities(total_score)")
            conn.execute("CREATE INDEX IF NOT EXISTS idx_opportunities_cluster_id ON opportunities(cluster_id)")

            # æ·»åŠ å¯¹é½è·Ÿè¸ªåˆ—åˆ°clustersè¡¨ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
            self._add_alignment_columns_to_clusters(conn)

            conn.commit()

    def _add_alignment_columns_to_clusters(self, conn):
        """ä¸ºclustersè¡¨æ·»åŠ å¯¹é½è·Ÿè¸ªåˆ—ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰"""
        try:
            # æ£€æŸ¥åˆ—æ˜¯å¦å­˜åœ¨
            cursor = conn.execute("PRAGMA table_info(clusters)")
            existing_columns = {row['name'] for row in cursor.fetchall()}

            # æ·»åŠ alignment_statusåˆ—ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
            if 'alignment_status' not in existing_columns:
                conn.execute("""
                    ALTER TABLE clusters
                    ADD COLUMN alignment_status TEXT DEFAULT 'unprocessed'
                """)
                logger.info("Added alignment_status column to clusters table")

            # æ·»åŠ aligned_problem_idåˆ—ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
            if 'aligned_problem_id' not in existing_columns:
                conn.execute("""
                    ALTER TABLE clusters
                    ADD COLUMN aligned_problem_id TEXT
                """)
                logger.info("Added aligned_problem_id column to clusters table")

            # åˆ›å»ºç›¸å…³ç´¢å¼•
            conn.execute("CREATE INDEX IF NOT EXISTS idx_clusters_alignment_status ON clusters(alignment_status)")
            conn.execute("CREATE INDEX IF NOT EXISTS idx_clusters_aligned_problem_id ON clusters(aligned_problem_id)")

        except Exception as e:
            logger.error(f"Failed to add alignment columns to clusters table: {e}")

    # Raw posts operations
    def insert_raw_post(self, post_data: Dict[str, Any]) -> bool:
        """æ’å…¥åŸå§‹å¸–å­æ•°æ®ï¼ˆæ”¯æŒå¤šæ•°æ®æºï¼‰"""
        try:
            with self.get_connection("raw") as conn:
                conn.execute("""
                    INSERT OR REPLACE INTO posts
                    (id, title, body, subreddit, url, source, source_id, platform_data,
                     score, num_comments, upvote_ratio, is_self, created_utc, created_at,
                     author, category, raw_data)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                """, (
                    post_data.get("id"),                    # ç»Ÿä¸€ID (å…¼å®¹æ—§æ•°æ®)
                    post_data["title"],
                    post_data.get("body", ""),
                    post_data.get("subreddit", "unknown"),   # å…¼å®¹æ€§ï¼šå¦‚æœæ²¡æœ‰subredditï¼Œä½¿ç”¨unknown
                    post_data["url"],
                    post_data.get("source", "reddit"),      # æ–°å­—æ®µï¼Œé»˜è®¤ä¸ºreddit
                    post_data.get("source_id"),             # æ–°å­—æ®µ
                    json.dumps(post_data.get("platform_data", {})),  # æ–°å­—æ®µ
                    post_data["score"],
                    post_data["num_comments"],
                    post_data.get("upvote_ratio"),          # å¯èƒ½ä¸ºNone (æ–°å­—æ®µ)
                    post_data.get("is_self"),               # å¯èƒ½ä¸ºNone (æ–°å­—æ®µ)
                    post_data.get("created_utc", 0),
                    post_data.get("created_at", datetime.now().isoformat()),  # æ–°å­—æ®µ
                    post_data.get("author", ""),
                    post_data.get("category", ""),
                    json.dumps(post_data)
                ))
                conn.commit()
                return True
        except Exception as e:
            logger.error(f"Failed to insert raw post {post_data.get('id')}: {e}")
            return False

    def get_unprocessed_posts(self, limit: int = 100) -> List[Dict]:
        """è·å–æœªå¤„ç†çš„å¸–å­"""
        try:
            # é¦–å…ˆè·å–æ‰€æœ‰å·²å¤„ç†çš„å¸–å­ID
            with self.get_connection("filtered") as conn:
                cursor = conn.execute("SELECT id FROM filtered_posts")
                processed_ids = {row['id'] for row in cursor.fetchall()}

            # ç„¶åè·å–æœªå¤„ç†çš„å¸–å­
            with self.get_connection("raw") as conn:
                if processed_ids:
                    # å¦‚æœæœ‰å·²å¤„ç†çš„å¸–å­ï¼Œæ’é™¤å®ƒä»¬
                    placeholders = ','.join('?' * len(processed_ids))
                    cursor = conn.execute(f"""
                        SELECT * FROM posts
                        WHERE id NOT IN ({placeholders})
                        ORDER BY collected_at DESC
                        LIMIT ?
                    """, list(processed_ids) + [limit])
                else:
                    # å¦‚æœæ²¡æœ‰å·²å¤„ç†çš„å¸–å­ï¼Œç›´æ¥è·å–
                    cursor = conn.execute("""
                        SELECT * FROM posts
                        ORDER BY collected_at DESC
                        LIMIT ?
                    """, (limit,))
                return [dict(row) for row in cursor.fetchall()]
        except Exception as e:
            logger.error(f"Failed to get unprocessed posts: {e}")
            return []

    def get_unprocessed_posts_by_source(self, source: str, limit: int = 100) -> List[Dict]:
        """è·å–æœªå¤„ç†çš„å¸–å­ï¼Œæ”¯æŒæŒ‰æ•°æ®æºè¿‡æ»¤"""
        try:
            # é¦–å…ˆè·å–æ‰€æœ‰å·²å¤„ç†çš„å¸–å­ID
            with self.get_connection("filtered") as conn:
                cursor = conn.execute("SELECT id FROM filtered_posts")
                processed_ids = {row['id'] for row in cursor.fetchall()}

            # ç„¶åè·å–æœªå¤„ç†çš„å¸–å­ï¼ŒæŒ‰æ•°æ®æºè¿‡æ»¤
            with self.get_connection("raw") as conn:
                if processed_ids:
                    # å¦‚æœæœ‰å·²å¤„ç†çš„å¸–å­ï¼Œæ’é™¤å®ƒä»¬
                    placeholders = ','.join('?' * len(processed_ids))
                    cursor = conn.execute(f"""
                        SELECT * FROM posts
                        WHERE source = ?
                        AND id NOT IN ({placeholders})
                        ORDER BY collected_at DESC
                        LIMIT ?
                    """, [source] + list(processed_ids) + [limit])
                else:
                    # å¦‚æœæ²¡æœ‰å·²å¤„ç†çš„å¸–å­ï¼Œç›´æ¥è·å–
                    cursor = conn.execute("""
                        SELECT * FROM posts
                        WHERE source = ?
                        ORDER BY collected_at DESC
                        LIMIT ?
                    """, (source, limit))
                return [dict(row) for row in cursor.fetchall()]
        except Exception as e:
            logger.error(f"Failed to get unprocessed posts for source {source}: {e}")
            return []

    # Filtered posts operations
    def insert_filtered_post(self, post_data: Dict[str, Any]) -> bool:
        """æ’å…¥è¿‡æ»¤åçš„å¸–å­"""
        try:
            with self.get_connection("filtered") as conn:
                conn.execute("""
                    INSERT OR REPLACE INTO filtered_posts
                    (id, title, body, subreddit, url, score, num_comments,
                     upvote_ratio, pain_score, pain_keywords, filter_reason)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                """, (
                    post_data["id"],
                    post_data["title"],
                    post_data.get("body", ""),
                    post_data["subreddit"],
                    post_data["url"],
                    post_data["score"],
                    post_data["num_comments"],
                    post_data.get("upvote_ratio", 0.0),
                    post_data.get("pain_score", 0.0),
                    json.dumps(post_data.get("pain_keywords", [])),
                    post_data.get("filter_reason", "")
                ))
                conn.commit()
                return True
        except Exception as e:
            logger.error(f"Failed to insert filtered post {post_data.get('id')}: {e}")
            return False

    def get_filtered_posts(self, limit: int = 100, min_pain_score: float = 0.0) -> List[Dict]:
        """è·å–è¿‡æ»¤åçš„å¸–å­"""
        try:
            # é¦–å…ˆè·å–æ‰€æœ‰å·²æå–çš„å¸–å­ID
            with self.get_connection("pain") as conn:
                cursor = conn.execute("SELECT DISTINCT post_id FROM pain_events")
                extracted_ids = {row['post_id'] for row in cursor.fetchall()}

            # ç„¶åè·å–è¿‡æ»¤åçš„å¸–å­
            with self.get_connection("filtered") as conn:
                if extracted_ids:
                    # å¦‚æœæœ‰å·²æå–çš„å¸–å­ï¼Œæ’é™¤å®ƒä»¬
                    placeholders = ','.join('?' * len(extracted_ids))
                    cursor = conn.execute(f"""
                        SELECT * FROM filtered_posts
                        WHERE pain_score >= ?
                        AND id NOT IN ({placeholders})
                        ORDER BY pain_score DESC
                        LIMIT ?
                    """, [min_pain_score] + list(extracted_ids) + [limit])
                else:
                    # å¦‚æœæ²¡æœ‰å·²æå–çš„å¸–å­ï¼Œç›´æ¥è·å–
                    cursor = conn.execute("""
                        SELECT * FROM filtered_posts
                        WHERE pain_score >= ?
                        ORDER BY pain_score DESC
                        LIMIT ?
                    """, (min_pain_score, limit))
                return [dict(row) for row in cursor.fetchall()]
        except Exception as e:
            logger.error(f"Failed to get filtered posts: {e}")
            return []

    # Pain events operations
    def insert_pain_event(self, pain_data: Dict[str, Any]) -> Optional[int]:
        """æ’å…¥ç—›ç‚¹äº‹ä»¶"""
        try:
            with self.get_connection("pain") as conn:
                cursor = conn.execute("""
                    INSERT INTO pain_events
                    (post_id, actor, context, problem, current_workaround,
                     frequency, emotional_signal, mentioned_tools, extraction_confidence)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
                """, (
                    pain_data["post_id"],
                    pain_data.get("actor", ""),
                    pain_data.get("context", ""),
                    pain_data["problem"],
                    pain_data.get("current_workaround", ""),
                    pain_data.get("frequency", ""),
                    pain_data.get("emotional_signal", ""),
                    json.dumps(pain_data.get("mentioned_tools", [])),
                    pain_data.get("extraction_confidence", 0.0)
                ))
                pain_event_id = cursor.lastrowid
                conn.commit()
                return pain_event_id
        except Exception as e:
            logger.error(f"Failed to insert pain event: {e}")
            return None

    def insert_pain_embedding(self, pain_event_id: int, embedding_vector: List[float], model_name: str) -> bool:
        """æ’å…¥ç—›ç‚¹åµŒå…¥å‘é‡"""
        try:
            import pickle
            embedding_blob = pickle.dumps(embedding_vector)

            with self.get_connection("pain") as conn:
                conn.execute("""
                    INSERT OR REPLACE INTO pain_embeddings
                    (pain_event_id, embedding_vector, embedding_model)
                    VALUES (?, ?, ?)
                """, (pain_event_id, embedding_blob, model_name))
                conn.commit()
                return True
        except Exception as e:
            logger.error(f"Failed to insert pain embedding for event {pain_event_id}: {e}")
            return False

    def get_pain_events_without_embeddings(self, limit: int = 100) -> List[Dict]:
        """è·å–æ²¡æœ‰åµŒå…¥å‘é‡çš„ç—›ç‚¹äº‹ä»¶"""
        try:
            with self.get_connection("pain") as conn:
                cursor = conn.execute("""
                    SELECT p.* FROM pain_events p
                    LEFT JOIN pain_embeddings e ON p.id = e.pain_event_id
                    WHERE e.pain_event_id IS NULL
                    ORDER BY p.extracted_at DESC
                    LIMIT ?
                """, (limit,))
                return [dict(row) for row in cursor.fetchall()]
        except Exception as e:
            logger.error(f"Failed to get pain events without embeddings: {e}")
            return []

    def get_all_pain_events_with_embeddings(self) -> List[Dict]:
        """è·å–æ‰€æœ‰æœ‰åµŒå…¥å‘é‡çš„ç—›ç‚¹äº‹ä»¶"""
        try:
            import pickle
            with self.get_connection("pain") as conn:
                cursor = conn.execute("""
                    SELECT p.*, e.embedding_vector, e.embedding_model
                    FROM pain_events p
                    JOIN pain_embeddings e ON p.id = e.pain_event_id
                    ORDER BY p.extracted_at DESC
                """)
                results = []
                for row in cursor.fetchall():
                    event_data = dict(row)
                    # ååºåˆ—åŒ–åµŒå…¥å‘é‡
                    if event_data["embedding_vector"]:
                        event_data["embedding_vector"] = pickle.loads(event_data["embedding_vector"])
                    results.append(event_data)
                return results
        except Exception as e:
            logger.error(f"Failed to get pain events with embeddings: {e}")
            return []

    # Clusters operations
    def insert_cluster(self, cluster_data: Dict[str, Any]) -> Optional[int]:
        """æ’å…¥èšç±»"""
        try:
            with self.get_connection("clusters") as conn:
                cursor = conn.execute("""
                    INSERT INTO clusters
                    (cluster_name, cluster_description, source_type, centroid_summary,
                     common_pain, common_context, example_events, pain_event_ids, cluster_size,
                     avg_pain_score, workflow_confidence)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                """, (
                    cluster_data["cluster_name"],
                    cluster_data.get("cluster_description", ""),
                    cluster_data.get("source_type", ""),
                    cluster_data.get("centroid_summary", ""),
                    cluster_data.get("common_pain", ""),
                    cluster_data.get("common_context", ""),
                    json.dumps(cluster_data.get("example_events", [])),
                    json.dumps(cluster_data["pain_event_ids"]),
                    cluster_data["cluster_size"],
                    cluster_data.get("avg_pain_score", 0.0),
                    cluster_data.get("workflow_confidence", 0.0)
                ))
                cluster_id = cursor.lastrowid
                conn.commit()
                return cluster_id
        except Exception as e:
            logger.error(f"Failed to insert cluster: {e}")
            return None

    def insert_opportunity(self, opportunity_data: Dict[str, Any]) -> Optional[int]:
        """æ’å…¥æœºä¼š"""
        try:
            with self.get_connection("clusters") as conn:
                cursor = conn.execute("""
                    INSERT INTO opportunities
                    (cluster_id, opportunity_name, description, current_tools,
                     missing_capability, why_existing_fail, target_users,
                     pain_frequency_score, market_size_score, mvp_complexity_score,
                     competition_risk_score, integration_complexity_score, total_score, killer_risks, recommendation)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                """, (
                    opportunity_data["cluster_id"],
                    opportunity_data["opportunity_name"],
                    opportunity_data["description"],
                    opportunity_data.get("current_tools", ""),
                    opportunity_data.get("missing_capability", ""),
                    opportunity_data.get("why_existing_fail", ""),
                    opportunity_data.get("target_users", ""),
                    opportunity_data.get("pain_frequency_score", 0.0),
                    opportunity_data.get("market_size_score", 0.0),
                    opportunity_data.get("mvp_complexity_score", 0.0),
                    opportunity_data.get("competition_risk_score", 0.0),
                    opportunity_data.get("integration_complexity_score", 0.0),
                    opportunity_data.get("total_score", 0.0),
                    json.dumps(opportunity_data.get("killer_risks", [])),
                    opportunity_data.get("recommendation", "")
                ))
                opportunity_id = cursor.lastrowid
                conn.commit()
                return opportunity_id
        except Exception as e:
            logger.error(f"Failed to insert opportunity: {e}")
            return None

    def get_top_opportunities(self, limit: int = 20) -> List[Dict]:
        """è·å–æœ€é«˜åˆ†çš„æœºä¼š"""
        try:
            with self.get_connection("clusters") as conn:
                cursor = conn.execute("""
                    SELECT o.*, c.cluster_name, c.cluster_description
                    FROM opportunities o
                    JOIN clusters c ON o.cluster_id = c.id
                    ORDER BY o.total_score DESC
                    LIMIT ?
                """, (limit,))
                return [dict(row) for row in cursor.fetchall()]
        except Exception as e:
            logger.error(f"Failed to get top opportunities: {e}")
            return []

    # Statistics operations
    def get_statistics(self) -> Dict[str, Any]:
        """è·å–æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯"""
        stats = {}

        try:
            # Raw posts count
            with self.get_connection("raw") as conn:
                cursor = conn.execute("SELECT COUNT(*) as count FROM posts")
                stats["raw_posts_count"] = cursor.fetchone()["count"]

            # Filtered posts count
            with self.get_connection("filtered") as conn:
                cursor = conn.execute("SELECT COUNT(*) as count FROM filtered_posts")
                stats["filtered_posts_count"] = cursor.fetchone()["count"]

                cursor = conn.execute("SELECT AVG(pain_score) as avg_score FROM filtered_posts")
                stats["avg_pain_score"] = cursor.fetchone()["avg_score"] or 0

            # Pain events count
            with self.get_connection("pain") as conn:
                cursor = conn.execute("SELECT COUNT(*) as count FROM pain_events")
                stats["pain_events_count"] = cursor.fetchone()["count"]

            # Clusters count
            with self.get_connection("clusters") as conn:
                cursor = conn.execute("SELECT COUNT(*) as count FROM clusters")
                stats["clusters_count"] = cursor.fetchone()["count"]

                cursor = conn.execute("SELECT COUNT(*) as count FROM opportunities")
                stats["opportunities_count"] = cursor.fetchone()["count"]

                # å¯¹é½ç»Ÿè®¡
                cursor = conn.execute("SELECT COUNT(*) as count FROM aligned_problems")
                stats["aligned_problems_count"] = cursor.fetchone()["count"]

                cursor = conn.execute("""
                    SELECT alignment_status, COUNT(*) as count
                    FROM clusters
                    WHERE alignment_status IS NOT NULL
                    GROUP BY alignment_status
                """)
                stats["clusters_by_alignment_status"] = {
                    row['alignment_status']: row['count']
                    for row in cursor.fetchall()
                }

            # æŒ‰æ•°æ®æºç»Ÿè®¡åŸå§‹å¸–å­
            with self.get_connection("raw") as conn:
                cursor = conn.execute("""
                    SELECT source, COUNT(*) as count
                    FROM posts
                    GROUP BY source
                """)
                stats["posts_by_source"] = {row['source']: row['count'] for row in cursor.fetchall()}

        except Exception as e:
            logger.error(f"Failed to get statistics: {e}")

        return stats

# æ·»åŠ ä¸€äº›ä¾¿åˆ©æ–¹æ³•
    def is_unified(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä½¿ç”¨ç»Ÿä¸€æ•°æ®åº“"""
        return self.unified

    def get_database_path(self) -> str:
        """è·å–å½“å‰ä½¿ç”¨çš„æ•°æ®åº“è·¯å¾„"""
        return self.unified_db_path if self.unified else "Multiple DB files"

    def switch_to_unified(self):
        """åˆ‡æ¢åˆ°ç»Ÿä¸€æ•°æ®åº“æ¨¡å¼ï¼ˆéœ€è¦é‡å¯åº”ç”¨ï¼‰"""
        if not self.unified:
            logger.warning("Switch to unified database mode requires application restart")

    # è·¨æºå¯¹é½ç›¸å…³æ–¹æ³•
    def get_aligned_problems(self) -> List[Dict]:
        """è·å–æ‰€æœ‰å¯¹é½é—®é¢˜"""
        try:
            with self.get_connection("clusters") as conn:
                cursor = conn.execute("""
                    SELECT id, aligned_problem_id, sources, core_problem,
                           why_they_look_different, evidence, cluster_ids, created_at
                    FROM aligned_problems
                    ORDER BY created_at DESC
                """)

                results = []
                for row in cursor.fetchall():
                    result = dict(row)
                    result['sources'] = json.loads(result['sources'])
                    result['evidence'] = json.loads(result['evidence'])
                    result['cluster_ids'] = json.loads(result['cluster_ids'])
                    results.append(result)

                return results

        except Exception as e:
            logger.error(f"Failed to get aligned problems: {e}")
            return []

    def update_cluster_alignment_status(self, cluster_name: str, status: str, aligned_problem_id: str = None):
        """æ›´æ–°èšç±»å¯¹é½çŠ¶æ€"""
        try:
            with self.get_connection("clusters") as conn:
                conn.execute("""
                    UPDATE clusters
                    SET alignment_status = ?, aligned_problem_id = ?
                    WHERE cluster_name = ?
                """, (status, aligned_problem_id, cluster_name))
                conn.commit()
        except Exception as e:
            logger.error(f"Failed to update cluster alignment status: {e}")
            raise

    def insert_aligned_problem(self, aligned_problem_data: Dict):
        """æ’å…¥æ–°çš„å¯¹é½é—®é¢˜"""
        try:
            with self.get_connection("clusters") as conn:
                conn.execute("""
                    INSERT OR REPLACE INTO aligned_problems
                    (id, aligned_problem_id, sources, core_problem,
                     why_they_look_different, evidence, cluster_ids)
                    VALUES (?, ?, ?, ?, ?, ?, ?)
                """, (
                    aligned_problem_data['id'],
                    aligned_problem_data['aligned_problem_id'],
                    json.dumps(aligned_problem_data['sources']),
                    aligned_problem_data['core_problem'],
                    aligned_problem_data['why_they_look_different'],
                    json.dumps(aligned_problem_data['evidence']),
                    json.dumps(aligned_problem_data['cluster_ids'])
                ))
                conn.commit()
        except Exception as e:
            logger.error(f"Failed to insert aligned problem: {e}")
            raise

    def get_clusters_for_opportunity_mapping(self) -> List[Dict]:
        """è·å–ç”¨äºæœºä¼šæ˜ å°„çš„èšç±»ï¼ˆåŒ…æ‹¬å¯¹é½é—®é¢˜ï¼‰"""
        try:
            with self.get_connection("clusters") as conn:
                # è·å–æœªå¯¹é½çš„åŸå§‹èšç±»
                cursor = conn.execute("""
                    SELECT id, cluster_name, source_type, centroid_summary,
                           common_pain, pain_event_ids, cluster_size,
                           cluster_description, workflow_confidence, created_at
                    FROM clusters
                    WHERE alignment_status IN ('unprocessed', 'processed')
                    OR alignment_status IS NULL
                """)

                clusters = [dict(row) for row in cursor.fetchall()]

                # è·å–å¯¹é½é—®é¢˜ä½œä¸º"è™šæ‹Ÿèšç±»"ï¼Œåœ¨Pythonä¸­è®¡ç®—cluster_size
                cursor = conn.execute("""
                    SELECT aligned_problem_id as cluster_name,
                           'aligned' as source_type,
                           core_problem as centroid_summary,
                           '' as common_pain,
                           '[]' as pain_event_ids,
                           cluster_ids
                    FROM aligned_problems
                """)

                aligned_clusters = []
                for row in cursor.fetchall():
                    cluster_dict = dict(row)
                    # åœ¨Pythonä¸­è®¡ç®—JSONæ•°ç»„çš„é•¿åº¦
                    cluster_ids = json.loads(cluster_dict['cluster_ids'])
                    cluster_dict['cluster_size'] = len(cluster_ids)
                    aligned_clusters.append(cluster_dict)

                # åˆå¹¶ç»“æœ
                return clusters + aligned_clusters

        except Exception as e:
            logger.error(f"Failed to get clusters for opportunity mapping: {e}")
            return []

    def get_clusters_for_aligned_problem(self, aligned_problem_id: str) -> List[Dict]:
        """è·å–å¯¹é½é—®é¢˜çš„æ”¯æŒèšç±»"""
        try:
            with self.get_connection("clusters") as conn:
                # é¦–å…ˆè·å–å¯¹é½é—®é¢˜çš„cluster_ids
                cursor = conn.execute("""
                    SELECT cluster_ids
                    FROM aligned_problems
                    WHERE aligned_problem_id = ?
                """, (aligned_problem_id,))

                result = cursor.fetchone()
                if not result:
                    return []

                cluster_ids = json.loads(result['cluster_ids'])

                # è·å–è¿™äº›èšç±»çš„è¯¦ç»†ä¿¡æ¯
                if not cluster_ids:
                    return []

                placeholders = ','.join(['?' for _ in cluster_ids])
                cursor = conn.execute(f"""
                    SELECT cluster_name, source_type, centroid_summary,
                           common_pain, cluster_size
                    FROM clusters
                    WHERE cluster_name IN ({placeholders})
                """, cluster_ids)

                return [dict(row) for row in cursor.fetchall()]

        except Exception as e:
            logger.error(f"Failed to get clusters for aligned problem {aligned_problem_id}: {e}")
            return []

    def get_cross_table_stats(self) -> Dict[str, Any]:
        """è·å–è·¨è¡¨ç»Ÿè®¡ä¿¡æ¯ï¼ˆä»…åœ¨ç»Ÿä¸€æ¨¡å¼ä¸‹æœ‰æ•ˆï¼‰"""
        if not self.unified:
            logger.warning("Cross-table stats only available in unified mode")
            return {}

        stats = {}
        try:
            with self.get_connection("raw") as conn:
                # è·å–å„è¡¨çš„è®°å½•æ•°
                tables = ['posts', 'filtered_posts', 'pain_events', 'clusters', 'opportunities']
                for table in tables:
                    cursor = conn.execute(f"SELECT COUNT(*) as count FROM {table}")
                    stats[f"{table}_count"] = cursor.fetchone()["count"]

                # è·å–ä¸€äº›è·¨è¡¨çš„å…³è”ç»Ÿè®¡
                # æœ‰å¤šå°‘filtered_postsæœ‰å¯¹åº”çš„pain_events
                cursor = conn.execute("""
                    SELECT COUNT(DISTINCT p.id) as count
                    FROM filtered_posts p
                    JOIN pain_events pe ON p.id = pe.post_id
                """)
                stats["filtered_with_pain_events"] = cursor.fetchone()["count"]

                # å¹³å‡æ¯ä¸ªclusteræœ‰å¤šå°‘opportunities
                cursor = conn.execute("""
                    SELECT AVG(opp_count) as avg_opportunities
                    FROM (
                        SELECT COUNT(o.id) as opp_count
                        FROM clusters c
                        LEFT JOIN opportunities o ON c.id = o.cluster_id
                        GROUP BY c.id
                    )
                """)
                result = cursor.fetchone()
                stats["avg_opportunities_per_cluster"] = result["avg_opportunities"] or 0

        except Exception as e:
            logger.error(f"Failed to get cross-table stats: {e}")

        return stats


# å…¨å±€æ•°æ®åº“å®ä¾‹ï¼ˆä½¿ç”¨ç»Ÿä¸€æ•°æ®åº“ï¼‰
db = WiseCollectionDB(unified=True)

# ä¿æŒå‘åå…¼å®¹çš„å¤šæ•°æ®åº“å®ä¾‹
db_multi = WiseCollectionDB(unified=False)
```


================================================================================
æ–‡ä»¶: utils/embedding.py
================================================================================

```python
"""
Embedding utilities for Reddit Pain Point Finder
å‘é‡åŒ–å·¥å…·ï¼Œç”¨äºç—›ç‚¹äº‹ä»¶èšç±»
"""
import os
import logging
import pickle
import numpy as np
from typing import List, Dict, Any, Optional, Tuple
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.cluster import DBSCAN
import yaml
from openai import OpenAI
import backoff

logger = logging.getLogger(__name__)

class EmbeddingClient:
    """åµŒå…¥å‘é‡å®¢æˆ·ç«¯"""

    def __init__(self, config_path: str = "config/llm.yaml"):
        """åˆå§‹åŒ–åµŒå…¥å®¢æˆ·ç«¯"""
        self.config = self._load_config(config_path)
        self.client = self._init_client()
        self.model_name = self._get_model_name()
        self.embedding_cache = {}
        self.stats = {
            "embeddings_created": 0,
            "cache_hits": 0,
            "total_tokens": 0
        }

    def _load_config(self, config_path: str) -> Dict[str, Any]:
        """åŠ è½½é…ç½®"""
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                return yaml.safe_load(f)
        except Exception as e:
            logger.error(f"Failed to load config from {config_path}: {e}")
            raise

    def _init_client(self) -> OpenAI:
        """åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯"""
        api_key = os.getenv(self.config['api']['api_key_env'])
        if not api_key:
            raise ValueError(f"API key not found: {self.config['api']['api_key_env']}")

        return OpenAI(
            api_key=api_key,
            base_url=self.config['api']['base_url']
        )

    def _get_model_name(self) -> str:
        """è·å–åµŒå…¥æ¨¡å‹åç§°"""
        embedding_config = self.config.get("embedding", {})
        env_name = embedding_config.get("env_name")
        if env_name and os.getenv(env_name):
            return os.getenv(env_name)
        return embedding_config.get("model", "text-embedding-ada-002")

    @backoff.on_exception(
        backoff.expo,
        Exception,
        max_tries=3,
        base=1,
        max_value=60
    )
    def create_embedding(self, text: str) -> List[float]:
        """åˆ›å»ºæ–‡æœ¬åµŒå…¥å‘é‡"""
        try:
            # æ£€æŸ¥ç¼“å­˜
            if text in self.embedding_cache:
                self.stats["cache_hits"] += 1
                return self.embedding_cache[text]

            # è°ƒç”¨API
            response = self.client.embeddings.create(
                model=self.model_name,
                input=text
            )

            embedding = response.data[0].embedding

            # æ›´æ–°ç»Ÿè®¡
            self.stats["embeddings_created"] += 1
            self.stats["total_tokens"] += response.usage.total_tokens

            # ç¼“å­˜ç»“æœ
            self.embedding_cache[text] = embedding

            logger.info(f"Created embedding for text length {len(text)}: {len(embedding)} dimensions")
            return embedding

        except Exception as e:
            logger.error(f"Failed to create embedding: {e}")
            raise

    def create_batch_embeddings(self, texts: List[str], batch_size: int = None) -> List[List[float]]:
        """æ‰¹é‡åˆ›å»ºåµŒå…¥å‘é‡"""
        if batch_size is None:
            batch_size = self.config.get("embedding", {}).get("batch_size", 32)

        embeddings = []
        for i in range(0, len(texts), batch_size):
            batch = texts[i:i + batch_size]
            logger.info(f"Processing batch {i//batch_size + 1}/{(len(texts) + batch_size - 1)//batch_size}")

            for text in batch:
                embedding = self.create_embedding(text)
                embeddings.append(embedding)

        return embeddings

    def create_pain_event_embedding(self, pain_event: Dict[str, Any]) -> List[float]:
        """ä¸ºç—›ç‚¹äº‹ä»¶åˆ›å»ºåµŒå…¥å‘é‡"""
        # æ„å»ºåµŒå…¥æ–‡æœ¬ï¼Œé‡ç‚¹å…³æ³¨é—®é¢˜çš„æœ¬è´¨
        text_parts = []

        if pain_event.get("actor"):
            text_parts.append(pain_event["actor"])

        if pain_event.get("context"):
            text_parts.append(pain_event["context"])

        if pain_event.get("problem"):
            text_parts.append(pain_event["problem"])

        if pain_event.get("current_workaround"):
            text_parts.append(pain_event["current_workaround"])

        # ç”¨ " | " è¿æ¥å„ä¸ªéƒ¨åˆ†ï¼Œä¿æŒè¯­ä¹‰ç»“æ„
        embedding_text = " | ".join(text_parts)

        return self.create_embedding(embedding_text)

    def calculate_similarity_matrix(self, embeddings: List[List[float]]) -> np.ndarray:
        """è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ"""
        return cosine_similarity(embeddings)

    def find_similar_events(
        self,
        target_embedding: List[float],
        candidate_embeddings: List[List[float]],
        threshold: float = 0.7,
        top_k: int = 10
    ) -> List[Tuple[int, float]]:
        """æ‰¾åˆ°ç›¸ä¼¼çš„ç—›ç‚¹äº‹ä»¶"""
        similarities = cosine_similarity([target_embedding], candidate_embeddings)[0]

        # ç­›é€‰è¶…è¿‡é˜ˆå€¼çš„ç»“æœ
        results = []
        for idx, similarity in enumerate(similarities):
            if similarity >= threshold:
                results.append((idx, similarity))

        # æŒ‰ç›¸ä¼¼åº¦æ’åºï¼Œè¿”å›top_k
        results.sort(key=lambda x: x[1], reverse=True)
        return results[:top_k]

    def cluster_embeddings(
        self,
        embeddings: List[List[float]],
        eps: float = 0.5,
        min_samples: int = 3
    ) -> Dict[int, List[int]]:
        """ä½¿ç”¨DBSCANèšç±»åµŒå…¥å‘é‡"""
        if len(embeddings) < min_samples:
            return {0: list(range(len(embeddings)))}  # å¦‚æœæ ·æœ¬å¤ªå°‘ï¼Œå½’ä¸ºä¸€ç±»

        dbscan = DBSCAN(eps=eps, min_samples=min_samples, metric='cosine')
        cluster_labels = dbscan.fit_predict(embeddings)

        # æ„å»ºèšç±»å­—å…¸
        clusters = {}
        for idx, label in enumerate(cluster_labels):
            if label == -1:  # å™ªå£°ç‚¹
                continue
            if label not in clusters:
                clusters[label] = []
            clusters[label].append(idx)

        return clusters

    def analyze_cluster(
        self,
        cluster_indices: List[int],
        embeddings: List[List[float]],
        pain_events: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """åˆ†æä¸€ä¸ªèšç±»"""
        if not cluster_indices:
            return {}

        # è®¡ç®—èšç±»ä¸­å¿ƒ
        cluster_embeddings = [embeddings[i] for i in cluster_indices]
        centroid = np.mean(cluster_embeddings, axis=0)

        # è®¡ç®—æ¯ä¸ªç‚¹åˆ°ä¸­å¿ƒçš„è·ç¦»
        distances_to_center = [
            1 - cosine_similarity([embeddings[i]], [centroid])[0][0]
            for i in cluster_indices
        ]

        # è®¡ç®—èšç±»çš„å†…èšæ€§ï¼ˆå¹³å‡è·ç¦»ï¼‰
        cohesion = 1 - np.mean(distances_to_center)

        # è·å–è¯¥èšç±»çš„ç—›ç‚¹äº‹ä»¶
        cluster_events = [pain_events[i] for i in cluster_indices]

        return {
            "size": len(cluster_indices),
            "centroid": centroid.tolist(),
            "cohesion": cohesion,
            "events": cluster_events,
            "avg_distance_to_center": np.mean(distances_to_center),
            "max_distance_to_center": np.max(distances_to_center)
        }

    def get_embedding_statistics(self) -> Dict[str, Any]:
        """è·å–åµŒå…¥ç»Ÿè®¡ä¿¡æ¯"""
        return self.stats.copy()

    def save_embedding_cache(self, cache_path: str):
        """ä¿å­˜åµŒå…¥ç¼“å­˜"""
        try:
            with open(cache_path, 'wb') as f:
                pickle.dump(self.embedding_cache, f)
            logger.info(f"Saved embedding cache to {cache_path}")
        except Exception as e:
            logger.error(f"Failed to save embedding cache: {e}")

    def load_embedding_cache(self, cache_path: str):
        """åŠ è½½åµŒå…¥ç¼“å­˜"""
        try:
            if os.path.exists(cache_path):
                with open(cache_path, 'rb') as f:
                    self.embedding_cache = pickle.load(f)
                logger.info(f"Loaded embedding cache from {cache_path}: {len(self.embedding_cache)} entries")
        except Exception as e:
            logger.error(f"Failed to load embedding cache: {e}")

class PainEventClustering:
    """ç—›ç‚¹äº‹ä»¶èšç±»å·¥å…·"""

    def __init__(self, embedding_client: EmbeddingClient):
        """åˆå§‹åŒ–èšç±»å·¥å…·"""
        self.embedding_client = embedding_client
        self.clustering_config = self._load_clustering_config()

    def _load_clustering_config(self) -> Dict[str, Any]:
        """åŠ è½½èšç±»é…ç½®"""
        try:
            config_path = "config/clustering.yaml"
            with open(config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
            return config
        except Exception as e:
            logger.error(f"Failed to load clustering config: {e}")
            # è¿”å›é»˜è®¤é…ç½®
            return {
                "vector_similarity": {"similarity_threshold": 0.8, "top_k": 10},
                "dbscan": {"eps": 0.3, "min_samples": 2},
                "llm_validation": {"max_events_per_validation": 10, "confidence_threshold": 0.7},
                "post_processing": {"min_cluster_size": 2, "max_cluster_size": 15}
            }

    def cluster_pain_events(self, pain_events: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """èšç±»ç—›ç‚¹äº‹ä»¶"""
        if len(pain_events) < 2:
            return []

        logger.info(f"Clustering {len(pain_events)} pain events")

        # 1. åˆ›å»ºåµŒå…¥å‘é‡
        logger.info("Creating embeddings for pain events...")
        embeddings = []
        for event in pain_events:
            embedding = self.embedding_client.create_pain_event_embedding(event)
            embeddings.append(embedding)

        # 2. ä½¿ç”¨å‘é‡ç›¸ä¼¼åº¦è¿›è¡Œåˆæ­¥èšç±»
        logger.info("Performing vector similarity clustering...")
        similarity_threshold = self.clustering_config.get(
            "vector_similarity", {}
        ).get("similarity_threshold", 0.7)

        dbscan_eps = self.clustering_config.get(
            "dbscan", {}
        ).get("eps", 0.5)
        min_samples = self.clustering_config.get(
            "dbscan", {}
        ).get("min_samples", 3)

        # DBSCANèšç±»
        clusters = self.embedding_client.cluster_embeddings(
            embeddings, eps=dbscan_eps, min_samples=min_samples
        )

        # 3. åˆ†ææ¯ä¸ªèšç±»
        logger.info(f"Found {len(clusters)} clusters")
        cluster_results = []

        for cluster_id, indices in clusters.items():
            if len(indices) < 2:  # è·³è¿‡å•ä¸ªäº‹ä»¶çš„èšç±»
                continue

            cluster_analysis = self.embedding_client.analyze_cluster(
                indices, embeddings, pain_events
            )

            cluster_result = {
                "cluster_id": cluster_id,
                "pain_event_ids": indices,
                "cluster_size": len(indices),
                "cohesion": cluster_analysis["cohesion"],
                "events": cluster_analysis["events"]
            }

            cluster_results.append(cluster_result)

        # æŒ‰èšç±»å¤§å°æ’åº
        cluster_results.sort(key=lambda x: x["cluster_size"], reverse=True)

        logger.info(f"Successfully created {len(cluster_results)} clusters")
        return cluster_results

    def find_similar_events(
        self,
        target_event: Dict[str, Any],
        candidate_events: List[Dict[str, Any]],
        threshold: float = None,
        top_k: int = 10
    ) -> List[Dict[str, Any]]:
        """æ‰¾åˆ°ä¸ç›®æ ‡äº‹ä»¶ç›¸ä¼¼çš„å€™é€‰äº‹ä»¶"""
        if threshold is None:
            threshold = self.clustering_config.get(
                "vector_similarity", {}
            ).get("similarity_threshold", 0.7)

        # åˆ›å»ºç›®æ ‡äº‹ä»¶çš„åµŒå…¥
        target_embedding = self.embedding_client.create_pain_event_embedding(target_event)

        # åˆ›å»ºå€™é€‰äº‹ä»¶çš„åµŒå…¥
        candidate_embeddings = []
        for event in candidate_events:
            embedding = self.embedding_client.create_pain_event_embedding(event)
            candidate_embeddings.append(embedding)

        # æ‰¾åˆ°ç›¸ä¼¼äº‹ä»¶
        similar_indices = self.embedding_client.find_similar_events(
            target_embedding, candidate_embeddings, threshold, top_k
        )

        # è¿”å›ç›¸ä¼¼äº‹ä»¶åŠå…¶ç›¸ä¼¼åº¦
        results = []
        for idx, similarity in similar_indices:
            result = candidate_events[idx].copy()
            result["similarity_score"] = similarity
            results.append(result)

        return results

# å…¨å±€åµŒå…¥å®¢æˆ·ç«¯å®ä¾‹
embedding_client = EmbeddingClient()
pain_clustering = PainEventClustering(embedding_client)
```


================================================================================
æ–‡ä»¶: utils/llm_client.py
================================================================================

```python
"""
LLM Client for Reddit Pain Point Finder
åŸºäºSiliconFlow APIçš„LLMå®¢æˆ·ç«¯
"""
import os
import json
import logging
import time
from typing import Dict, List, Any, Optional, Union
import yaml
from openai import OpenAI
import backoff
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

logger = logging.getLogger(__name__)

class LLMClient:
    """SiliconFlow LLMå®¢æˆ·ç«¯"""

    def __init__(self, config_path: str = "config/llm.yaml"):
        """åˆå§‹åŒ–LLMå®¢æˆ·ç«¯"""
        self.config = self._load_config(config_path)
        self.client = self._init_client()
        self.stats = {
            "requests": 0,
            "tokens_used": 0,
            "cost": 0.0,
            "errors": 0
        }

    def _load_config(self, config_path: str) -> Dict[str, Any]:
        """åŠ è½½LLMé…ç½®"""
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                return yaml.safe_load(f)
        except Exception as e:
            logger.error(f"Failed to load LLM config from {config_path}: {e}")
            raise

    def _init_client(self) -> OpenAI:
        """åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯"""
        api_key = os.getenv(self.config['api']['api_key_env'])
        if not api_key:
            raise ValueError(f"API key not found in environment variable: {self.config['api']['api_key_env']}")

        return OpenAI(
            api_key=api_key,
            base_url=self.config['api']['base_url']
        )

    def get_model_name(self, model_type: str = "main") -> str:
        """è·å–æŒ‡å®šç±»å‹çš„æ¨¡å‹åç§°"""
        if model_type in self.config.get("models", {}):
            model_config = self.config["models"][model_type]
            # å¦‚æœæœ‰ç¯å¢ƒå˜é‡é…ç½®ï¼Œä¼˜å…ˆä½¿ç”¨
            env_name = model_config.get("env_name")
            if env_name and os.getenv(env_name):
                return os.getenv(env_name)
            return model_config["name"]

        # ä»task_mappingä¸­æŸ¥æ‰¾
        task_mapping = self.config.get("task_mapping", {})
        if model_type in task_mapping:
            mapped_model = task_mapping[model_type]["model"]
            return self.get_model_name(mapped_model)

        # é»˜è®¤è¿”å›mainæ¨¡å‹
        return self.config["models"]["main"]["name"]

    def get_model_config(self, model_type: str = "main") -> Dict[str, Any]:
        """è·å–æ¨¡å‹é…ç½®"""
        # ä»task_mappingä¸­æŸ¥æ‰¾
        task_mapping = self.config.get("task_mapping", {})
        if model_type in task_mapping:
            mapped_model = task_mapping[model_type]["model"]
            base_config = self.config["models"][mapped_model].copy()
            # è¦†ç›–ä»»åŠ¡ç‰¹å®šé…ç½®
            base_config.update(task_mapping[model_type])
            return base_config

        # ç›´æ¥ä»modelsä¸­æŸ¥æ‰¾
        if model_type in self.config.get("models", {}):
            return self.config["models"][model_type].copy()

        # é»˜è®¤è¿”å›mainæ¨¡å‹é…ç½®
        return self.config["models"]["main"].copy()

    def chat_completion(
        self,
        messages: List[Dict[str, str]],
        model_type: str = "main",
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
        json_mode: bool = False
    ) -> Dict[str, Any]:
        """èŠå¤©è¡¥å…¨è¯·æ±‚"""

        max_retries = 5
        base_delay = 1
        max_delay = 120

        for attempt in range(max_retries):
            try:
                model_config = self.get_model_config(model_type)
                model_name = self.get_model_name(model_type)

                # å‚æ•°é…ç½®
                params = {
                    "model": model_name,
                    "messages": messages,
                    "temperature": temperature if temperature is not None else model_config.get("temperature", 0.1),
                    "max_tokens": max_tokens if max_tokens is not None else model_config.get("max_tokens", 2000),
                    "timeout": model_config.get("timeout", 180)
                }

                # JSONæ¨¡å¼
                if json_mode:
                    params["response_format"] = {"type": "json_object"}

                # è®°å½•è¯·æ±‚å¼€å§‹æ—¶é—´
                start_time = time.time()

                logger.info(f"LLM Request {attempt + 1}/{max_retries}: model={model_name}, timeout={params['timeout']}s")

                # å‘é€è¯·æ±‚
                response = self.client.chat.completions.create(**params)

                # è®¡ç®—è¯·æ±‚æ—¶é—´
                request_time = time.time() - start_time

                # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
                self.stats["requests"] += 1
                if hasattr(response.usage, 'total_tokens'):
                    self.stats["tokens_used"] += response.usage.total_tokens

                # æå–å“åº”å†…å®¹
                content = response.choices[0].message.content

                # å¦‚æœæ˜¯JSONæ¨¡å¼ï¼Œå°è¯•è§£æ
                if json_mode:
                    try:
                        content = json.loads(content)
                    except json.JSONDecodeError as e:
                        logger.error(f"Failed to parse JSON response: {e}")
                        logger.error(f"Raw content: {content}")
                        # å°è¯•ä¿®å¤JSON
                        content = self._try_fix_json(content)

                result = {
                    "content": content,
                    "model": model_name,
                    "usage": {
                        "prompt_tokens": response.usage.prompt_tokens if response.usage else 0,
                        "completion_tokens": response.usage.completion_tokens if response.usage else 0,
                        "total_tokens": response.usage.total_tokens if response.usage else 0
                    },
                    "request_time": request_time
                }

                logger.info(f"âœ… LLM request {attempt + 1}/{max_retries} completed: {result['usage']['total_tokens']} tokens in {request_time:.2f}s")
                return result

            except Exception as e:
                error_msg = f"âŒ LLM request {attempt + 1}/{max_retries} failed: {e}"
                self.stats["errors"] += 1

                if attempt < max_retries - 1:
                    # è®¡ç®—é€€é¿å»¶è¿Ÿ
                    delay = min(base_delay * (2 ** attempt), max_delay)
                    logger.warning(f"{error_msg} - Retrying in {delay:.2f}s...")
                    time.sleep(delay)
                    continue
                else:
                    logger.error(f"{error_msg} - Max retries exceeded")
                    raise

    def _try_fix_json(self, content: str) -> Dict[str, Any]:
        """å°è¯•ä¿®å¤æŸåçš„JSON"""
        try:
            # å°è¯•æå–JSONéƒ¨åˆ†
            start_idx = content.find('{')
            end_idx = content.rfind('}') + 1
            if start_idx != -1 and end_idx > start_idx:
                json_str = content[start_idx:end_idx]
                return json.loads(json_str)
            else:
                raise ValueError("No JSON found in response")
        except Exception as e:
            logger.error(f"Failed to fix JSON: {e}")
            return {"error": "Failed to parse JSON", "raw_content": content}

    def extract_pain_points(
        self,
        title: str,
        body: str,
        subreddit: str,
        upvotes: int,
        comments_count: int
    ) -> Dict[str, Any]:
        """ä»Redditå¸–å­ä¸­æå–ç—›ç‚¹"""
        prompt = self._get_pain_extraction_prompt()

        messages = [
            {"role": "system", "content": prompt},
            {"role": "user", "content": f"""
Title: {title}
Body: {body}
Subreddit: {subreddit}
Upvotes: {upvotes}
Comments: {comments_count}
"""}
        ]

        return self.chat_completion(
            messages=messages,
            model_type="pain_extraction",
            json_mode=True
        )

    def cluster_pain_events(
        self,
        pain_events: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """èšç±»ç—›ç‚¹äº‹ä»¶"""
        prompt = self._get_workflow_clustering_prompt()

        # æ„å»ºç—›ç‚¹äº‹ä»¶æ–‡æœ¬
        events_text = "\n\n".join([
            f"Event {i+1}: {event.get('problem', '')} (Context: {event.get('context', '')}, Workaround: {event.get('current_workaround', '')})"
            for i, event in enumerate(pain_events)
        ])

        messages = [
            {"role": "system", "content": prompt},
            {"role": "user", "content": f"Pain events:\n{events_text}"}
        ]

        return self.chat_completion(
            messages=messages,
            model_type="clustering",
            json_mode=True
        )

    def summarize_source_cluster(
        self,
        pain_events: List[Dict[str, Any]],
        source_type: str
    ) -> Dict[str, Any]:
        """ä¸ºåŒä¸€sourceçš„èšç±»ç”Ÿæˆæ‘˜è¦"""
        prompt = self._get_cluster_summarizer_prompt()

        # æ„å»ºç—›ç‚¹äº‹ä»¶æ–‡æœ¬ï¼Œé‡ç‚¹å…³æ³¨é—®é¢˜å’Œä¸Šä¸‹æ–‡
        events_text = "\n\n".join([
            f"Event {i+1}:\nProblem: {event.get('problem', '')}\nContext: {event.get('context', '')}\nWorkaround: {event.get('current_workaround', '')}\n"
            for i, event in enumerate(pain_events[:10])  # æœ€å¤šå¤„ç†10ä¸ªäº‹ä»¶ä»¥æ§åˆ¶tokené•¿åº¦
        ])

        messages = [
            {"role": "system", "content": prompt},
            {"role": "user", "content": f"Source Type: {source_type}\n\nPain Events:\n{events_text}"}
        ]

        return self.chat_completion(
            messages=messages,
            model_type="cluster_summarizer",
            json_mode=True
        )

    def map_opportunity(
        self,
        cluster_summary: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ä»ç—›ç‚¹èšç±»æ˜ å°„æœºä¼š"""
        prompt = self._get_opportunity_mapping_prompt()

        messages = [
            {"role": "system", "content": prompt},
            {"role": "user", "content": f"Pain cluster:\n{json.dumps(cluster_summary, indent=2)}"}
        ]

        return self.chat_completion(
            messages=messages,
            model_type="opportunity_mapping",
            json_mode=True
        )

    def score_viability(
        self,
        opportunity_description: str
    ) -> Dict[str, Any]:
        """è¯„ä¼°æœºä¼šå¯è¡Œæ€§"""
        prompt = self._get_viability_scoring_prompt()

        messages = [
            {"role": "system", "content": prompt},
            {"role": "user", "content": f"Idea:\n{opportunity_description}"}
        ]

        return self.chat_completion(
            messages=messages,
            model_type="viability_scoring",
            json_mode=True
        )

    def validate_pain_signal(
        self,
        text: str
    ) -> Dict[str, Any]:
        """éªŒè¯ç—›ç‚¹ä¿¡å·"""
        prompt = self._get_signal_validation_prompt()

        messages = [
            {"role": "system", "content": prompt},
            {"role": "user", "content": text}
        ]

        return self.chat_completion(
            messages=messages,
            model_type="signal_validation",
            json_mode=True
        )

    def _get_pain_extraction_prompt(self) -> str:
        """è·å–ç—›ç‚¹æŠ½å–æç¤º"""
        return """You are an information extraction engine.

Your task:
From the following Reddit post, extract concrete PAIN EVENTS.
A pain event is a specific recurring problem experienced by the author,
not opinions, not general complaints.

Rules:
- Do NOT summarize the post
- Do NOT give advice
- If no concrete pain exists, return an empty list
- Be literal and conservative
- Focus on actionable problems people face repeatedly

Output JSON only with this format:
{
  "pain_events": [
    {
      "actor": "who experiences the problem",
      "context": "what they are trying to do",
      "problem": "the concrete difficulty",
      "current_workaround": "how they currently cope (if any)",
      "frequency": "how often it happens (explicit or inferred)",
      "emotional_signal": "frustration, anxiety, exhaustion, etc.",
      "mentioned_tools": ["tool1", "tool2"],
      "confidence": 0.8
    }
  ],
  "extraction_summary": "brief summary of findings"
}

Fields explanation:
- actor: who has this problem (developer, manager, user, etc.)
- context: the situation or workflow where the problem occurs
- problem: specific, concrete issue (not "things are slow" but "compilation takes 30 minutes")
- current_workaround: current solutions people use (if mentioned)
- frequency: how often this happens (daily, weekly, occasionally, etc.)
- emotional_signal: the emotion expressed (frustration, anger, disappointment, etc.)
- mentioned_tools: tools, software, or methods explicitly mentioned
- confidence: how confident you are this is a real pain point (0-1)"""

    def _get_workflow_clustering_prompt(self) -> str:
        """è·å–å·¥ä½œæµèšç±»æç¤º"""
        return """You are analyzing user pain events.

Given the following pain events, determine whether they belong to THE SAME UNDERLYING WORKFLOW problem.

A workflow means:
- the same repeated activity
- where different people fail in similar ways
- with similar root causes

If they belong to the same workflow:
- give the workflow a short descriptive name
- provide a brief description of the workflow
- estimate confidence (0-1)

If they should NOT be clustered:
- say they should not be clustered
- explain why briefly

Return JSON only with this format:
{
  "same_workflow": true/false,
  "workflow_name": "name if same workflow",
  "workflow_description": "description if same workflow",
  "confidence": 0.8,
  "reasoning": "brief explanation"
}

Be conservative - only cluster if they're clearly the same workflow."""

    def _get_opportunity_mapping_prompt(self) -> str:
        """è·å–æœºä¼šæ˜ å°„æç¤º"""
        return """You are a brutally practical product thinker for solo founders.

Given a cluster of pain events that belong to the same workflow:

1. Identify what tools people CURRENTLY use to survive this problem
2. Identify what capability is missing
3. Explain why existing tools fail (too heavy, too generic, etc.)
4. Propose ONE narrow micro-tool opportunity

Rules:
- No platforms (unless you can justify the MVP)
- No marketplaces
- Assume a solo founder building an MVP in 1-3 months
- Focus on specific, painful problems with clear solutions
- If no viable tool opportunity exists, say so

Return JSON only with this format:
{
  "current_tools": ["tool1", "tool2", "manual methods"],
  "missing_capability": "what's missing that would solve this",
  "why_existing_fail": "why current solutions don't work well",
  "opportunity": {
    "name": "short descriptive name",
    "description": "what the micro-tool does",
    "target_users": "who would use this",
    "pain_frequency": "how often this pain occurs (1-10)",
    "market_size": "how many people have this problem (1-10)",
    "mvp_complexity": "how hard to build MVP (1-10, lower is better)",
    "competition_risk": "risk of competitors (1-10, lower is better)",
    "integration_complexity": "how hard to integrate (1-10, lower is better)"
  }
}

Focus on narrow, specific problems that a solo founder can actually solve."""

    def _get_viability_scoring_prompt(self) -> str:
        """è·å–å¯è¡Œæ€§è¯„åˆ†æç¤º"""
        return """You are an experienced solo-founder investor.

Score the following idea for a ONE-PERSON COMPANY.

Criteria:
- Pain frequency: How often does this pain occur? (daily=10, rarely=1)
- Clear buyer: Can we easily identify who would pay? (clear=10, vague=1)
- MVP buildable: Can one person build MVP in 1-3 months? (easy=10, hard=1)
- Crowded market: How competitive is this space? (empty=10, saturated=1)
- Integration: How easy to integrate with existing tools? (easy=10, hard=1)

Score each criteria 0-10, then calculate total score.

Also list the TOP 3 killer risks that could kill this project.

Return JSON only with this format:
{
  "scores": {
    "pain_frequency": 8,
    "clear_buyer": 7,
    "mvp_buildable": 6,
    "crowded_market": 5,
    "integration": 7
  },
  "total_score": 6.6,
  "killer_risks": [
    "Risk 1: specific and concrete",
    "Risk 2: specific and concrete",
    "Risk 3: specific and concrete"
  ],
  "recommendation": "pursue/modify/abandon with brief reason"
}

Be realistic and conservative in scoring."""

    def _get_cluster_summarizer_prompt(self) -> str:
        """è·å–èšç±»æ‘˜è¦æç¤º"""
        return """You are a cluster summarizer for pain events.

These pain events come from the same source and discourse style.
Your task is to summarize the SHARED UNDERLYING PROBLEM, ignoring emotional tone and individual details.

Focus on:
1. What is the common problem across all these events?
2. What shared context or workflow is involved?
3. What is the essential pain point, stripped of emotional language?
4. Provide 2-3 representative examples that capture the essence

BE CONSERVATIVE - only identify patterns that truly exist across multiple events.

Return JSON only with this format:
{
  "centroid_summary": "brief summary of the core shared problem",
  "common_pain": "the main difficulty or challenge (technical language)",
  "common_context": "the shared workflow or situation where this occurs",
  "example_events": [
    "Event 1: representative problem description",
    "Event 2: representative problem description"
  ],
  "coherence_score": 0.8,  # how well do these events belong together (0-1)
  "reasoning": "brief explanation of why these belong together"
}

Do not exaggerate similarities. Be literal and precise."""

    def _get_signal_validation_prompt(self) -> str:
        """è·å–ä¿¡å·éªŒè¯æç¤º"""
        return """You are a pain signal validator.

Given this text, determine if it contains a genuine pain point.

A genuine pain point:
- Describes a specific problem or difficulty
- Shows frustration or struggle
- Is not just venting or seeking help
- Represents a recurring issue

Return JSON only with this format:
{
  "is_pain_point": true/false,
  "confidence": 0.8,
  "pain_type": "frustration/inefficiency/complexity/workflow/cost/other",
  "specificity": 0.9,  # How specific is the problem (0-1)
  "emotional_intensity": 0.7,  # How strong is the emotion (0-1)
  "keywords": ["struggling", "frustrated", "can't figure out"]
}

Be conservative - only flag clear pain points."""

    def get_statistics(self) -> Dict[str, Any]:
        """è·å–ä½¿ç”¨ç»Ÿè®¡"""
        return self.stats.copy()

    def reset_statistics(self):
        """é‡ç½®ç»Ÿè®¡"""
        self.stats = {
            "requests": 0,
            "tokens_used": 0,
            "cost": 0.0,
            "errors": 0
        }

# å…¨å±€LLMå®¢æˆ·ç«¯å®ä¾‹
llm_client = LLMClient()
```


================================================================================
æå–å®Œæˆ
================================================================================
æ€»å…±æå–äº† 15 ä¸ªæ–‡ä»¶
