#!/usr/bin/env python3
"""
Phase 3 Task 4: Opportunity Report Evaluator
è¯„ä¼°æœºä¼šåˆ†ææŠ¥å‘Šçš„è´¨é‡å’Œå®Œæ•´æ€§
"""
import sys
import os
import argparse
import json
import logging
import re
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Tuple

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class OpportunityReportEvaluator:
    """æœºä¼šæŠ¥å‘Šè¯„ä¼°å™¨"""

    def __init__(self, reports_dir: str):
        """åˆå§‹åŒ–è¯„ä¼°å™¨"""
        self.reports_dir = Path(reports_dir)
        logger.info(f"æŠ¥å‘Šç›®å½•: {self.reports_dir}")

    def load_reports(self) -> List[Path]:
        """åŠ è½½æ‰€æœ‰markdownæŠ¥å‘Š"""
        logger.info("æ‰«ææŠ¥å‘Šæ–‡ä»¶...")
        md_files = list(self.reports_dir.glob("*.md"))
        # æ’é™¤README.md
        md_files = [f for f in md_files if f.name != "README.md"]
        logger.info(f"æ‰¾åˆ° {len(md_files)} ä¸ªæŠ¥å‘Šæ–‡ä»¶")
        return md_files

    def evaluate_report(self, filepath: Path) -> Dict[str, Any]:
        """è¯„ä¼°å•ä¸ªæŠ¥å‘Š"""
        logger.debug(f"è¯„ä¼°æŠ¥å‘Š: {filepath.name}")

        with open(filepath, 'r', encoding='utf-8') as f:
            content = f.read()

        report_name = filepath.stem
        evaluation = {
            'file': filepath.name,
            'report_name': report_name,
            'metrics': {}
        }

        # 1. æ£€æŸ¥è¯„è®ºè¯æ® (comment|è¯„è®º|evidence.*comment)
        comment_patterns = [
            r'è¯„è®º|comment',
            r'evidence.*comment',
            r'comment.*evidence',
            r'åé¦ˆ|feedback',
            r'ç”¨æˆ·åé¦ˆ|user feedback'
        ]
        evaluation['metrics']['has_comment_evidence'] = any(
            re.search(pattern, content, re.IGNORECASE)
            for pattern in comment_patterns
        )

        # 2. æ£€æŸ¥é—®é¢˜æè¿° (**é—®é¢˜**)
        problem_patterns = [
            r'\*\*é—®é¢˜\*\*',
            r'é—®é¢˜[:ï¼š]',
            r'æ ¸å¿ƒé—®é¢˜',
            r'ä¸»è¦é—®é¢˜',
            r'é—®é¢˜åˆ†æ',
            r'problem analysis',
            r'key problem'
        ]
        problem_matches = []
        for pattern in problem_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            problem_matches.extend(matches)

        evaluation['metrics']['has_problem_description'] = len(problem_matches) > 0
        evaluation['metrics']['problem_description_count'] = len(problem_matches)

        # 3. æ£€æŸ¥MVPå»ºè®® (MVP|mvp|æœ€å°å¯è¡Œäº§å“|feature.*suggest)
        mvp_patterns = [
            r'\bMVP\b',
            r'mvp',
            r'æœ€å°å¯è¡Œäº§å“',
            r'feature.*suggest',
            r'suggest.*feature',
            r'åŠŸèƒ½å»ºè®®',
            r'MVPåŠŸèƒ½',
            r'æ ¸å¿ƒåŠŸèƒ½'
        ]
        evaluation['metrics']['has_mvp_suggestions'] = any(
            re.search(pattern, content, re.IGNORECASE)
            for pattern in mvp_patterns
        )

        # 4. æ£€æŸ¥ç›®æ ‡ç”¨æˆ· (ç›®æ ‡ç”¨æˆ·|target.*user|ç”¨æˆ·ç¾¤ä½“|user.*group)
        user_patterns = [
            r'ç›®æ ‡ç”¨æˆ·',
            r'target.*user',
            r'ç”¨æˆ·ç¾¤ä½“',
            r'user.*group',
            r'ç›®æ ‡å—ä¼—',
            r'ç›®æ ‡å®¢æˆ·',
            r'user profile',
            r'user persona'
        ]
        evaluation['metrics']['has_target_users'] = any(
            re.search(pattern, content, re.IGNORECASE)
            for pattern in user_patterns
        )

        # 5. æ£€æŸ¥é£é™©åˆ†æ (é£é™©|risk|æŒ‘æˆ˜|challenge|éšœç¢|barrier)
        risk_patterns = [
            r'é£é™©',
            r'\brisk\b',
            r'æŒ‘æˆ˜',
            r'\bchallenge\b',
            r'éšœç¢',
            r'\bbarrier\b',
            r'æ½œåœ¨é£é™©',
            r'ä¸»è¦é£é™©',
            r'é£é™©åº”å¯¹',
            r'killer risk'
        ]
        evaluation['metrics']['has_risk_analysis'] = any(
            re.search(pattern, content, re.IGNORECASE)
            for pattern in risk_patterns
        )

        # 6. æå–æœºä¼šæ•°é‡ (**<name>** (è¯„åˆ†:)
        opportunity_pattern = r'\*\*([^*]+)\*\*\s*\(è¯„åˆ†:\s*([\d.]+)\)'
        opportunity_matches = re.findall(opportunity_pattern, content)
        evaluation['metrics']['opportunity_count'] = len(opportunity_matches)

        # æå–æœºä¼šåç§°å’Œè¯„åˆ†
        evaluation['opportunities'] = [
            {'name': name.strip(), 'score': float(score)}
            for name, score in opportunity_matches
        ]

        # 7. æ–‡ä»¶å¤§å°å’Œå­—ç¬¦æ•°
        evaluation['metrics']['file_size_bytes'] = filepath.stat().st_size
        evaluation['metrics']['char_count'] = len(content)
        evaluation['metrics']['line_count'] = len(content.split('\n'))

        # 8. æ£€æŸ¥å…³é”®ç« èŠ‚
        section_patterns = {
            'has_overview_section': r'èšç±»æ¦‚è§ˆ|æ¦‚è¿°|overview',
            'has_analysis_section': r'æ·±åº¦åˆ†æ|è¯¦ç»†åˆ†æ|analysis',
            'has_design_section': r'äº§å“è®¾è®¡|æ–¹æ¡ˆè®¾è®¡|design',
            'has_action_section': r'è¡ŒåŠ¨è®¡åˆ’|å¯æ‰§è¡Œ|action',
            'has_data_section': r'åŸå§‹æ•°æ®|æ•°æ®|data'
        }

        for key, pattern in section_patterns.items():
            evaluation['metrics'][key] = bool(re.search(pattern, content, re.IGNORECASE))

        return evaluation

    def calculate_aggregated_metrics(self, evaluations: List[Dict]) -> Dict[str, Any]:
        """è®¡ç®—èšåˆæŒ‡æ ‡"""
        total_reports = len(evaluations)

        if total_reports == 0:
            return {}

        aggregated = {
            'total_reports': total_reports,
            'completeness_scores': {},
            'content_metrics': {},
            'section_coverage': {}
        }

        # å®Œæ•´æ€§è¯„åˆ†
        completeness_fields = [
            'has_comment_evidence',
            'has_problem_description',
            'has_mvp_suggestions',
            'has_target_users',
            'has_risk_analysis'
        ]

        for field in completeness_fields:
            count = sum(1 for e in evaluations if e['metrics'].get(field, False))
            aggregated['completeness_scores'][field] = {
                'count': count,
                'percentage': round(count / total_reports * 100, 2)
            }

        # å†…å®¹æŒ‡æ ‡
        aggregated['content_metrics'] = {
            'avg_file_size': round(sum(e['metrics']['file_size_bytes'] for e in evaluations) / total_reports, 2),
            'avg_char_count': round(sum(e['metrics']['char_count'] for e in evaluations) / total_reports, 2),
            'avg_line_count': round(sum(e['metrics']['line_count'] for e in evaluations) / total_reports, 2),
            'total_opportunities': sum(e['metrics']['opportunity_count'] for e in evaluations),
            'avg_opportunities_per_report': round(sum(e['metrics']['opportunity_count'] for e in evaluations) / total_reports, 2)
        }

        # ç« èŠ‚è¦†ç›–ç‡
        section_fields = [
            'has_overview_section',
            'has_analysis_section',
            'has_design_section',
            'has_action_section',
            'has_data_section'
        ]

        for field in section_fields:
            count = sum(1 for e in evaluations if e['metrics'].get(field, False))
            aggregated['section_coverage'][field] = {
                'count': count,
                'percentage': round(count / total_reports * 100, 2)
            }

        return aggregated

    def generate_markdown_report(self, evaluations: List[Dict], aggregated: Dict) -> str:
        """ç”ŸæˆmarkdownæŠ¥å‘Š"""
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')

        report = f"""# æœºä¼šåˆ†ææŠ¥å‘Šè´¨é‡è¯„ä¼°

> **ç”Ÿæˆæ—¶é—´**: {timestamp}
> **è¯„ä¼°æŠ¥å‘Šæ•°**: {aggregated.get('total_reports', 0)}

---

## ğŸ“Š æ€»ä½“æ¦‚è§ˆ

æœ¬æ¬¡è¯„ä¼°å…±åˆ†æäº† {aggregated.get('total_reports', 0)} ä»½æœºä¼šåˆ†ææŠ¥å‘Šï¼Œä»å®Œæ•´æ€§ã€å†…å®¹è´¨é‡å’Œç« èŠ‚è¦†ç›–ä¸‰ä¸ªç»´åº¦è¿›è¡Œè¯„ä¼°ã€‚

---

## âœ… å®Œæ•´æ€§åˆ†æ

### å…³é”®è¦ç´ è¦†ç›–æƒ…å†µ

| è¦ç´  | æŠ¥å‘Šæ•° | è¦†ç›–ç‡ |
|------|--------|--------|
"""

        for field, stats in aggregated.get('completeness_scores', {}).items():
            field_name_cn = {
                'has_comment_evidence': 'è¯„è®ºè¯æ®',
                'has_problem_description': 'é—®é¢˜æè¿°',
                'has_mvp_suggestions': 'MVPå»ºè®®',
                'has_target_users': 'ç›®æ ‡ç”¨æˆ·',
                'has_risk_analysis': 'é£é™©åˆ†æ'
            }.get(field, field)

            report += f"| {field_name_cn} | {stats['count']} | {stats['percentage']}% |\n"

        report += f"""
---

## ğŸ“ˆ å†…å®¹è´¨é‡æŒ‡æ ‡

### æ–‡ä»¶è§„æ¨¡

| æŒ‡æ ‡ | æ•°å€¼ |
|------|------|
| å¹³å‡æ–‡ä»¶å¤§å° | {aggregated.get('content_metrics', {}).get('avg_file_size', 0):.0f} bytes |
| å¹³å‡å­—ç¬¦æ•° | {aggregated.get('content_metrics', {}).get('avg_char_count', 0):.0f} |
| å¹³å‡è¡Œæ•° | {aggregated.get('content_metrics', {}).get('avg_line_count', 0):.0f} |
| æœºä¼šæ€»æ•° | {aggregated.get('content_metrics', {}).get('total_opportunities', 0)} |
| å¹³å‡æ¯æŠ¥å‘Šæœºä¼šæ•° | {aggregated.get('content_metrics', {}).get('avg_opportunities_per_report', 0):.1f} |

---

## ğŸ“‹ ç« èŠ‚è¦†ç›–æƒ…å†µ

### å¿…å¤‡ç« èŠ‚å®Œæ•´æ€§

| ç« èŠ‚ | æŠ¥å‘Šæ•° | è¦†ç›–ç‡ |
|------|--------|--------|
"""

        for field, stats in aggregated.get('section_coverage', {}).items():
            field_name_cn = {
                'has_overview_section': 'èšç±»æ¦‚è§ˆ',
                'has_analysis_section': 'æ·±åº¦åˆ†æ',
                'has_design_section': 'äº§å“è®¾è®¡',
                'has_action_section': 'è¡ŒåŠ¨è®¡åˆ’',
                'has_data_section': 'åŸå§‹æ•°æ®'
            }.get(field, field)

            report += f"| {field_name_cn} | {stats['count']} | {stats['percentage']}% |\n"

        report += """
---

## ğŸ“„ è¯¦ç»†æŠ¥å‘Šåˆ—è¡¨

### å„æŠ¥å‘Šè¯„ä¼°ç»“æœ

"""

        for i, eval_result in enumerate(evaluations, 1):
            metrics = eval_result['metrics']

            completeness_count = sum([
                metrics.get('has_comment_evidence', False),
                metrics.get('has_problem_description', False),
                metrics.get('has_mvp_suggestions', False),
                metrics.get('has_target_users', False),
                metrics.get('has_risk_analysis', False)
            ])

            completeness_pct = round(completeness_count / 5 * 100, 1)

            report += f"""
#### {i}. {eval_result['report_name']}

**æ–‡ä»¶**: `{eval_result['file']}`
**å®Œæ•´æ€§**: {completeness_pct}% ({completeness_count}/5)
**æ–‡ä»¶å¤§å°**: {metrics['file_size_bytes']} bytes
**å­—ç¬¦æ•°**: {metrics['char_count']}
**è¡Œæ•°**: {metrics['line_count']}
**æœºä¼šæ•°**: {metrics['opportunity_count']}

**å…³é”®è¦ç´ **:
- è¯„è®ºè¯æ®: {'âœ…' if metrics.get('has_comment_evidence') else 'âŒ'}
- é—®é¢˜æè¿°: {'âœ…' if metrics.get('has_problem_description') else 'âŒ'}
- MVPå»ºè®®: {'âœ…' if metrics.get('has_mvp_suggestions') else 'âŒ'}
- ç›®æ ‡ç”¨æˆ·: {'âœ…' if metrics.get('has_target_users') else 'âŒ'}
- é£é™©åˆ†æ: {'âœ…' if metrics.get('has_risk_analysis') else 'âŒ'}

"""

            if eval_result.get('opportunities'):
                report += "**è¯†åˆ«çš„æœºä¼š**:\n"
                for opp in eval_result['opportunities'][:5]:
                    report += f"- {opp['name']} (è¯„åˆ†: {opp['score']:.2f})\n"
                report += "\n"

        report += """
---

## ğŸ’¡ æ”¹è¿›å»ºè®®

### çŸ­æœŸæ”¹è¿› (1-2å‘¨)

1. **æå‡è¯„è®ºè¯æ®è¦†ç›–ç‡**: åœ¨æŠ¥å‘Šä¸­å¢åŠ æ›´å¤šæ¥è‡ªç”¨æˆ·è¯„è®ºçš„ç›´æ¥å¼•ç”¨å’Œè¯æ®
2. **å®Œå–„é—®é¢˜æè¿°**: ç¡®ä¿æ¯ä»½æŠ¥å‘Šéƒ½æœ‰æ¸…æ™°ã€å…·ä½“çš„é—®é¢˜æè¿°
3. **å¼ºåŒ–MVPå»ºè®®**: ä¸ºæ¯ä¸ªæœºä¼šæä¾›æ›´å…·ä½“çš„MVPåŠŸèƒ½å»ºè®®

### ä¸­æœŸä¼˜åŒ– (1ä¸ªæœˆ)

1. **æ ‡å‡†åŒ–æŠ¥å‘Šç»“æ„**: ç¡®ä¿æ‰€æœ‰æŠ¥å‘ŠåŒ…å«å®Œæ•´çš„6ä¸ªç« èŠ‚
2. **å¢å¼ºé£é™©åˆ†æ**: æ·±å…¥åˆ†ææ¯ä¸ªæœºä¼šçš„æ½œåœ¨é£é™©å’Œåº”å¯¹æªæ–½
3. **ç»†åŒ–ç›®æ ‡ç”¨æˆ·**: æä¾›æ›´ç²¾ç¡®çš„ç”¨æˆ·ç”»åƒå’Œä½¿ç”¨åœºæ™¯

### é•¿æœŸæå‡ (æŒç»­)

1. **å»ºç«‹è´¨é‡åŸºå‡†**: è®¾å®šæœ€ä½è´¨é‡æ ‡å‡†ï¼Œæœªè¾¾æ ‡æŠ¥å‘Šéœ€é‡æ–°ç”Ÿæˆ
2. **è‡ªåŠ¨åŒ–æ£€æŸ¥**: å°†è¯„ä¼°æŒ‡æ ‡é›†æˆåˆ°ç”Ÿæˆæµç¨‹ä¸­ï¼Œå®æ—¶åé¦ˆ
3. **æŒç»­è¿­ä»£**: æ ¹æ®ç”¨æˆ·åé¦ˆå’Œå¸‚åœºéœ€æ±‚è°ƒæ•´åˆ†æç»´åº¦

---

*æœ¬æŠ¥å‘Šç”± Opportunity Report Evaluator è‡ªåŠ¨ç”Ÿæˆ*
"""

        return report

    def evaluate_all(self) -> Tuple[List[Dict], Dict[str, Any]]:
        """è¯„ä¼°æ‰€æœ‰æŠ¥å‘Š"""
        logger.info("å¼€å§‹è¯„ä¼°æ‰€æœ‰æŠ¥å‘Š...")

        report_files = self.load_reports()
        if not report_files:
            logger.warning("æœªæ‰¾åˆ°ä»»ä½•æŠ¥å‘Šæ–‡ä»¶")
            return [], {}

        evaluations = []
        for filepath in report_files:
            try:
                evaluation = self.evaluate_report(filepath)
                evaluations.append(evaluation)
                logger.debug(f"å®Œæˆè¯„ä¼°: {filepath.name}")
            except Exception as e:
                logger.error(f"è¯„ä¼°å¤±è´¥ {filepath.name}: {e}")

        aggregated = self.calculate_aggregated_metrics(evaluations)

        logger.info(f"è¯„ä¼°å®Œæˆ: {len(evaluations)} ä»½æŠ¥å‘Š")
        return evaluations, aggregated


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="Phase 3: è¯„ä¼°æœºä¼šåˆ†ææŠ¥å‘Šçš„è´¨é‡å’Œå®Œæ•´æ€§"
    )
    parser.add_argument(
        "--reports-dir",
        default="pain_analysis_reports",
        help="æŠ¥å‘Šç›®å½•è·¯å¾„ (é»˜è®¤: pain_analysis_reports)"
    )
    parser.add_argument(
        "--output",
        default="docs/reports/opportunity_report_evaluation.md",
        help="è¾“å‡ºæŠ¥å‘Šè·¯å¾„"
    )

    args = parser.parse_args()

    try:
        logger.info("=" * 60)
        logger.info("æœºä¼šæŠ¥å‘Šè¯„ä¼°å™¨å¯åŠ¨")
        logger.info(f"æŠ¥å‘Šç›®å½•: {args.reports_dir}")
        logger.info(f"è¾“å‡ºæ–‡ä»¶: {args.output}")
        logger.info("=" * 60)

        # åˆ›å»ºè¯„ä¼°å™¨
        evaluator = OpportunityReportEvaluator(args.reports_dir)

        # æ‰§è¡Œè¯„ä¼°
        evaluations, aggregated = evaluator.evaluate_all()

        if not evaluations:
            logger.warning("æœªæ‰¾åˆ°å¯è¯„ä¼°çš„æŠ¥å‘Š")
            return

        # ç”ŸæˆæŠ¥å‘Š
        logger.info("ç”ŸæˆmarkdownæŠ¥å‘Š...")
        report_content = evaluator.generate_markdown_report(evaluations, aggregated)

        # ä¿å­˜æŠ¥å‘Š
        output_path = Path(args.output)
        output_path.parent.mkdir(parents=True, exist_ok=True)

        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(report_content)

        logger.info(f"âœ… æŠ¥å‘Šå·²ä¿å­˜: {output_path}")

        # è¾“å‡ºæ‘˜è¦
        print(f"\nğŸ“Š è¯„ä¼°å®Œæˆ!")
        print(f"   â€¢ è¯„ä¼°æŠ¥å‘Šæ•°: {aggregated.get('total_reports', 0)}")
        print(f"   â€¢ å¹³å‡å®Œæ•´æ€§: è®¡ç®—ä¸­...")
        print(f"   â€¢ è¾“å‡ºæ–‡ä»¶: {output_path}")

        # åŒæ—¶ä¿å­˜JSONæ•°æ®
        json_path = output_path.with_suffix('.json')
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump({
                'evaluations': evaluations,
                'aggregated': aggregated,
                'timestamp': datetime.now().isoformat()
            }, f, indent=2, ensure_ascii=False)

        logger.info(f"âœ… JSONæ•°æ®å·²ä¿å­˜: {json_path}")

    except Exception as e:
        logger.error(f"è¯„ä¼°å¤±è´¥: {e}")
        import traceback
        logger.error(traceback.format_exc())
        sys.exit(1)


if __name__ == "__main__":
    main()
