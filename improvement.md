# Python é¡¹ç›® "wise_collection" æ”¹è¿›å»ºè®®

è¿™æ˜¯ä¸€ä¸ªå¯¹å½“å‰ Python é¡¹ç›®ä»£ç çš„åˆ†æä¸æ”¹è¿›å»ºè®®ã€‚è¯¥é¡¹ç›®å±•ç¤ºäº†å¼ºå¤§çš„æ•°æ®å¤„ç†å’Œ AI é›†æˆèƒ½åŠ›ï¼Œä»¥ä¸‹å»ºè®®æ—¨åœ¨ä½¿å…¶æ›´åŠ å¥å£®ã€å¯ç»´æŠ¤å’Œé«˜æ•ˆã€‚

## 1. æ•´ä½“é¡¹ç›®çº§æ”¹è¿›

è¿™äº›å»ºè®®é€‚ç”¨äºé¡¹ç›®ä¸­çš„æ‰€æœ‰è„šæœ¬ã€‚

### 1.1. ç»Ÿä¸€é…ç½®ç®¡ç†

**é—®é¢˜**: `analyze_topics.py`, `curate.py`, `reddit_collection.py` ç­‰å¤šä¸ªæ–‡ä»¶éƒ½åœ¨æ–‡ä»¶é¡¶éƒ¨ç¡¬ç¼–ç äº†é…ç½®å˜é‡ï¼ˆå¦‚ç›®å½•è·¯å¾„ã€æ¨¡å‹åç§°ã€DBSCAN å‚æ•°ç­‰ï¼‰ã€‚è¿™ä½¿å¾—ä¿®æ”¹é…ç½®å˜å¾—å›°éš¾ä¸”å®¹æ˜“å‡ºé”™ã€‚

**å»ºè®®**:
åˆ›å»ºä¸€ä¸ªä¸­å¤®é…ç½®æ–‡ä»¶ï¼Œä¾‹å¦‚ `config.py` æˆ– `config.json`ï¼Œæ¥ç®¡ç†æ‰€æœ‰å…±äº«çš„é…ç½®ã€‚

**ç¤ºä¾‹ (`config.py`)**:
```python
# config.py
from pathlib import Path

# --- Directories ---
BASE_DIR = Path(__file__).parent
CONTENT_DIR = BASE_DIR / "content"
OUTPUT_DIR = BASE_DIR / "output"
REDDIT_RAW_DIR = CONTENT_DIR / "reddit"
REDDIT_CURATED_DIR = CONTENT_DIR / "reddit_english_curated"
REJECTED_DIR = CONTENT_DIR / "processed_json"
TOPICS_OUTPUT_DIR = OUTPUT_DIR / "topics"

# --- Database ---
DATABASE_FILE = BASE_DIR / "topics_database.db"

# --- API & Models ---
# API Keys should remain in .env
ANALYSIS_MODEL = "Qwen/Qwen3-32B"
TRANSLATION_MODEL = "Qwen/Qwen2.5-7B-Instruct"
JUDGE_MODEL = "Qwen/Qwen2.5-7B-Instruct"

# --- Algorithm Parameters ---
DBSCAN_EPS = 0.8
DBSCAN_MIN_SAMPLES = 2
COMMENTS_TO_FETCH = 20
```
ç„¶åï¼Œåœ¨å…¶ä»–è„šæœ¬ä¸­å¯¼å…¥è¿™äº›é…ç½®ï¼š
```python
# analyze_topics.py
import config

# ä½¿ç”¨ config.REDDIT_CURATED_DIR è€Œä¸æ˜¯ç¡¬ç¼–ç çš„å­—ç¬¦ä¸²
input_path = config.REDDIT_CURATED_DIR
```

### 1.2. åˆ›å»ºå…±äº«å·¥å…·æ¨¡å— (`utils.py`)

**é—®é¢˜**: `analyze_topics.py` ä¸­çš„ `call_llm` å‡½æ•°å’Œ `curate.py` ä¸­çš„ `get_llm_judgement` å‡½æ•°åŠŸèƒ½éå¸¸ç›¸ä¼¼ã€‚æ•°æ®åº“è¿æ¥é€»è¾‘ä¹Ÿåœ¨å¤šä¸ªåœ°æ–¹é‡å¤ã€‚

**å»ºè®®**:
åˆ›å»ºä¸€ä¸ª `utils.py` æ–‡ä»¶æ¥å­˜æ”¾è¿™äº›é€šç”¨å‡½æ•°ã€‚

**ç¤ºä¾‹ (`utils.py`)**:
```python
# utils.py
import os
import json
import sqlite3
from openai import OpenAI
from dotenv import load_dotenv
import config # å¯¼å…¥ä¸­å¤®é…ç½®

load_dotenv()
API_KEY = os.getenv("SILICONFLOW_API_KEY")
if not API_KEY:
    raise ValueError("Siliconflow API key not found.")

client = OpenAI(api_key=API_KEY, base_url="https://api.siliconflow.cn/v1")

def call_llm(model: str, prompt: str, temperature: float = 0.3, max_tokens: int = 3000) -> str | None:
    """é€šç”¨ LLM è°ƒç”¨å‡½æ•°ã€‚"""
    messages = [{"role": "user", "content": prompt}]
    try:
        chat_completion = client.chat.completions.create(
            model=model,
            messages=messages,
            temperature=temperature,
            max_tokens=max_tokens
        )
        return chat_completion.choices[0].message.content.strip()
    except Exception as e:
        print(f"LLM call failed: {e}") # å»ºè®®ä½¿ç”¨ logging æ¨¡å—
        return None

def get_db_connection():
    """è·å–æ•°æ®åº“è¿æ¥ã€‚"""
    return sqlite3.connect(config.DATABASE_FILE)
```

### 1.3. ä¾èµ–ç®¡ç† (`requirements.txt`)

**é—®é¢˜**: é¡¹ç›®æ²¡æœ‰æ˜ç¡®çš„ä¾èµ–åˆ—è¡¨ï¼Œè¿™ä½¿å¾—åœ¨æ–°ç¯å¢ƒä¸­éƒ¨ç½²å˜å¾—å›°éš¾ã€‚

**å»ºè®®**:
åˆ›å»ºä¸€ä¸ª `requirements.txt` æ–‡ä»¶ã€‚
```
# requirements.txt
praw
python-dotenv
scikit-learn
openai
pandas
```
å¯ä»¥ä½¿ç”¨ `pip freeze > requirements.txt` å‘½ä»¤ç”Ÿæˆï¼Œä½†æœ€å¥½æ‰‹åŠ¨æ¸…ç†ä¸€ä¸‹ï¼Œåªä¿ç•™é¡¶çº§ä¾èµ–ã€‚

### 1.4. ä½¿ç”¨ `logging` æ¨¡å—

**é—®é¢˜**: æ‰€æœ‰è„šæœ¬éƒ½ä½¿ç”¨ `print()` æ¥è¾“å‡ºçŠ¶æ€ã€è­¦å‘Šå’Œé”™è¯¯ã€‚

**å»ºè®®**:
ä½¿ç”¨ Python çš„ `logging` æ¨¡å—ã€‚å®ƒå¯ä»¥æä¾›æ›´çµæ´»çš„æ§åˆ¶ï¼Œä¾‹å¦‚ï¼š
-   æŒ‰ä¸¥é‡æ€§ï¼ˆDEBUG, INFO, WARNING, ERRORï¼‰è¿‡æ»¤æ—¥å¿—ã€‚
-   è½»æ¾åœ°å°†æ—¥å¿—è¾“å‡ºåˆ°æ–‡ä»¶è€Œä¸æ˜¯æ§åˆ¶å°ã€‚
-   åŒ…å«æ—¶é—´æˆ³å’Œæ¨¡å—åï¼Œæ–¹ä¾¿è°ƒè¯•ã€‚

---

## 2. æ–‡ä»¶çº§æ”¹è¿›å»ºè®®

### 2.1. `curate.py`

**ğŸ”´ å…³é”®ç¼ºé™·ä¿®å¤**:
**é—®é¢˜**: åœ¨ `main` å‡½æ•°ä¸­ï¼Œç”¨äºä¿å­˜æ–‡ä»¶çš„é€»è¾‘å­˜åœ¨ç¼ºé™·ã€‚`if destination.is_dir():` æ°¸è¿œä¸º `False`ï¼Œå¯¼è‡´æ‰€æœ‰æ–‡ä»¶ï¼ˆæ— è®ºæ˜¯æ¥å—è¿˜æ˜¯æ‹’ç»ï¼‰éƒ½åªé€šè¿‡ `json_file.rename(destination)` è¢«ç§»åŠ¨ã€‚è¿™æ„å‘³ç€ä¸ºâ€œæ¥å—â€çš„å¸–å­ç”Ÿæˆçš„ `curation_metadata` **ä»æœªè¢«ä¿å­˜**ã€‚

**ä¿®æ­£å»ºè®®**:
é‡æ„æ–‡ä»¶å¤„ç†é€»è¾‘ï¼Œç¡®ä¿åœ¨æ¥å—å¸–å­æ—¶ï¼Œå°†åŒ…å«æ–°å…ƒæ•°æ®çš„å†…å®¹å†™å…¥æ–°æ–‡ä»¶ã€‚

```python
# curate.py -> main() å¾ªç¯å†…
# ...
            judgement = get_llm_judgement(data)
            
            if judgement.get("is_rejected"):
                reason = judgement.get('rejection_reason', 'Unknown reason')
                print(f" âŒ REJECTED ({reason})")
                destination = rejected_path / json_file.name
                json_file.rename(destination) # ç›´æ¥ç§»åŠ¨åŸæ–‡ä»¶
                rejected_count += 1
            else:
                print(f" âœ… ACCEPTED (Type: {judgement.get('content_type')})")
                # æ·»åŠ å…ƒæ•°æ®å¹¶å†™å…¥æ–°æ–‡ä»¶
                data['curation_metadata'] = judgement
                destination = curated_path / json_file.name
                with open(destination, 'w', encoding='utf-8') as f:
                    json.dump(data, f, indent=2, ensure_ascii=False)
                json_file.unlink() # åˆ é™¤æ”¶ä»¶ç®±ä¸­çš„åŸæ–‡ä»¶
                accepted_count += 1
# ...
```

### 2.2. `analyze_topics.py`

**é—®é¢˜**:
1.  **æ•°æ®åº“è¿æ¥**: `init_db` å’Œ `log_topic_to_db` æ¯æ¬¡éƒ½æ‰“å¼€å’Œå…³é—­æ•°æ®åº“è¿æ¥ï¼Œæ•ˆç‡è¾ƒä½ã€‚
2.  **ä»£ç å¯è¯»æ€§**: `[clusters[label].append(posts[i]) for i, label in enumerate(labels)]` è¿™ç§åˆ—è¡¨æ¨å¯¼å¼è¢«ç”¨äºå…¶å‰¯ä½œç”¨ï¼ˆå¡«å……å­—å…¸ï¼‰ï¼Œè¿™ä¸ç¬¦åˆ Python çš„æœ€ä½³å®è·µï¼Œé™ä½äº†å¯è¯»æ€§ã€‚

**å»ºè®®**:
1.  åœ¨ `main` å‡½æ•°å¼€å§‹æ—¶å»ºç«‹ä¸€æ¬¡æ•°æ®åº“è¿æ¥ï¼Œå¹¶å°†å…¶ä¼ é€’ç»™éœ€è¦çš„å‡½æ•°ã€‚
2.  ä½¿ç”¨æ ‡å‡†çš„ `for` å¾ªç¯æ¥å¡«å…… `clusters` å­—å…¸ã€‚

```python
# analyze_topics.py -> main()
def main():
    # ...
    conn = sqlite3.connect(DATABASE_FILE)
    init_db(conn) # ä¿®æ”¹ init_db ä»¥æ¥å—è¿æ¥å¯¹è±¡

    # ...
    clusters = defaultdict(list)
    for i, label in enumerate(labels):
        clusters[label].append(posts[i]) # æ›´æ¸…æ™°çš„å†™æ³•

    # ... å¾ªç¯å†…
        log_topic_to_db(conn, topic_name, ...) # ä¼ é€’è¿æ¥å¯¹è±¡
    
    conn.close() # åœ¨è„šæœ¬æœ«å°¾å…³é—­è¿æ¥
# ...
```

### 2.3. `reddit_collection.py`

**é—®é¢˜**:
1.  **è·¯å¾„å¤„ç†**: è¯¥æ–‡ä»¶ä½¿ç”¨ `os.path.join`ï¼Œè€Œé¡¹ç›®ä¸­å…¶ä»–è„šæœ¬å·²åœ¨ä½¿ç”¨æ›´ç°ä»£ã€æ›´é¢å‘å¯¹è±¡çš„ `pathlib.Path`ã€‚
2.  **å‡½æ•°è¿‡é•¿**: `process_and_save_submission` å‡½æ•°æ‰¿æ‹…äº†å¤ªå¤šè´£ä»»ï¼šè¿‡æ»¤ã€æ£€æŸ¥äº¤å‰å¸–å­ã€è·å–è¯„è®ºã€æ„å»ºæ•°æ®ç»“æ„å’Œä¿å­˜æ–‡ä»¶ã€‚

**å»ºè®®**:
1.  ç»Ÿä¸€ä½¿ç”¨ `pathlib.Path` è¿›è¡Œæ‰€æœ‰è·¯å¾„æ“ä½œã€‚
2.  å°† `process_and_save_submission` åˆ†è§£ä¸ºæ›´å°çš„è¾…åŠ©å‡½æ•°ï¼Œä¾‹å¦‚ `is_high_quality()`, `fetch_comments()`, `save_post_data()`ã€‚

### 2.4. `trend_analyzer.py`

**é—®é¢˜**:
1.  **æ•°æ®åº“æ•ˆç‡**: æ¯ä¸ªå‡½æ•°ï¼ˆ`query_topics_by_keywords`, `get_related_keywords` ç­‰ï¼‰éƒ½é‡æ–°è¿æ¥æ•°æ®åº“ã€‚å¯¹äºä¸€ä¸ªäº¤äº’å¼å·¥å…·ï¼Œè¿™ä¼šå¸¦æ¥ä¸å¿…è¦çš„å¼€é”€ã€‚
2.  **â€œé­”æ•°â€ (Magic Numbers)**: `calculate_relevance_score` ä¸­çš„æƒé‡ï¼ˆ30, 15, 20, 10, 5ï¼‰æ˜¯ç¡¬ç¼–ç çš„â€œé­”æ•°â€ï¼Œå…¶å«ä¹‰ä¸æ˜ç¡®ã€‚
3.  **UI å­—ç¬¦ä¸²**: äº¤äº’ç•Œé¢ä¸­çš„æç¤ºè¯­å°†ä¸­è‹±æ–‡ç¡¬ç¼–ç åœ¨ä¸€èµ·ï¼ˆä¾‹å¦‚ `"ğŸ” æ™ºèƒ½è¶‹åŠ¿åˆ†æå·¥å…· | Intelligent Trend Analysis Tool"`ï¼‰ï¼Œä¸åˆ©äºç»´æŠ¤å’Œæœªæ¥çš„å›½é™…åŒ–ã€‚

**å»ºè®®**:
1.  åœ¨ `interactive_search` å‡½æ•°çš„å¼€å¤´å»ºç«‹ä¸€ä¸ªæ•°æ®åº“è¿æ¥ï¼Œå¹¶å°†å…¶ä¼ é€’ç»™å…¶ä»–æŸ¥è¯¢å‡½æ•°ã€‚
2.  å°†æƒé‡å®šä¹‰ä¸ºæœ‰æ„ä¹‰çš„å¸¸é‡ã€‚
    ```python
    # trend_analyzer.py
    SCORE_EXACT_KEYWORD = 30
    SCORE_PARTIAL_KEYWORD = 15
    # ...
    ```
3.  ä¸º UI æ–‡æœ¬åˆ›å»ºä¸€ä¸ªå­—å…¸ï¼Œä»¥ä¾¿äºç®¡ç†ã€‚
    ```python
    # trend_analyzer.py
    UI_TEXT = {
        "main_title": "ğŸ” æ™ºèƒ½è¶‹åŠ¿åˆ†æå·¥å…· | Intelligent Trend Analysis Tool",
        "options": "é€‰é¡¹ | Options:",
        # ...
    }
    print(UI_TEXT["main_title"])
    ```
