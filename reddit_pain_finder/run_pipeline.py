#!/usr/bin/env python3
"""
Wise Collection - Main Pipeline Runner
ä¸»è¦çš„pipelineæ‰§è¡Œè„šæœ¬ - ä¸€é”®è¿è¡Œæ•´ä¸ªæ•°æ®æ”¶é›†æµç¨‹
"""
import os
import sys
import argparse
import logging
import json
import time
from datetime import datetime
from typing import Dict, Any, Optional, List

# è®¾ç½®é¡¹ç›®æ ¹ç›®å½•
project_root = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, project_root)

# å¯¼å…¥pipelineæ¨¡å—
from pipeline.fetch import RedditSourceFetcher
from pipeline.filter_signal import PainSignalFilter
from pipeline.extract_pain import PainPointExtractor
from pipeline.embed import PainEventEmbedder
from pipeline.cluster import PainEventClusterer
from pipeline.score_viability import ViabilityScorer
from pipeline.map_opportunity import OpportunityMapper
from pipeline.align_cross_sources import CrossSourceAligner

# å¯¼å…¥å·¥å…·æ¨¡å—
from utils.db import db
from utils.llm_client import LLMClient

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/pipeline.log'),
        logging.StreamHandler(sys.stdout)
    ]
)

logger = logging.getLogger(__name__)

class WiseCollectionPipeline:
    """Wise Collectionæ•°æ®æ”¶é›†Pipeline"""

    def __init__(self):
        """åˆå§‹åŒ–pipeline"""
        self.pipeline_start_time = datetime.now()
        self.stats = {
            "start_time": self.pipeline_start_time.isoformat(),
            "stages_completed": [],
            "stages_failed": [],
            "stage_results": {},
            "total_runtime_seconds": 0,
            "final_summary": {}
        }

        # ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨
        os.makedirs("logs", exist_ok=True)

    def _load_config(self, config_path: str = "config/llm.yaml") -> Dict[str, Any]:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            import yaml
            with open(config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
            return config
        except Exception as e:
            logger.warning(f"Failed to load config from {config_path}: {e}")
            # è¿”å›é»˜è®¤é…ç½®
            return {
                'database': {'path': 'data/wise_collection.db'},
                'llm': {
                    'models': {
                        'main': 'gpt-4',
                        'medium': 'gpt-3.5-turbo',
                        'small': 'gpt-3.5-turbo'
                    }
                }
            }

    def run_stage_fetch(self, limit_sources: Optional[int] = None,
                       sources: Optional[List[str]] = None) -> Dict[str, Any]:
        """é˜¶æ®µ1: æ•°æ®æŠ“å–ï¼ˆæ”¯æŒå¤šæ•°æ®æºï¼‰"""
        logger.info("=" * 50)
        logger.info("STAGE 1: Multi-Source Posts Fetcher")
        logger.info("=" * 50)

        try:
            from pipeline.fetch import MultiSourceFetcher

            # ä½¿ç”¨æŒ‡å®šçš„æ•°æ®æºï¼Œé»˜è®¤ä¸º reddit + hackernews
            fetch_sources = sources or ['reddit', 'hackernews']
            fetcher = MultiSourceFetcher(sources=fetch_sources)
            result = fetcher.fetch_all(limit_sources=limit_sources)

            self.stats["stages_completed"].append("fetch")
            self.stats["stage_results"]["fetch"] = result

            logger.info(f"âœ… Stage 1 completed: Found {result['total_saved']} posts from {len(result['sources_processed'])} sources")

            # è¾“å‡ºå„æ•°æ®æºç»Ÿè®¡
            for source, stats in result.get("source_stats", {}).items():
                if "error" not in stats:
                    logger.info(f"   - {source}: {stats.get('total_saved', 0)} posts")
                else:
                    logger.error(f"   - {source}: ERROR - {stats['error']}")

            return result

        except Exception as e:
            logger.error(f"âŒ Stage 1 failed: {e}")
            self.stats["stages_failed"].append("fetch")
            raise

    def run_stage_filter(self, limit_posts: Optional[int] = None) -> Dict[str, Any]:
        """é˜¶æ®µ2: ä¿¡å·è¿‡æ»¤"""
        logger.info("=" * 50)
        logger.info("STAGE 2: Filtering pain signals")
        logger.info("=" * 50)

        try:
            filter = PainSignalFilter()

            # è·å–æœªè¿‡æ»¤çš„å¸–å­
            unfiltered_posts = db.get_unprocessed_posts(limit=limit_posts or 1000)

            if not unfiltered_posts:
                logger.info("No posts to filter")
                result = {"processed": 0, "filtered": 0}
            else:
                logger.info(f"Filtering {len(unfiltered_posts)} posts")
                filtered_posts = filter.filter_posts_batch(unfiltered_posts)

                # ä¿å­˜è¿‡æ»¤ç»“æœ
                saved_count = 0
                for post in filtered_posts:
                    if db.insert_filtered_post(post):
                        saved_count += 1

                result = {
                    "processed": len(unfiltered_posts),
                    "filtered": len(filtered_posts),
                    "saved": saved_count,
                    "filter_stats": filter.get_statistics()
                }

            self.stats["stages_completed"].append("filter")
            self.stats["stage_results"]["filter"] = result

            logger.info(f"âœ… Stage 2 completed: Filtered {result['saved']} posts")
            return result

        except Exception as e:
            logger.error(f"âŒ Stage 2 failed: {e}")
            self.stats["stages_failed"].append("filter")
            raise

    def run_stage_extract(self, limit_posts: Optional[int] = None) -> Dict[str, Any]:
        """é˜¶æ®µ3: ç—›ç‚¹æŠ½å–"""
        logger.info("=" * 50)
        logger.info("STAGE 3: Extracting pain points")
        logger.info("=" * 50)

        try:
            extractor = PainPointExtractor()
            result = extractor.process_unextracted_posts(limit=limit_posts or 100)

            self.stats["stages_completed"].append("extract")
            self.stats["stage_results"]["extract"] = result

            logger.info(f"âœ… Stage 3 completed: Extracted {result['pain_events_saved']} pain events")
            return result

        except Exception as e:
            logger.error(f"âŒ Stage 3 failed: {e}")
            self.stats["stages_failed"].append("extract")
            raise

    def run_stage_embed(self, limit_events: Optional[int] = None) -> Dict[str, Any]:
        """é˜¶æ®µ4: å‘é‡åŒ–"""
        logger.info("=" * 50)
        logger.info("STAGE 4: Creating embeddings")
        logger.info("=" * 50)

        try:
            embedder = PainEventEmbedder()
            result = embedder.process_missing_embeddings(limit=limit_events or 200)

            self.stats["stages_completed"].append("embed")
            self.stats["stage_results"]["embed"] = result

            logger.info(f"âœ… Stage 4 completed: Created {result['embeddings_created']} embeddings")
            return result

        except Exception as e:
            logger.error(f"âŒ Stage 4 failed: {e}")
            self.stats["stages_failed"].append("embed")
            raise

    def run_stage_cluster(self, limit_events: Optional[int] = None) -> Dict[str, Any]:
        """é˜¶æ®µ5: èšç±»"""
        logger.info("=" * 50)
        logger.info("STAGE 5: Clustering pain events")
        logger.info("=" * 50)

        try:
            clusterer = PainEventClusterer()
            result = clusterer.cluster_pain_events(limit=limit_events or 200)

            self.stats["stages_completed"].append("cluster")
            self.stats["stage_results"]["cluster"] = result

            logger.info(f"âœ… Stage 5 completed: Created {result['clusters_created']} clusters")
            return result

        except Exception as e:
            logger.error(f"âŒ Stage 5 failed: {e}")
            self.stats["stages_failed"].append("cluster")
            raise

    def run_stage_cross_source_alignment(self) -> Dict[str, Any]:
        """é˜¶æ®µ5.5: è·¨æºå¯¹é½"""
        logger.info("=" * 50)
        logger.info("STAGE 5.5: Cross-Source Alignment")
        logger.info("=" * 50)

        try:
            # åˆå§‹åŒ–å¯¹é½å™¨
            llm_client = LLMClient()  # Uses default config path
            aligner = CrossSourceAligner(db, llm_client)

            # æ‰§è¡Œè·¨æºå¯¹é½
            logger.info("Processing cross-source alignment...")
            aligner.process_alignments()

            # è·å–å¯¹é½ç»“æœ
            aligned_problems = db.get_aligned_problems()

            result = {
                "aligned_problems_count": len(aligned_problems),
                "aligned_problems": aligned_problems
            }

            self.stats["stages_completed"].append("alignment")
            self.stats["stage_results"]["alignment"] = result

            logger.info(f"âœ… Stage 5.5 completed: Found {len(aligned_problems)} aligned problems")

            # æ˜¾ç¤ºå¯¹é½æ‘˜è¦
            if aligned_problems:
                logger.info("\nAlignment Summary:")
                logger.info(f"- Total aligned problems: {len(aligned_problems)}")
                for problem in aligned_problems[:3]:  # æ˜¾ç¤ºå‰3ä¸ª
                    logger.info(f"  {problem['aligned_problem_id']}: {problem['core_problem'][:100]}...")
                    logger.info(f"  Sources: {', '.join(problem['sources'])}")
            else:
                logger.info("No cross-source alignments found in this run")

            return result

        except Exception as e:
            logger.error(f"âŒ Stage 5.5 failed: {e}")
            self.stats["stages_failed"].append("alignment")
            raise

    def run_stage_map_opportunities(self, limit_clusters: Optional[int] = None) -> Dict[str, Any]:
        """é˜¶æ®µ6: æœºä¼šæ˜ å°„"""
        logger.info("=" * 50)
        logger.info("STAGE 6: Mapping opportunities")
        logger.info("=" * 50)

        try:
            mapper = OpportunityMapper()
            result = mapper.map_opportunities_for_clusters(limit=limit_clusters or 50)

            self.stats["stages_completed"].append("map_opportunities")
            self.stats["stage_results"]["map_opportunities"] = result

            logger.info(f"âœ… Stage 6 completed: Mapped {result['opportunities_created']} opportunities")
            return result

        except Exception as e:
            logger.error(f"âŒ Stage 6 failed: {e}")
            self.stats["stages_failed"].append("map_opportunities")
            raise

    def run_stage_score(self, limit_opportunities: Optional[int] = None) -> Dict[str, Any]:
        """é˜¶æ®µ7: å¯è¡Œæ€§è¯„åˆ†"""
        logger.info("=" * 50)
        logger.info("STAGE 7: Scoring viability")
        logger.info("=" * 50)

        try:
            scorer = ViabilityScorer()
            result = scorer.score_opportunities(limit=limit_opportunities or 100)

            self.stats["stages_completed"].append("score")
            self.stats["stage_results"]["score"] = result

            logger.info(f"âœ… Stage 7 completed: Scored {result['opportunities_scored']} opportunities")
            return result

        except Exception as e:
            logger.error(f"âŒ Stage 7 failed: {e}")
            self.stats["stages_failed"].append("score")
            raise

    def generate_final_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š"""
        logger.info("=" * 50)
        logger.info("GENERATING FINAL REPORT")
        logger.info("=" * 50)

        try:
            # è·å–æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯
            db_stats = db.get_statistics()

            # è·å–æœ€é«˜åˆ†çš„æœºä¼š
            top_opportunities = []
            try:
                with db.get_connection("clusters") as conn:
                    cursor = conn.execute("""
                        SELECT o.opportunity_name, o.total_score, o.recommendation, c.cluster_name
                        FROM opportunities o
                        JOIN clusters c ON o.cluster_id = c.id
                        WHERE o.total_score > 0
                        ORDER BY o.total_score DESC
                        LIMIT 10
                    """)
                    top_opportunities = [dict(row) for row in cursor.fetchall()]
            except Exception as e:
                logger.warning(f"Failed to get top opportunities: {e}")

            # è®¡ç®—è¿è¡Œæ—¶é—´
            end_time = datetime.now()
            total_runtime = (end_time - self.pipeline_start_time).total_seconds()
            self.stats["total_runtime_seconds"] = total_runtime

            # ç”Ÿæˆæœ€ç»ˆæ‘˜è¦
            final_summary = {
                "pipeline_completed": len(self.stats["stages_failed"]) == 0,
                "stages_completed": len(self.stats["stages_completed"]),
                "stages_failed": len(self.stats["stages_failed"]),
                "total_runtime_minutes": round(total_runtime / 60, 2),
                "database_statistics": db_stats,
                "top_opportunities": top_opportunities,
                "pipeline_efficiency": {
                    "posts_per_minute": self.stats["stage_results"].get("fetch", {}).get("total_saved", 0) / max(total_runtime / 60, 1),
                    "pain_events_per_hour": self.stats["stage_results"].get("extract", {}).get("pain_events_saved", 0) / max(total_runtime / 3600, 1),
                    "opportunities_per_hour": self.stats["stage_results"].get("map_opportunities", {}).get("opportunities_created", 0) / max(total_runtime / 3600, 1)
                }
            }

            self.stats["final_summary"] = final_summary

            logger.info("ğŸ‰ PIPELINE COMPLETED SUCCESSFULLY!")
            logger.info(f"ğŸ“Š Final Summary:")
            logger.info(f"   â€¢ Runtime: {final_summary['total_runtime_minutes']} minutes")
            logger.info(f"   â€¢ Stages completed: {final_summary['stages_completed']}/7")
            logger.info(f"   â€¢ Raw posts collected: {db_stats.get('raw_posts_count', 0)}")
            logger.info(f"   â€¢ Pain events extracted: {db_stats.get('pain_events_count', 0)}")
            logger.info(f"   â€¢ Clusters created: {db_stats.get('clusters_count', 0)}")
            logger.info(f"   â€¢ Opportunities identified: {db_stats.get('opportunities_count', 0)}")

            if top_opportunities:
                logger.info(f"   â€¢ Top opportunity: {top_opportunities[0]['opportunity_name']} (Score: {top_opportunities[0]['total_score']:.1f})")

            return final_summary

        except Exception as e:
            logger.error(f"Failed to generate final report: {e}")
            return {"error": str(e)}

    def run_full_pipeline(
        self,
        limit_sources: Optional[int] = None,
        limit_posts: Optional[int] = None,
        limit_events: Optional[int] = None,
        limit_clusters: Optional[int] = None,
        limit_opportunities: Optional[int] = None,
        sources: Optional[List[str]] = None,
        stop_on_error: bool = False
    ) -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´pipeline"""
        logger.info("ğŸš€ Starting Wise Collection Multi-Source Pipeline")
        logger.info(f"â° Started at: {self.pipeline_start_time}")

        # ä½¿ç”¨æŒ‡å®šçš„æ•°æ®æºï¼Œé»˜è®¤ä¸º reddit + hackernews
        fetch_sources = sources or ['reddit', 'hackernews']
        logger.info(f"ğŸ“¡ Data sources: {', '.join(fetch_sources)}")

        stages = [
            ("fetch", lambda: self.run_stage_fetch(limit_sources, fetch_sources)),
            ("filter", lambda: self.run_stage_filter(limit_posts)),
            ("extract", lambda: self.run_stage_extract(limit_posts)),
            ("embed", lambda: self.run_stage_embed(limit_events)),
            ("cluster", lambda: self.run_stage_cluster(limit_events)),
            ("alignment", lambda: self.run_stage_cross_source_alignment()),
            ("map_opportunities", lambda: self.run_stage_map_opportunities(limit_clusters)),
            ("score", lambda: self.run_stage_score(limit_opportunities))
        ]

        for stage_name, stage_func in stages:
            try:
                stage_func()
            except Exception as e:
                logger.error(f"Stage '{stage_name}' failed: {e}")
                if stop_on_error:
                    logger.error("Stopping pipeline due to error")
                    break
                else:
                    logger.warning(f"Continuing pipeline despite '{stage_name}' failure")

        # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
        final_report = self.generate_final_report()

        return final_report

    def run_single_stage(self, stage_name: str, **kwargs) -> Dict[str, Any]:
        """è¿è¡Œå•ä¸ªé˜¶æ®µ"""
        stage_map = {
            "fetch": lambda: self.run_stage_fetch(kwargs.get("limit_sources"), kwargs.get("sources")),
            "filter": lambda: self.run_stage_filter(kwargs.get("limit_posts")),
            "extract": lambda: self.run_stage_extract(kwargs.get("limit_posts")),
            "embed": lambda: self.run_stage_embed(kwargs.get("limit_events")),
            "cluster": lambda: self.run_stage_cluster(kwargs.get("limit_events")),
            "alignment": lambda: self.run_stage_cross_source_alignment(),
            "map": lambda: self.run_stage_map_opportunities(kwargs.get("limit_clusters")),
            "score": lambda: self.run_stage_score(kwargs.get("limit_opportunities"))
        }

        if stage_name not in stage_map:
            raise ValueError(f"Unknown stage: {stage_name}")

        return stage_map[stage_name]()

    def save_results(self, filename: Optional[str] = None):
        """ä¿å­˜pipelineç»“æœ"""
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"pipeline_results_{timestamp}.json"

        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(self.stats, f, indent=2, default=str)

            logger.info(f"ğŸ“ Results saved to: {filename}")
            return filename

        except Exception as e:
            logger.error(f"Failed to save results: {e}")
            return None

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="Wise Collection Multi-Source Pipeline")

    # è¿è¡Œæ¨¡å¼
    parser.add_argument("--stage", choices=["fetch", "filter", "extract", "embed", "cluster", "alignment", "map", "score", "all"],
                       default="all", help="Which stage to run (default: all)")

    # æ•°æ®æºé€‰æ‹©
    parser.add_argument("--sources", nargs="+", choices=["reddit", "hackernews"],
                       default=["reddit", "hackernews"], help="Data sources to fetch (default: reddit hackernews)")

    # é™åˆ¶å‚æ•°
    parser.add_argument("--limit-sources", type=int, help="Limit number of sources to fetch")
    parser.add_argument("--limit-posts", type=int, help="Limit number of posts to process")
    parser.add_argument("--limit-events", type=int, help="Limit number of pain events to process")
    parser.add_argument("--limit-clusters", type=int, help="Limit number of clusters to process")
    parser.add_argument("--limit-opportunities", type=int, help="Limit number of opportunities to score")

    # å…¶ä»–é€‰é¡¹
    parser.add_argument("--stop-on-error", action="store_true", help="Stop pipeline on first error")
    parser.add_argument("--save-results", action="store_true", help="Save results to file")
    parser.add_argument("--results-file", help="Custom results filename")

    args = parser.parse_args()

    try:
        # åˆå§‹åŒ–pipeline
        pipeline = WiseCollectionPipeline()

        if args.stage == "all":
            # è¿è¡Œå®Œæ•´pipeline
            result = pipeline.run_full_pipeline(
                limit_sources=args.limit_sources,
                limit_posts=args.limit_posts,
                limit_events=args.limit_events,
                limit_clusters=args.limit_clusters,
                limit_opportunities=args.limit_opportunities,
                sources=args.sources,
                stop_on_error=args.stop_on_error
            )
        else:
            # è¿è¡Œå•ä¸ªé˜¶æ®µ
            stage_kwargs = {
                "limit_sources": args.limit_sources,
                "limit_posts": args.limit_posts,
                "limit_events": args.limit_events,
                "limit_clusters": args.limit_clusters,
                "limit_opportunities": args.limit_opportunities,
                "sources": args.sources
            }

            # åªä¼ é€’ç›¸å…³çš„å‚æ•°
            relevant_kwargs = {k: v for k, v in stage_kwargs.items() if v is not None}
            result = pipeline.run_single_stage(args.stage, **relevant_kwargs)

        # ä¿å­˜ç»“æœ
        if args.save_results:
            pipeline.save_results(args.results_file)

        # è¾“å‡ºç»“æœ
        print("\n" + "=" * 60)
        print("PIPELINE RESULTS")
        print("=" * 60)
        print(json.dumps(result, indent=2, default=str))

    except KeyboardInterrupt:
        logger.info("Pipeline interrupted by user")
        sys.exit(1)
    except Exception as e:
        logger.error(f"Pipeline failed: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()